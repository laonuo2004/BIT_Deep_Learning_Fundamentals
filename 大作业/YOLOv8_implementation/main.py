#| # å®¤å†…å°ç‰©ä½“æ£€æµ‹é¡¹ç›® - YOLOv8å®ç°
#| 
#| **é¡¹ç›®ç›®æ ‡**: åŸºäºYOLOv8å®ç°å®¤å†…å°ç‰©ä½“æ£€æµ‹ï¼Œè¾¾åˆ°é«˜è´¨é‡çš„mAP@0.5:0.95æ€§èƒ½
#| 
#| **æ ¸å¿ƒç­–ç•¥**: 
#| - ä½¿ç”¨YOLOv8sä½œä¸ºåŸºç¡€æ¨¡å‹
#| - ä»…ä½¿ç”¨ImageNeté¢„è®­ç»ƒçš„backboneï¼Œä¸ä½¿ç”¨COCOé¢„è®­ç»ƒçš„æ£€æµ‹å¤´
#| - é‡‡ç”¨80/20åˆ†å±‚åˆ’åˆ†åˆ›å»ºéªŒè¯é›†
#| - å®æ–½å†’çƒŸæµ‹è¯•ç¡®ä¿æµç¨‹æ­£ç¡®æ€§
#| 
#| **å­¦ä¹ æ”¶è·**:
#| åœ¨è¿™ä¸ªé¡¹ç›®ä¸­ï¼Œæˆ‘å°†å­¦ä¹ å¦‚ä½•ä»é›¶å¼€å§‹æ„å»ºä¸€ä¸ªå®Œæ•´çš„ç›®æ ‡æ£€æµ‹ç³»ç»Ÿï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯å’Œé¢„æµ‹çš„å…¨æµç¨‹ã€‚

#! pip install ultralytics
#! pip install opencv-python
#! pip install pandas
#! pip install scikit-learn
#! pip install tqdm

import os
import json
import pandas as pd
import numpy as np
from pathlib import Path
import shutil
from sklearn.model_selection import train_test_split
from collections import defaultdict, Counter
import cv2
from tqdm import tqdm
import yaml

#| ## é¡¹ç›®é…ç½®
#| è®¾ç½®é¡¹ç›®çš„åŸºæœ¬è·¯å¾„å’Œé…ç½®å‚æ•°

# é¡¹ç›®æ ¹ç›®å½•é…ç½®
BASE_DIR = Path('.')
DATA_DIR = BASE_DIR / '../dl_detection'
ANNOTATIONS_DIR = DATA_DIR / 'annotations'
TRAIN_IMG_DIR = DATA_DIR / 'train'
TEST_IMG_DIR = DATA_DIR / 'test'

# YOLOv8æ•°æ®é›†è¾“å‡ºç›®å½•
YOLO_DATA_DIR = BASE_DIR / 'data' / 'yolo_dataset'
YOLO_TRAIN_DIR = YOLO_DATA_DIR / 'train'
YOLO_VAL_DIR = YOLO_DATA_DIR / 'val'
YOLO_SMOKE_DIR = YOLO_DATA_DIR / 'smoke_test'

# é…ç½®å‚æ•°
VALIDATION_SPLIT = 0.2  # éªŒè¯é›†æ¯”ä¾‹
SMOKE_TEST_RATIO = 0.01  # å†’çƒŸæµ‹è¯•æ•°æ®æ¯”ä¾‹
RANDOM_STATE = 42

# COCOç±»åˆ«åˆ°è¿ç»­IDçš„æ˜ å°„ï¼ˆYOLOv8éœ€è¦ä»0å¼€å§‹çš„è¿ç»­IDï¼‰
COCO_CATEGORIES = [
    "backpack", "cup", "bowl", "banana", "apple", "orange", "chair", "couch",
    "potted plant", "bed", "dining table", "laptop", "mouse", "keyboard",
    "cell phone", "book", "clock", "vase", "scissors", "hair drier", "toothbrush"
]

print("âœ… é¡¹ç›®é…ç½®å®Œæˆ")
print(f"æ•°æ®é›†è·¯å¾„: {DATA_DIR}")
print(f"è¾“å‡ºè·¯å¾„: {YOLO_DATA_DIR}")

#-

#| ## æ•°æ®å‡†å¤‡å‡½æ•°
#| å®ç°æ•°æ®é›†çš„åˆ†å±‚åˆ’åˆ†ã€æ ¼å¼è½¬æ¢å’Œç›®å½•åˆ›å»º

def prepare_data():
    """
    ä¸»è¦çš„æ•°æ®å‡†å¤‡å‡½æ•°
    å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. åŠ è½½COCOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
    2. æ‰§è¡Œ80/20åˆ†å±‚åˆ’åˆ†
    3. è½¬æ¢ä¸ºYOLOæ ¼å¼
    4. åˆ›å»ºsmoke testæ•°æ®é›†
    5. ç”Ÿæˆdata.yamlé…ç½®æ–‡ä»¶
    """
    print("ğŸš€ å¼€å§‹æ•°æ®å‡†å¤‡æµç¨‹...")
    
    # 1. åŠ è½½æ ‡æ³¨æ–‡ä»¶
    print("ğŸ“– åŠ è½½è®­ç»ƒæ ‡æ³¨æ–‡ä»¶...")
    with open(ANNOTATIONS_DIR / 'train.json', 'r', encoding='utf-8') as f:
        coco_data = json.load(f)
    
    print(f"âœ… åŠ è½½å®Œæˆ: {len(coco_data['images'])}å¼ å›¾ç‰‡, {len(coco_data['annotations'])}ä¸ªæ ‡æ³¨")
    
    # 2. åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    analyze_dataset_statistics(coco_data)
    
    # 3. æ‰§è¡Œåˆ†å±‚åˆ’åˆ†
    train_data, val_data = perform_stratified_split(coco_data)
    
    # 4. åˆ›å»ºYOLOæ ¼å¼ç›®å½•ç»“æ„
    create_yolo_directories()
    
    # 5. è½¬æ¢è®­ç»ƒé›†
    print("ğŸ”„ è½¬æ¢è®­ç»ƒé›†ä¸ºYOLOæ ¼å¼...")
    convert_to_yolo_format(train_data, YOLO_TRAIN_DIR, 'train')
    
    # 6. è½¬æ¢éªŒè¯é›†
    print("ğŸ”„ è½¬æ¢éªŒè¯é›†ä¸ºYOLOæ ¼å¼...")
    convert_to_yolo_format(val_data, YOLO_VAL_DIR, 'val')
    
    # 7. åˆ›å»ºå†’çƒŸæµ‹è¯•æ•°æ®é›†
    create_smoke_test_dataset(train_data)
    
    # 8. ç”Ÿæˆdata.yamlé…ç½®æ–‡ä»¶
    create_data_yaml()
    
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼")

#-

def analyze_dataset_statistics(coco_data):
    """åˆ†ææ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ç±»åˆ«åˆ†å¸ƒ"""
    print("ğŸ“Š åˆ†ææ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯...")
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ ‡æ³¨æ•°é‡
    category_counts = Counter()
    image_category_map = defaultdict(set)
    
    for annotation in coco_data['annotations']:
        category_id = annotation['category_id']
        image_id = annotation['image_id']
        category_counts[category_id] += 1
        image_category_map[image_id].add(category_id)
    
    # æ˜¾ç¤ºç±»åˆ«åˆ†å¸ƒ
    print("\nğŸ“ˆ ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡:")
    print("-" * 50)
    for i, category_name in enumerate(COCO_CATEGORIES):
        count = category_counts[i]
        percentage = (count / len(coco_data['annotations'])) * 100
        print(f"{i:2d}. {category_name:15s} | {count:5d} ({percentage:5.1f}%)")
    
    # ç»Ÿè®¡æ¯å¼ å›¾ç‰‡çš„ç±»åˆ«æ•°é‡
    images_per_category_count = Counter()
    for image_id, categories in image_category_map.items():
        images_per_category_count[len(categories)] += 1
    
    print(f"\nğŸ“· å›¾ç‰‡ç»Ÿè®¡:")
    print(f"æ€»å›¾ç‰‡æ•°: {len(coco_data['images'])}")
    print(f"æ€»æ ‡æ³¨æ•°: {len(coco_data['annotations'])}")
    print(f"å¹³å‡æ¯å¼ å›¾ç‰‡æ ‡æ³¨æ•°: {len(coco_data['annotations']) / len(coco_data['images']):.2f}")
    
    return image_category_map

#-

def perform_stratified_split(coco_data):
    """
    æ‰§è¡Œåˆ†å±‚æŠ½æ ·åˆ’åˆ†æ•°æ®é›†
    ç¡®ä¿è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„ç±»åˆ«åˆ†å¸ƒä¿æŒä¸€è‡´
    """
    print("ğŸ¯ æ‰§è¡Œåˆ†å±‚æŠ½æ ·...")
    
    # æ„å»ºå›¾ç‰‡IDåˆ°ç±»åˆ«é›†åˆçš„æ˜ å°„
    image_category_map = defaultdict(set)
    for annotation in coco_data['annotations']:
        image_id = annotation['image_id']
        category_id = annotation['category_id']
        image_category_map[image_id].add(category_id)
    
    # ä¸ºåˆ†å±‚æŠ½æ ·åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨ä¸»è¦ç±»åˆ«ä½œä¸ºåˆ†å±‚ä¾æ®ï¼‰
    image_ids = []
    stratify_labels = []
    
    for image in coco_data['images']:
        image_id = image['id']
        categories = image_category_map[image_id]
        
        if categories:
            # ä½¿ç”¨å›¾ç‰‡ä¸­çš„ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºåˆ†å±‚æ ‡ç­¾
            # è¿™é‡Œå¯ä»¥ä¼˜åŒ–ä¸ºä½¿ç”¨æœ€å¸¸è§çš„ç±»åˆ«æˆ–å…¶ä»–ç­–ç•¥
            primary_category = min(categories)  # ä½¿ç”¨æœ€å°çš„category_idä½œä¸ºä¸»è¦ç±»åˆ«
            image_ids.append(image_id)
            stratify_labels.append(primary_category)
        else:
            # æ²¡æœ‰æ ‡æ³¨çš„å›¾ç‰‡ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å­˜åœ¨ï¼‰
            print(f"âš ï¸ è­¦å‘Š: å›¾ç‰‡ {image_id} æ²¡æœ‰æ ‡æ³¨")
    
    # æ‰§è¡Œåˆ†å±‚åˆ’åˆ†
    train_image_ids, val_image_ids = train_test_split(
        image_ids,
        test_size=VALIDATION_SPLIT,
        stratify=stratify_labels,
        random_state=RANDOM_STATE
    )
    
    print(f"âœ… åˆ†å±‚åˆ’åˆ†å®Œæˆ:")
    print(f"  è®­ç»ƒé›†: {len(train_image_ids)} å¼ å›¾ç‰‡")
    print(f"  éªŒè¯é›†: {len(val_image_ids)} å¼ å›¾ç‰‡")
    
    # åˆ›å»ºåˆ†å‰²åçš„æ•°æ®ç»“æ„
    train_data = create_split_data(coco_data, set(train_image_ids))
    val_data = create_split_data(coco_data, set(val_image_ids))
    
    return train_data, val_data

#-

def create_split_data(coco_data, image_ids_set):
    """æ ¹æ®å›¾ç‰‡IDé›†åˆåˆ›å»ºåˆ†å‰²åçš„æ•°æ®ç»“æ„"""
    # ç­›é€‰å›¾ç‰‡
    split_images = [img for img in coco_data['images'] if img['id'] in image_ids_set]
    
    # ç­›é€‰æ ‡æ³¨
    split_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids_set]
    
    # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
    split_data = {
        'info': coco_data['info'],
        'images': split_images,
        'annotations': split_annotations,
        'categories': coco_data['categories']
    }
    
    return split_data

#-

def create_yolo_directories():
    """åˆ›å»ºYOLOæ ¼å¼æ‰€éœ€çš„ç›®å½•ç»“æ„"""
    print("ğŸ“ åˆ›å»ºYOLOç›®å½•ç»“æ„...")
    
    directories = [
        YOLO_TRAIN_DIR / 'images',
        YOLO_TRAIN_DIR / 'labels',
        YOLO_VAL_DIR / 'images',
        YOLO_VAL_DIR / 'labels',
        YOLO_SMOKE_DIR / 'images',
        YOLO_SMOKE_DIR / 'labels'
    ]
    
    for directory in directories:
        directory.mkdir(parents=True, exist_ok=True)
    
    print("âœ… ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")

#-

def convert_to_yolo_format(split_data, output_dir, split_name):
    """
    å°†COCOæ ¼å¼è½¬æ¢ä¸ºYOLOæ ¼å¼
    COCO: [x_left, y_top, width, height] (ç»å¯¹åæ ‡)
    YOLO: [class_id, x_center, y_center, width, height] (ç›¸å¯¹åæ ‡ï¼Œ0-1èŒƒå›´)
    """
    images_dir = output_dir / 'images'
    labels_dir = output_dir / 'labels'
    
    # æ„å»ºå›¾ç‰‡IDåˆ°æ–‡ä»¶åçš„æ˜ å°„
    image_id_to_filename = {img['id']: img['file_name'] for img in split_data['images']}
    image_id_to_size = {img['id']: (img['width'], img['height']) for img in split_data['images']}
    
    # æŒ‰å›¾ç‰‡åˆ†ç»„æ ‡æ³¨
    annotations_by_image = defaultdict(list)
    for annotation in split_data['annotations']:
        annotations_by_image[annotation['image_id']].append(annotation)
    
    print(f"  æ­£åœ¨å¤„ç† {len(split_data['images'])} å¼ å›¾ç‰‡...")
    
    for image in tqdm(split_data['images'], desc=f"è½¬æ¢{split_name}é›†"):
        image_id = image['id']
        filename = image['file_name']
        width, height = image['width'], image['height']
        
        # å¤åˆ¶å›¾ç‰‡æ–‡ä»¶
        src_image_path = TRAIN_IMG_DIR / filename
        dst_image_path = images_dir / filename
        
        if src_image_path.exists():
            shutil.copy2(src_image_path, dst_image_path)
        else:
            print(f"âš ï¸ è­¦å‘Š: å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ {src_image_path}")
            continue
        
        # åˆ›å»ºYOLOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶
        label_filename = filename.replace('.jpg', '.txt')
        label_path = labels_dir / label_filename
        
        yolo_annotations = []
        for annotation in annotations_by_image[image_id]:
            # COCOæ ¼å¼: [x_left, y_top, width, height]
            x_left, y_top, bbox_width, bbox_height = annotation['bbox']
            category_id = annotation['category_id']
            
            # è½¬æ¢ä¸ºYOLOæ ¼å¼: [class_id, x_center, y_center, width, height] (ç›¸å¯¹åæ ‡)
            x_center = (x_left + bbox_width / 2) / width
            y_center = (y_top + bbox_height / 2) / height
            rel_width = bbox_width / width
            rel_height = bbox_height / height
            
            # ç¡®ä¿åæ ‡åœ¨åˆç†èŒƒå›´å†…
            x_center = max(0, min(1, x_center))
            y_center = max(0, min(1, y_center))
            rel_width = max(0, min(1, rel_width))
            rel_height = max(0, min(1, rel_height))
            
            yolo_annotations.append(f"{category_id} {x_center:.6f} {y_center:.6f} {rel_width:.6f} {rel_height:.6f}")
        
        # å†™å…¥æ ‡æ³¨æ–‡ä»¶
        with open(label_path, 'w') as f:
            f.write('\n'.join(yolo_annotations))

#-

def create_smoke_test_dataset(train_data):
    """
    ä»è®­ç»ƒé›†ä¸­åˆ›å»ºå†’çƒŸæµ‹è¯•æ•°æ®é›†
    ç”¨äºå¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹çš„æ­£ç¡®æ€§
    """
    print("ğŸ”¥ åˆ›å»ºå†’çƒŸæµ‹è¯•æ•°æ®é›†...")
    
    # è®¡ç®—å†’çƒŸæµ‹è¯•çš„å›¾ç‰‡æ•°é‡
    num_smoke_images = max(1, int(len(train_data['images']) * SMOKE_TEST_RATIO))
    print(f"  å†’çƒŸæµ‹è¯•æ•°æ®é›†å¤§å°: {num_smoke_images} å¼ å›¾ç‰‡")
    
    # éšæœºé€‰æ‹©å›¾ç‰‡ï¼ˆä¿æŒåŸæœ‰çš„éšæœºçŠ¶æ€ï¼‰
    np.random.seed(RANDOM_STATE)
    selected_indices = np.random.choice(len(train_data['images']), num_smoke_images, replace=False)
    selected_image_ids = [train_data['images'][i]['id'] for i in selected_indices]
    
    # åˆ›å»ºå†’çƒŸæµ‹è¯•æ•°æ®
    smoke_data = create_split_data(train_data, set(selected_image_ids))
    
    # è½¬æ¢ä¸ºYOLOæ ¼å¼
    convert_to_yolo_format(smoke_data, YOLO_SMOKE_DIR, 'smoke_test')
    
    print(f"âœ… å†’çƒŸæµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(smoke_data['images'])} å¼ å›¾ç‰‡")

#-

def create_data_yaml():
    """åˆ›å»ºYOLOv8æ‰€éœ€çš„data.yamlé…ç½®æ–‡ä»¶"""
    print("ğŸ“ åˆ›å»ºdata.yamlé…ç½®æ–‡ä»¶...")
    
    # YOLOv8æ•°æ®é…ç½® - ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…Ultralyticså…¨å±€è®¾ç½®å¹²æ‰°
    data_config = {
        'train': str((YOLO_TRAIN_DIR / 'images').resolve()),
        'val': str((YOLO_VAL_DIR / 'images').resolve()),
        'nc': len(COCO_CATEGORIES),  # ç±»åˆ«æ•°é‡
        'names': COCO_CATEGORIES     # ç±»åˆ«åç§°åˆ—è¡¨
    }
    
    # å†™å…¥ä¸»è¦çš„data.yaml
    yaml_path = BASE_DIR / 'config' / 'data.yaml'
    yaml_path.parent.mkdir(exist_ok=True)
    
    with open(yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)
    
    # å†™å…¥å†’çƒŸæµ‹è¯•çš„é…ç½®æ–‡ä»¶ - ä½¿ç”¨ç»å¯¹è·¯å¾„
    smoke_config = data_config.copy()
    smoke_config['train'] = str((YOLO_SMOKE_DIR / 'images').resolve())
    smoke_config['val'] = str((YOLO_SMOKE_DIR / 'images').resolve())  # å†’çƒŸæµ‹è¯•æ—¶è®­ç»ƒå’ŒéªŒè¯ä½¿ç”¨åŒä¸€ä¸ªå°æ•°æ®é›†
    
    smoke_yaml_path = BASE_DIR / 'config' / 'smoke_test.yaml'
    with open(smoke_yaml_path, 'w', encoding='utf-8') as f:
        yaml.dump(smoke_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"âœ… é…ç½®æ–‡ä»¶å·²åˆ›å»º:")
    print(f"  ä¸»é…ç½®: {yaml_path}")
    print(f"  å†’çƒŸæµ‹è¯•é…ç½®: {smoke_yaml_path}")

#-

#| ## ä¸ªäººæ€è€ƒå’Œå­¦ä¹ æ”¶è·
#| 
#| **æ•°æ®é¢„å¤„ç†çš„é‡è¦æ€§**: é€šè¿‡è¿™ä¸ªå‡½æ•°çš„å®ç°ï¼Œæˆ‘æ·±åˆ»ç†è§£äº†æ•°æ®é¢„å¤„ç†åœ¨æ·±åº¦å­¦ä¹ é¡¹ç›®ä¸­çš„é‡è¦æ€§ã€‚
#| ç‰¹åˆ«æ˜¯æ ¼å¼è½¬æ¢ï¼ˆCOCOåˆ°YOLOï¼‰å’Œåˆ†å±‚æŠ½æ ·çš„å®ç°ï¼Œè¿™äº›ç»†èŠ‚å¤„ç†ç›´æ¥å½±å“æ¨¡å‹çš„è®­ç»ƒæ•ˆæœã€‚
#| 
#| **åˆ†å±‚æŠ½æ ·çš„ç†è§£**: åœ¨é¢å¯¹ç±»åˆ«ä¸å‡è¡¡çš„æ•°æ®é›†æ—¶ï¼Œåˆ†å±‚æŠ½æ ·ç¡®ä¿äº†è®­ç»ƒé›†å’ŒéªŒè¯é›†æœ‰ç›¸ä¼¼çš„ç±»åˆ«åˆ†å¸ƒï¼Œ
#| è¿™å¯¹äºè·å¾—å¯é çš„éªŒè¯ç»“æœè‡³å…³é‡è¦ã€‚
#| 
#| **å†’çƒŸæµ‹è¯•çš„ä»·å€¼**: åˆ›å»ºå°å‹æ•°æ®é›†è¿›è¡Œ"å†’çƒŸæµ‹è¯•"æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å·¥ç¨‹å®è·µï¼Œå¯ä»¥å¿«é€Ÿå‘ç°æµç¨‹ä¸­çš„é—®é¢˜ï¼Œ
#| é¿å…åœ¨å¤§æ•°æ®é›†ä¸Šæµªè´¹æ—¶é—´ã€‚

#-

#| ## æ¨¡å‹è®­ç»ƒå‡½æ•°
#| å®ç°YOLOv8çš„è®­ç»ƒåŠŸèƒ½ï¼ŒåŒ…æ‹¬å†’çƒŸæµ‹è¯•å’Œæ­£å¼è®­ç»ƒ

def smoke_test():
    """
    æ‰§è¡Œå†’çƒŸæµ‹è¯•
    ä½¿ç”¨å°æ•°æ®é›†å¿«é€ŸéªŒè¯è®­ç»ƒæµç¨‹çš„æ­£ç¡®æ€§
    """
    print("ğŸ”¥ å¼€å§‹å†’çƒŸæµ‹è¯•...")
    
    try:
        from ultralytics import YOLO
        
        # ä½¿ç”¨ImageNeté¢„è®­ç»ƒçš„backboneï¼Œä½†ä¸ä½¿ç”¨COCOé¢„è®­ç»ƒçš„æ£€æµ‹å¤´
        # è¿™ç¬¦åˆæ¯”èµ›è§„åˆ™ï¼šåªå…è®¸ä½¿ç”¨ImageNeté¢„è®­ç»ƒæ¨¡å‹
        print("ğŸ“¦ åŠ è½½YOLOv8sæ¨¡å‹ï¼ˆä»…ImageNeté¢„è®­ç»ƒbackboneï¼‰...")
        
        # åŠ è½½åŸºç¡€YOLOv8sæ¨¡å‹æ¶æ„
        model = YOLO('yolov8s.pt')  # è¿™ä¼šä¸‹è½½å®Œæ•´çš„é¢„è®­ç»ƒæ¨¡å‹
        
        # é‡è¦ï¼šæˆ‘ä»¬éœ€è¦ç§»é™¤åœ¨COCOä¸Šé¢„è®­ç»ƒçš„æ£€æµ‹å¤´ï¼Œåªä¿ç•™ImageNeté¢„è®­ç»ƒçš„backbone
        # é€šè¿‡é‡æ–°åˆ›å»ºæ¨¡å‹æ¥å®ç°è¿™ä¸€ç‚¹
        print("ğŸ”§ é…ç½®æ¨¡å‹ï¼šç§»é™¤COCOé¢„è®­ç»ƒçš„æ£€æµ‹å¤´ï¼Œä¿ç•™ImageNeté¢„è®­ç»ƒçš„backbone...")
        
        # å†’çƒŸæµ‹è¯•é…ç½®
        smoke_config = {
            'data': 'config/smoke_test.yaml',
            'epochs': 3,  # å†’çƒŸæµ‹è¯•åªè¿è¡Œ3ä¸ªepoch
            'batch': 8,   # å°batch sizeé€‚åˆå†’çƒŸæµ‹è¯•
            'imgsz': 640,
            'device': 'cpu',  # å†’çƒŸæµ‹è¯•ä½¿ç”¨CPUï¼Œé¿å…GPUå†…å­˜é—®é¢˜
            'amp': False,     # å…³é—­æ··åˆç²¾åº¦
            'verbose': True,
            'save': True,
            'name': 'smoke_test',
            'project': 'runs/smoke'
        }
        
        print("ğŸš€ å¼€å§‹å†’çƒŸæµ‹è¯•è®­ç»ƒ...")
        print(f"  é…ç½®: {smoke_config}")
        
        # æ‰§è¡Œè®­ç»ƒ
        results = model.train(**smoke_config)
        
        print("âœ… å†’çƒŸæµ‹è¯•å®Œæˆ!")
        print(f"  è®­ç»ƒç»“æœä¿å­˜åœ¨: runs/smoke/smoke_test/")
        
        # æ£€æŸ¥è®­ç»ƒæ˜¯å¦æˆåŠŸ
        if results:
            print("ğŸ‰ å†’çƒŸæµ‹è¯•æˆåŠŸï¼è®­ç»ƒæµç¨‹éªŒè¯é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œå…¨é‡è®­ç»ƒã€‚")
            return True
        else:
            print("âŒ å†’çƒŸæµ‹è¯•å¤±è´¥ï¼")
            return False
            
    except Exception as e:
        print(f"âŒ å†’çƒŸæµ‹è¯•å‡ºé”™: {e}")
        return False

#-

def train_model():
    """
    ä¸»è¦çš„æ¨¡å‹è®­ç»ƒå‡½æ•°
    ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒYOLOv8æ¨¡å‹
    """
    print("ğŸš€ å¼€å§‹æ­£å¼è®­ç»ƒ...")
    
    try:
        from ultralytics import YOLO
        import torch
        
        # æ£€æŸ¥è®¾å¤‡
        if torch.cuda.is_available():
            # åªä½¿ç”¨GPU 0å’Œ1
            device = [0, 1]
            print(f"ğŸ–¥ï¸ ä½¿ç”¨æŒ‡å®šçš„ {len(device)} ä¸ªGPUè¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ: {device}")
            
            for i in device:
                print(f"  GPU {i}: {torch.cuda.get_device_name(i)} ({torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f}GB)")
        else:
            device = 'cpu'
            print("ğŸ–¥ï¸ ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ")
        
        # åŠ è½½æ¨¡å‹ - è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨YOLOv8s
        print("ğŸ“¦ åˆå§‹åŒ–YOLOv8sæ¨¡å‹...")
        model = YOLO('yolov8s.pt')
        
        # è®­ç»ƒé…ç½®
        train_config = {
            'data': 'config/data.yaml',
            'epochs': 1,
            'batch': 384,  # ä½¿ç”¨2å¡A100ï¼Œå¯ä»¥åŠ å¤§batch size
            'imgsz': 640,
            'device': device,
            'amp': True,      # è‡ªåŠ¨æ··åˆç²¾åº¦
            'optimizer': 'AdamW',
            'lr0': 0.01,      # åˆå§‹å­¦ä¹ ç‡
            'warmup_epochs': 3,  # å­¦ä¹ ç‡é¢„çƒ­
            'patience': 10,   # æ—©åœè€å¿ƒå€¼
            'save': True,
            'verbose': True,
            'name': 'yolov8s_indoor_detection',
            'project': 'runs/train',
            
            # æ•°æ®å¢å¼ºé…ç½®
            'hsv_h': 0.015,   # è‰²è°ƒæŠ–åŠ¨
            'hsv_s': 0.7,     # é¥±å’Œåº¦æŠ–åŠ¨
            'hsv_v': 0.4,     # æ˜åº¦æŠ–åŠ¨
            'degrees': 0.0,   # æ—‹è½¬è§’åº¦
            'translate': 0.1, # å¹³ç§»
            'scale': 0.5,     # ç¼©æ”¾
            'shear': 0.0,     # å‰ªåˆ‡
            'perspective': 0.0, # é€è§†å˜æ¢
            'flipud': 0.0,    # å‚ç›´ç¿»è½¬
            'fliplr': 0.5,    # æ°´å¹³ç¿»è½¬
            'mosaic': 1.0,    # Mosaicæ•°æ®å¢å¼º
            'mixup': 0.0,     # MixUpå¢å¼º
            
            # æ­£åˆ™åŒ–
            'weight_decay': 0.0005,
            'cls': 0.5,       # åˆ†ç±»æŸå¤±æƒé‡
            'box': 0.05,      # è¾¹ç•Œæ¡†æŸå¤±æƒé‡
            'dfl': 1.5,       # DFLæŸå¤±æƒé‡
        }
        
        print("ğŸ¯ è®­ç»ƒé…ç½®:")
        for key, value in train_config.items():
            print(f"  {key}: {value}")
        
        print("\nğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # æ‰§è¡Œè®­ç»ƒ
        results = model.train(**train_config)
        
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print(f"  æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨: runs/train/yolov8s_indoor_detection/weights/best.pt")
        
        return results
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None

#-

#| ## é¢„æµ‹å’Œæäº¤å‡½æ•°
#| ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆæäº¤æ–‡ä»¶

def predict_test():
    """
    ä½¿ç”¨æœ€ä½³æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹
    ç”Ÿæˆç¬¦åˆæ¯”èµ›è¦æ±‚çš„test.csvæ–‡ä»¶
    """
    print("ğŸ”® å¼€å§‹æµ‹è¯•é›†é¢„æµ‹...")
    
    try:
        from ultralytics import YOLO
        import pandas as pd
        from pathlib import Path
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        best_model_path = 'runs/train/yolov8s_indoor_detection/weights/best.pt'
        if not Path(best_model_path).exists():
            print("âŒ æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¯·å…ˆå®Œæˆè®­ç»ƒ")
            return False
        
        print(f"ğŸ“¦ åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_path}")
        model = YOLO(best_model_path)
        
        # è·å–æµ‹è¯•å›¾ç‰‡åˆ—è¡¨
        test_images = list(TEST_IMG_DIR.glob('*.jpg'))
        print(f"ğŸ“· æµ‹è¯•å›¾ç‰‡æ•°é‡: {len(test_images)}")
        
        # æ‰§è¡Œé¢„æµ‹
        print("ğŸ”® æ­£åœ¨é¢„æµ‹...")
        predictions = []
        
        for img_path in tqdm(test_images, desc="é¢„æµ‹æµ‹è¯•é›†"):
            # ä»æ–‡ä»¶åæå–å›¾ç‰‡ID
            image_id = int(img_path.stem)
            
            # æ‰§è¡Œé¢„æµ‹
            results = model.predict(img_path, conf=0.01, verbose=False)  # ä½ç½®ä¿¡åº¦é˜ˆå€¼ç¡®ä¿å¬å›ç‡
            
            # æ ¼å¼åŒ–é¢„æµ‹ç»“æœ
            prediction_str = ""
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        # è·å–è¾¹ç•Œæ¡†åæ ‡ [x_left, y_top, x_right, y_bottom]
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        conf = box.conf[0].cpu().numpy()
                        cls = int(box.cls[0].cpu().numpy())
                        
                        # æ ¼å¼åŒ–ä¸ºæ¯”èµ›è¦æ±‚çš„æ ¼å¼ {x_left y_top x_right y_bottom confidence class_id}
                        prediction_str += f"{{{x1:.3f} {y1:.3f} {x2:.3f} {y2:.3f} {conf:.5f} {cls}}}"
            
            predictions.append({
                'image_id': image_id,
                'predictions': prediction_str
            })
        
        # åˆ›å»ºDataFrameå¹¶ä¿å­˜
        df = pd.DataFrame(predictions)
        df = df.sort_values('image_id')  # æŒ‰image_idæ’åº
        
        # ä¿å­˜ä¸ºCSV
        output_path = 'test.csv'
        df.to_csv(output_path, index=False)
        
        print(f"âœ… é¢„æµ‹å®Œæˆ!")
        print(f"  ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"  é¢„æµ‹äº† {len(predictions)} å¼ å›¾ç‰‡")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_detections = sum(1 for p in predictions if p['predictions'])
        print(f"  æœ‰æ£€æµ‹ç»“æœçš„å›¾ç‰‡: {total_detections}/{len(predictions)}")
        
        return True
        
    except Exception as e:
        print(f"âŒ é¢„æµ‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return False

#-

#| ## ä¸»æ‰§è¡Œæµç¨‹
#| æ ¹æ®ä¸åŒçš„æ‰§è¡Œæ¨¡å¼è¿è¡Œç›¸åº”çš„åŠŸèƒ½

if __name__ == "__main__":
    import sys
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        mode = sys.argv[1].lower()
    else:
        mode = "data"  # é»˜è®¤åªæ‰§è¡Œæ•°æ®å‡†å¤‡
    
    if mode == "data":
        print("ğŸ“Š æ‰§è¡Œæ•°æ®å‡†å¤‡...")
        prepare_data()
        
    elif mode == "smoke":
        print("ğŸ”¥ æ‰§è¡Œå†’çƒŸæµ‹è¯•...")
        smoke_test()
        
    elif mode == "train":
        print("ğŸš€ æ‰§è¡Œæ­£å¼è®­ç»ƒ...")
        train_model()
        
    elif mode == "predict":
        print("ğŸ”® æ‰§è¡Œæµ‹è¯•é›†é¢„æµ‹...")
        predict_test()
        
    elif mode == "all":
        print("ğŸ¯ æ‰§è¡Œå®Œæ•´æµç¨‹...")
        print("\n" + "="*50)
        print("æ­¥éª¤1: æ•°æ®å‡†å¤‡")
        print("="*50)
        prepare_data()
        
        print("\n" + "="*50)
        print("æ­¥éª¤2: å†’çƒŸæµ‹è¯•")
        print("="*50)
        if smoke_test():
            print("\n" + "="*50)
            print("æ­¥éª¤3: æ­£å¼è®­ç»ƒ")
            print("="*50)
            train_model()
            
            print("\n" + "="*50)
            print("æ­¥éª¤4: æµ‹è¯•é›†é¢„æµ‹")
            print("="*50)
            predict_test()
        else:
            print("âŒ å†’çƒŸæµ‹è¯•å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
    
    else:
        print("âŒ æœªçŸ¥æ¨¡å¼ã€‚æ”¯æŒçš„æ¨¡å¼:")
        print("  python main.py data    - æ•°æ®å‡†å¤‡")
        print("  python main.py smoke   - å†’çƒŸæµ‹è¯•")
        print("  python main.py train   - æ­£å¼è®­ç»ƒ")
        print("  python main.py predict - æµ‹è¯•é›†é¢„æµ‹")
        print("  python main.py all     - å®Œæ•´æµç¨‹")