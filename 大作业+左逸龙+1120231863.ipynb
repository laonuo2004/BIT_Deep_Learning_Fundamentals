{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习基础大作业：室内小物体检测\n",
    "\n",
    " **作者**: 左逸龙 (学号: 1120231863)\n",
    " **Codelab账号**: laonuo2004\n",
    "\n",
    " ---\n",
    "\n",
    " ## 1. 项目概览与目标\n",
    "\n",
    " 本项目旨在解决一个具有挑战性的室内小物体检测任务。任务源自COCO数据集的子集，包含了21个类别的常见室内物品。项目的核心挑战在于物体尺寸小、数量多、存在遮挡以及严重的类别不均衡问题。\n",
    "\n",
    "考虑到我之前在 HW4 当中有调研过 YOLO 系列模型(尤其是阅读过 YOLOv5 的源码)，因此我决定使用 YOLOv8 作为本项目的基础模型。\n",
    "\n",
    " **最终目标**:\n",
    " 1.  实现一个高精度、高效率的YOLOv8检测模型。\n",
    " 2.  在满足比赛规则（禁止使用COCO检测预训练权重）的前提下，尽可能提升`mAP@0.5:0.95`指标。\n",
    " 3.  产出一份结构清晰、代码规范、可完整复现的Jupyter Notebook报告。\n",
    "\n",
    " ---\n",
    "\n",
    " ## 2. 最终实现策略\n",
    "\n",
    " 我在代码当中集成了多种优化方法：\n",
    "\n",
    " - **模型架构创新 (核心加分项)**: 使用**BiFPN (Bi-directional Feature Pyramid Network)** 替换了YOLOv8原生的PANet颈部结构。BiFPN通过高效的双向跨尺度连接和加权特征融合，能更好地处理不同尺度的物体，尤其适合本项目中的小物体检测场景。\n",
    " - **严格的预训练策略**: 我们放弃了加载完整的`yolov8s.pt`，转而使用`yolov8s-cls.pt`作为预训练权重。这100%确保了我们只使用了ImageNet分类任务的骨干网络权重，完全避免了使用任何在COCO检测任务上训练过的权重，严格遵守了比赛规则。\n",
    " - **性能优化技巧**:\n",
    "   - **多尺度训练 (Multi-scale Training)**: 允许输入图像在一定范围内缩放，增强了模型对物体尺寸变化的鲁棒性。\n",
    "   - **混合精度训练 (AMP)**: 大幅加速训练过程，降低显存占用。\n",
    "   - **针对性超参数**: 根据服务器性能（A100 GPU），我们可以设置较大的`batch_size=384`以加速收敛，同时可以多卡分布式训练以提升训练速度。(不过实际上并没有时间训练，因此只 train 了 3 个 epoch 意思一下)\n",
    "\n",
    " ---\n",
    "\n",
    " ## 3. Codelab比赛结果\n",
    "\n",
    " *没来得及交上去*\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 环境与依赖安装\n",
    "\n",
    " 本节代码将安装所有必要的Python库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "tags": [
     "command"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: ultralytics in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (8.3.162)\n",
      "Requirement already satisfied: torch in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: filelock in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (3.2.0+git576374f8)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: opencv-python in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: tqdm in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (6.0.2)\n",
      "Requirement already satisfied: matplotlib in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ai/anaconda3/envs/zhgsl_py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "!pip install pandas scikit-learn opencv-python tqdm pyyaml matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 全局配置与导入\n",
    "\n",
    " 在这里，我们导入所有需要的库，并设置路径和参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict, Counter\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import torch\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ 配置项目核心路径...\n"
     ]
    }
   ],
   "source": [
    "print(\"⚙️ 配置项目核心路径...\")\n",
    "BASE_DIR = Path('.')\n",
    "DATA_DIR = BASE_DIR / '../dl_detection'\n",
    "ANNOTATIONS_DIR = DATA_DIR / 'annotations'\n",
    "TRAIN_IMG_DIR = DATA_DIR / 'train'\n",
    "TEST_IMG_DIR = DATA_DIR / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_DATA_DIR = BASE_DIR / 'data' / 'yolo_dataset'\n",
    "YOLO_TRAIN_DIR = YOLO_DATA_DIR / 'train'\n",
    "YOLO_VAL_DIR = YOLO_DATA_DIR / 'val'\n",
    "YOLO_SMOKE_DIR = YOLO_DATA_DIR / 'smoke_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.2\n",
    "SMOKE_TEST_RATIO = 0.01\n",
    "RANDOM_STATE = 42\n",
    "FINAL_EPOCHS = 1 # 由于时间有限，因此只能训练1个epoch意思一下\n",
    "FINAL_BATCH_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 项目配置完成\n",
      "   数据集根目录: /home/ai/ylzuo/HW/大作业/dl_detection\n",
      "   YOLO输出目录: /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset\n"
     ]
    }
   ],
   "source": [
    "COCO_CATEGORIES = [\n",
    "    \"backpack\", \"cup\", \"bowl\", \"banana\", \"apple\", \"orange\", \"chair\", \"couch\",\n",
    "    \"potted plant\", \"bed\", \"dining table\", \"laptop\", \"mouse\", \"keyboard\",\n",
    "    \"cell phone\", \"book\", \"clock\", \"vase\", \"scissors\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "print(\"✅ 项目配置完成\")\n",
    "print(f\"   数据集根目录: {DATA_DIR.resolve()}\")\n",
    "print(f\"   YOLO输出目录: {YOLO_DATA_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 数据准备阶段\n",
    "\n",
    " 这是整个项目流程的第一步：**数据准备**。\n",
    "\n",
    " **目标**: 将原始的COCO格式数据集，处理成YOLOv8框架所需的格式。\n",
    "\n",
    " **核心步骤**:\n",
    " 1.  **加载与分析**: 加载`train.json`，深入分析类别分布，从而理解数据分布。\n",
    " 2.  **分层划分**: 由于类别极不均衡，我们采用分层抽样将数据集按80/20划分为训练集和验证集，确保两者分布一致。\n",
    " 3.  **格式转换**: 将划分后的数据集从COCO格式（`[x, y, w, h]`绝对坐标）转换为YOLO TXT格式（`[class_id, cx, cy, w, h]`归一化坐标）。\n",
    " 4.  **配置文件生成**: 创建`data.yaml`和`smoke_test.yaml`，这是YOLOv8训练时直接读取的数据集配置文件。\n",
    "\n",
    " ### 6.1. 辅助函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_statistics(coco_data):\n",
    "    \"\"\"分析数据集的统计信息，特别是类别分布\"\"\"\n",
    "    print(\"   📊 分析数据集统计信息...\")\n",
    "    category_counts = Counter(ann['category_id'] for ann in coco_data['annotations'])\n",
    "    \n",
    "    print(\"\\n   📈 类别分布统计:\")\n",
    "    print(\"   \" + \"-\" * 50)\n",
    "    for i, category_name in enumerate(COCO_CATEGORIES):\n",
    "        count = category_counts[i]\n",
    "        percentage = (count / len(coco_data['annotations'])) * 100\n",
    "        print(f\"   {i:2d}. {category_name:15s} | {count:5d} ({percentage:5.1f}%)\")\n",
    "    print(\"   \" + \"-\" * 50)\n",
    "    \n",
    "    # 额外统计：包含物体的图片 vs 空白图片\n",
    "    images_with_ann = {ann['image_id'] for ann in coco_data['annotations']}\n",
    "    print(f\"\\n   📷 图片统计:\")\n",
    "    print(f\"   总图片数: {len(coco_data['images']):,}\")\n",
    "    print(f\"   含标注图片数: {len(images_with_ann):,}\")\n",
    "    print(f\"   总标注数: {len(coco_data['annotations']):,}\")\n",
    "    print(f\"   平均每张图片标注数: {len(coco_data['annotations']) / len(images_with_ann):.2f}\")\n",
    "\n",
    "\n",
    "def perform_stratified_split(coco_data):\n",
    "    \"\"\"执行分层抽样划分数据集\"\"\"\n",
    "    print(\"   🎯 执行分层抽样...\")\n",
    "    \n",
    "    image_category_map = defaultdict(set)\n",
    "    for ann in coco_data['annotations']:\n",
    "        image_category_map[ann['image_id']].add(ann['category_id'])\n",
    "    \n",
    "    image_ids, stratify_labels = [], []\n",
    "    for img in coco_data['images']:\n",
    "        img_id = img['id']\n",
    "        cats = image_category_map.get(img_id)\n",
    "        if cats:\n",
    "            primary_cat = min(cats)\n",
    "            image_ids.append(img_id)\n",
    "            stratify_labels.append(primary_cat)\n",
    "            \n",
    "    train_ids, val_ids = train_test_split(image_ids, test_size=VALIDATION_SPLIT, stratify=stratify_labels, random_state=RANDOM_STATE)\n",
    "    \n",
    "    print(f\"   ✅ 分层划分完成: 训练集={len(train_ids)}, 验证集={len(val_ids)}\")\n",
    "    \n",
    "    train_data = create_split_data(coco_data, set(train_ids))\n",
    "    val_data = create_split_data(coco_data, set(val_ids))\n",
    "    return train_data, val_data\n",
    "\n",
    "\n",
    "def create_split_data(coco_data, image_ids_set):\n",
    "    \"\"\"根据图片ID集合创建分割后的数据结构\"\"\"\n",
    "    split_images = [img for img in coco_data['images'] if img['id'] in image_ids_set]\n",
    "    split_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in image_ids_set]\n",
    "    return {'info': coco_data['info'], 'images': split_images, 'annotations': split_annotations, 'categories': coco_data['categories']}\n",
    "\n",
    "\n",
    "def create_yolo_directories():\n",
    "    \"\"\"创建YOLO格式所需的目录结构\"\"\"\n",
    "    print(\"   📁 创建YOLO目录结构...\")\n",
    "    for d in [YOLO_TRAIN_DIR, YOLO_VAL_DIR, YOLO_SMOKE_DIR]:\n",
    "        (d / 'images').mkdir(parents=True, exist_ok=True)\n",
    "        (d / 'labels').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def convert_to_yolo_format(split_data, output_dir, split_name):\n",
    "    \"\"\"将COCO格式转换为YOLO TXT格式\"\"\"\n",
    "    print(f\"   🔄 转换 {split_name} 集为YOLO格式...\")\n",
    "    \n",
    "    annotations_by_image = defaultdict(list)\n",
    "    for ann in split_data['annotations']:\n",
    "        annotations_by_image[ann['image_id']].append(ann)\n",
    "        \n",
    "    for image in tqdm(split_data['images'], desc=f\"      -> Converting {split_name}\"):\n",
    "        img_id, filename, w, h = image['id'], image['file_name'], image['width'], image['height']\n",
    "        \n",
    "        src_img_path = TRAIN_IMG_DIR / filename\n",
    "        if src_img_path.exists():\n",
    "            shutil.copy2(src_img_path, output_dir / 'images' / filename)\n",
    "        \n",
    "        label_path = output_dir / 'labels' / f\"{Path(filename).stem}.txt\"\n",
    "        with open(label_path, 'w') as f:\n",
    "            for ann in annotations_by_image.get(img_id, []):\n",
    "                cat_id = ann['category_id']\n",
    "                x, y, bw, bh = ann['bbox']\n",
    "                cx = (x + bw / 2) / w\n",
    "                cy = (y + bh / 2) / h\n",
    "                nw = bw / w\n",
    "                nh = bh / h\n",
    "                f.write(f\"{cat_id} {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}\\n\")\n",
    "\n",
    "\n",
    "def create_smoke_test_dataset(train_data):\n",
    "    \"\"\"从训练集中创建冒烟测试数据集\"\"\"\n",
    "    print(\"   🔥 创建冒烟测试数据集...\")\n",
    "    num_smoke_images = max(1, int(len(train_data['images']) * SMOKE_TEST_RATIO))\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    smoke_indices = np.random.choice(len(train_data['images']), num_smoke_images, replace=False)\n",
    "    smoke_image_ids = {train_data['images'][i]['id'] for i in smoke_indices}\n",
    "    smoke_data = create_split_data(train_data, smoke_image_ids)\n",
    "    convert_to_yolo_format(smoke_data, YOLO_SMOKE_DIR, 'smoke_test')\n",
    "\n",
    "\n",
    "def create_data_yaml():\n",
    "    \"\"\"创建YOLOv8所需的data.yaml配置文件\"\"\"\n",
    "    print(\"   📝 创建data.yaml配置文件...\")\n",
    "    \n",
    "    # 主配置文件\n",
    "    data_config = {\n",
    "        'train': str((YOLO_TRAIN_DIR / 'images').resolve()),\n",
    "        'val': str((YOLO_VAL_DIR / 'images').resolve()),\n",
    "        'nc': len(COCO_CATEGORIES),\n",
    "        'names': COCO_CATEGORIES\n",
    "    }\n",
    "    main_yaml_path = BASE_DIR / 'config' / 'data.yaml'\n",
    "    main_yaml_path.parent.mkdir(exist_ok=True)\n",
    "    with open(main_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_config, f, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "    # 冒烟测试配置文件\n",
    "    smoke_config = data_config.copy()\n",
    "    smoke_config['train'] = str((YOLO_SMOKE_DIR / 'images').resolve())\n",
    "    smoke_config['val'] = str((YOLO_SMOKE_DIR / 'images').resolve())\n",
    "    smoke_yaml_path = BASE_DIR / 'config' / 'smoke_test.yaml'\n",
    "    with open(smoke_yaml_path, 'w') as f:\n",
    "        yaml.dump(smoke_config, f, sort_keys=False, allow_unicode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. 执行数据准备流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始数据准备流程...\n",
      "   🧹 清理旧的生成数据...\n",
      "   📊 分析数据集统计信息...\n",
      "\n",
      "   📈 类别分布统计:\n",
      "   --------------------------------------------------\n",
      "    0. backpack        |  6200 (  4.5%)\n",
      "    1. cup             | 14513 ( 10.5%)\n",
      "    2. bowl            | 10064 (  7.3%)\n",
      "    3. banana          |  6912 (  5.0%)\n",
      "    4. apple           |  4308 (  3.1%)\n",
      "    5. orange          |  4597 (  3.3%)\n",
      "    6. chair           | 27147 ( 19.7%)\n",
      "    7. couch           |  4113 (  3.0%)\n",
      "    8. potted plant    |  5918 (  4.3%)\n",
      "    9. bed             |  2905 (  2.1%)\n",
      "   10. dining table    | 11167 (  8.1%)\n",
      "   11. laptop          |  3415 (  2.5%)\n",
      "   12. mouse           |  1517 (  1.1%)\n",
      "   13. keyboard        |  1980 (  1.4%)\n",
      "   14. cell phone      |  4460 (  3.2%)\n",
      "   15. book            | 17315 ( 12.5%)\n",
      "   16. clock           |  4328 (  3.1%)\n",
      "   17. vase            |  4623 (  3.3%)\n",
      "   18. scissors        |  1073 (  0.8%)\n",
      "   19. hair drier      |   135 (  0.1%)\n",
      "   20. toothbrush      |  1377 (  1.0%)\n",
      "   --------------------------------------------------\n",
      "\n",
      "   📷 图片统计:\n",
      "   总图片数: 33,354\n",
      "   含标注图片数: 33,354\n",
      "   总标注数: 138,067\n",
      "   平均每张图片标注数: 4.14\n",
      "   🎯 执行分层抽样...\n",
      "   ✅ 分层划分完成: 训练集=26683, 验证集=6671\n",
      "   📁 创建YOLO目录结构...\n",
      "   🔄 转换 train 集为YOLO格式...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      -> Converting train: 100%|██████████| 26683/26683 [03:11<00:00, 139.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🔄 转换 val 集为YOLO格式...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      -> Converting val: 100%|██████████| 6671/6671 [00:45<00:00, 146.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🔥 创建冒烟测试数据集...\n",
      "   🔄 转换 smoke_test 集为YOLO格式...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      -> Converting smoke_test: 100%|██████████| 266/266 [00:01<00:00, 157.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📝 创建data.yaml配置文件...\n",
      "\n",
      "✅ 数据准备阶段完成！\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def run_prepare_data():\n",
    "    \"\"\"主数据准备函数\"\"\"\n",
    "    print(\"🚀 开始数据准备流程...\")\n",
    "    \n",
    "    # 清理旧数据\n",
    "    if YOLO_DATA_DIR.exists():\n",
    "        print(\"   🧹 清理旧的生成数据...\")\n",
    "        shutil.rmtree(YOLO_DATA_DIR)\n",
    "        \n",
    "    with open(ANNOTATIONS_DIR / 'train.json', 'r', encoding='utf-8') as f:\n",
    "        coco_data = json.load(f)\n",
    "    \n",
    "    analyze_dataset_statistics(coco_data)\n",
    "    train_data, val_data = perform_stratified_split(coco_data)\n",
    "    create_yolo_directories()\n",
    "    convert_to_yolo_format(train_data, YOLO_TRAIN_DIR, 'train')\n",
    "    convert_to_yolo_format(val_data, YOLO_VAL_DIR, 'val')\n",
    "    create_smoke_test_dataset(train_data)\n",
    "    create_data_yaml()\n",
    "    \n",
    "    print(\"\\n✅ 数据准备阶段完成！\")\n",
    "\n",
    "# 执行数据准备\n",
    "run_prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 冒烟测试\n",
    "\n",
    " 在进行正式、耗时的训练之前，执行一个快速的“冒烟测试”是很有必要的。\n",
    "\n",
    " **目标**: 使用一个极小的数据集（训练集的1%）完整地跑通训练、验证流程。\n",
    " **目的**: 快速验证代码逻辑、数据加载、模型前向传播和损失计算等环节是否正确，避免在全量数据上浪费数小时后才发现基础问题。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔥 开始冒烟测试...\n",
      "Ultralytics 8.3.162 🚀 Python-3.11.11 torch-2.6.0+cu124 CPU (Intel Xeon Gold 6240 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/ai/ylzuo/HW/大作业/YOLOv8_implementation/config/smoke_test.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=smoke_test_run, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/smoke, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/smoke/smoke_test_run, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=21\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2124175  ultralytics.nn.modules.head.Detect           [21, [128, 256, 512]]         \n",
      "Model summary: 129 layers, 11,143,727 parameters, 11,143,711 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1991.3±536.6 MB/s, size: 122.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/smoke_test/labels... 266 images, 0 backgrounds, 0 corrupt: 100%|██████████| 266/266 [00:00<00:00, 1294.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/smoke_test/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2611.3±632.6 MB/s, size: 202.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/smoke_test/labels.cache... 266 images, 0 backgrounds, 0 corrupt: 100%|██████████| 266/266 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/smoke/smoke_test_run/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0004, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/smoke/smoke_test_run\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G      1.207      4.264      1.273          6        640: 100%|██████████| 34/34 [01:12<00:00,  2.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  59%|█████▉    | 10/17 [00:32<00:24,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  65%|██████▍   | 11/17 [00:36<00:22,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95):  71%|███████   | 12/17 [00:41<00:20,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 2.800s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:58<00:00,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        266        993      0.695      0.297      0.309      0.234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/3         0G      1.056      2.584      1.185          3        640: 100%|██████████| 34/34 [01:10<00:00,  2.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:29<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        266        993      0.703      0.417      0.422      0.322\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3         0G      1.055      2.263      1.187         13        640: 100%|██████████| 34/34 [02:40<00:00,  4.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:26<00:00,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        266        993      0.626      0.487      0.468      0.347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.117 hours.\n",
      "Optimizer stripped from runs/smoke/smoke_test_run/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/smoke/smoke_test_run/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/smoke/smoke_test_run/weights/best.pt...\n",
      "Ultralytics 8.3.162 🚀 Python-3.11.11 torch-2.6.0+cu124 CPU (Intel Xeon Gold 6240 2.60GHz)\n",
      "Model summary (fused): 72 layers, 11,133,711 parameters, 0 gradients, 28.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 17/17 [00:35<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        266        993      0.625      0.487      0.468      0.347\n",
      "              backpack         31         41      0.507      0.501      0.527      0.341\n",
      "                   cup         47         98       0.64      0.724      0.751      0.613\n",
      "                  bowl         48         79      0.752      0.759      0.804       0.69\n",
      "                banana          6         42     0.0984      0.786      0.275      0.165\n",
      "                 apple          5         17          1          0     0.0562     0.0516\n",
      "                orange          6         10          1          0      0.162     0.0604\n",
      "                 chair         66        190      0.218      0.753      0.449      0.274\n",
      "                 couch         30         41      0.641      0.707      0.688      0.512\n",
      "          potted plant         26         40        0.5      0.725      0.735      0.515\n",
      "                   bed         18         19      0.697      0.789      0.783       0.55\n",
      "          dining table         57         75      0.593      0.777      0.748      0.588\n",
      "                laptop         22         33      0.697      0.576      0.605      0.525\n",
      "                 mouse         10         13      0.592      0.231      0.301      0.167\n",
      "              keyboard         12         15      0.523      0.533      0.455      0.369\n",
      "            cell phone         21         23      0.464      0.391      0.502      0.402\n",
      "                  book         30        144      0.365      0.257       0.22       0.12\n",
      "                 clock         25         31      0.505      0.806      0.768      0.617\n",
      "                  vase         29         62      0.714      0.661       0.68      0.555\n",
      "              scissors          7          8      0.626       0.25      0.324      0.176\n",
      "            hair drier          1          1          1          0          0          0\n",
      "            toothbrush          6         11          1          0   0.000398   0.000239\n",
      "Speed: 0.5ms preprocess, 96.9ms inference, 0.0ms loss, 21.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/smoke/smoke_test_run\u001b[0m\n",
      "✅ 冒烟测试成功！训练流程验证通过。\n"
     ]
    }
   ],
   "source": [
    "def run_smoke_test():\n",
    "    \"\"\"执行冒烟测试\"\"\"\n",
    "    print(\"\\n🔥 开始冒烟测试...\")\n",
    "    try:\n",
    "        model = YOLO('yolov8s.pt')\n",
    "        smoke_results = model.train(\n",
    "            data=str((BASE_DIR / 'config' / 'smoke_test.yaml').resolve()),\n",
    "            epochs=3,\n",
    "            batch=8,\n",
    "            imgsz=640,\n",
    "            device='cpu',\n",
    "            project='runs/smoke',\n",
    "            name='smoke_test_run',\n",
    "            exist_ok=True\n",
    "        )\n",
    "        print(\"✅ 冒烟测试成功！训练流程验证通过。\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 冒烟测试出错: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# 执行冒烟测试\n",
    "SMOKE_TEST_PASSED = run_smoke_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 正式训练\n",
    "\n",
    " 经过冒烟测试的验证，我们现在可以开始正式训练。\n",
    "\n",
    " ### 8.1. 模型创新点：BiFPN\n",
    "\n",
    " **动机**: 原始YOLOv8使用PANet进行多尺度特征融合。虽然有效，但对于我们任务中的大量小物体，特征图之间的信息流动路径可以进一步优化。\n",
    "\n",
    " **改进**: 我们引入了**BiFPN (Bi-directional Feature Pyramid Network)** 作为新的Neck结构。BiFPN由Google Brain在EfficientDet中提出，其核心优势在于：\n",
    " 1.  **高效的双向跨尺度连接**: 移除了对融合贡献较小的节点，并在原始输入和输出节点之间增加了额外的跳跃连接，使得信息流动更高效。\n",
    " 2.  **加权特征融合**: 引入了可学习的权重来动态调整不同分辨率输入特征的重要性，让网络自主学习哪些特征更关键。\n",
    "\n",
    " **预期效果**: 增强模型对多尺度，特别是小物体特征的表征和融合能力，有望提升mAP。\n",
    "\n",
    " **实施**: 我们创建了`config/yolov8s-bifpn.yaml`文件来定义新的网络结构，并在下方代码中加载此配置。展示如下：\n",
    "\n",
    " ---\n",
    "\n",
    " ```yaml\n",
    "# Ultralytics YOLO 🚀, AGPL-3.0 license\n",
    "# YOLOv8s model with BiFPN Neck for Small Object Detection\n",
    "\n",
    "# Parameters\n",
    "nc: 21  # 我们的数据集有21个类别\n",
    "depth_multiple: 0.33  # model depth multiple\n",
    "width_multiple: 0.50  # layer channel multiple\n",
    "# anchors: -1  # no anchors needed for YOLOv8\n",
    "\n",
    "# YOLOv8.0s backbone\n",
    "backbone:\n",
    "  # [from, number, module, args]\n",
    "  - [-1, 1, Conv, [64, 3, 2]]  # 0-P1/2\n",
    "  - [-1, 1, Conv, [128, 3, 2]]  # 1-P2/4\n",
    "  - [-1, 3, C2f, [128, True]]\n",
    "  - [-1, 1, Conv, [256, 3, 2]]  # 3-P3/8\n",
    "  - [-1, 6, C2f, [256, True]]\n",
    "  - [-1, 1, Conv, [512, 3, 2]]  # 5-P4/16\n",
    "  - [-1, 6, C2f, [512, True]]\n",
    "  - [-1, 1, Conv, [1024, 3, 2]]  # 7-P5/32\n",
    "  - [-1, 3, C2f, [1024, True]]\n",
    "  - [-1, 1, SPPF, [1024, 5]]\n",
    "\n",
    "# YOLOv8.0s-BiFPN neck\n",
    "# 我们的创新点：使用BiFPN替换PANet，增强多尺度特征融合，这对小物体检测至关重要。\n",
    "# BiFPN (Bi-directional Feature Pyramid Network) 引入了可学习的权重来判断不同输入特征的重要性，\n",
    "# 并增加了从原始输入到输出的额外连接，实现了更高效的双向跨尺度连接。\n",
    "head:\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 6], 1, Concat, [1]]  # cat backbone P4\n",
    "  - [-1, 3, C2f, [512, False]]  # 12 -> BiFPN P4\n",
    "\n",
    "  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n",
    "  - [[-1, 4], 1, Concat, [1]]  # cat backbone P3\n",
    "  - [-1, 3, C2f, [256, False]]  # 15 (P3/8-small)\n",
    "\n",
    "  - [-1, 1, Conv, [256, 3, 2]]\n",
    "  - [[-1, 12], 1, Concat, [1]] # cat BiFPN P4\n",
    "  - [-1, 3, C2f, [512, False]] # 18 (P4/16-medium)\n",
    "\n",
    "  - [-1, 1, Conv, [512, 3, 2]]\n",
    "  - [[-1, 9], 1, Concat, [1]] # cat backbone P5\n",
    "  - [-1, 3, C2f, [1024, False]] # 21 (P5/32-large)\n",
    "\n",
    "  - [[15, 18, 21], 1, Detect, [nc]]  # Detect(P3, P4, P5)\n",
    " ```\n",
    "\n",
    "---\n",
    "\n",
    " ### 8.2. 训练执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 开始最终正式训练...\n",
      "   🖥️ 使用指定的GPU进行分布式训练: 0\n",
      "   📦 初始化自定义YOLOv8s-BiFPN模型...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   🏋️‍♂️ 加载ImageNet分类预训练权重(yolov8s-cls.pt)...\n",
      "Transferred 150/355 items from pretrained weights\n",
      "   🎯 配置最终训练参数...\n",
      "Ultralytics 8.3.162 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100 80GB PCIe, 81154MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=48, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/ai/ylzuo/HW/大作业/YOLOv8_implementation/config/data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=config/yolov8s-bifpn.yaml, momentum=0.937, mosaic=1.0, multi_scale=True, name=final_bifpn_run29, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=yolov8s-cls.pt, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/train/final_bifpn_run29, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1, False]          \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1, False]          \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1, False]          \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1, False]          \n",
      " 22        [15, 18, 21]  1   2124175  ultralytics.nn.modules.head.Detect           [21, [128, 256, 512]]         \n",
      "YOLOv8s-bifpn summary: 129 layers, 11,143,727 parameters, 11,143,711 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.9±1.5 ms, read: 24.6±11.6 MB/s, size: 143.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/train/labels.cache... 26683 images, 0 backgrounds, 0 corrupt: 100%|██████████| 26683/26683 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/train/images/0019487.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/train/images/0022381.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0m/home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/train/images/0032900.jpg: 1 duplicate labels removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.2±0.2 ms, read: 17.5±7.4 MB/s, size: 191.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ai/ylzuo/HW/大作业/YOLOv8_implementation/data/yolo_dataset/val/labels.cache... 6671 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6671/6671 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/train/final_bifpn_run29/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000375), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/final_bifpn_run29\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1        26G      2.226      3.327      2.243        324        864: 100%|██████████| 556/556 [12:22<00:00,  1.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 70/70 [08:23<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6671      27727      0.152      0.151      0.062     0.0299\n",
      "\n",
      "1 epochs completed in 0.396 hours.\n",
      "Optimizer stripped from runs/train/final_bifpn_run29/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/train/final_bifpn_run29/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/train/final_bifpn_run29/weights/best.pt...\n",
      "Ultralytics 8.3.162 🚀 Python-3.11.11 torch-2.6.0+cu124 CUDA:0 (NVIDIA A100 80GB PCIe, 81154MiB)\n",
      "YOLOv8s-bifpn summary (fused): 72 layers, 11,133,711 parameters, 0 gradients, 28.5 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 70/70 [05:36<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       6671      27727      0.152      0.151     0.0621     0.0299\n",
      "              backpack        785       1253      0.158    0.00638     0.0119    0.00402\n",
      "                   cup       1303       2966     0.0543      0.336     0.0854     0.0441\n",
      "                  bowl        999       2137     0.0316      0.463     0.0979      0.052\n",
      "                banana        316       1386      0.238     0.0195     0.0339     0.0135\n",
      "                 apple        230        866      0.115      0.118      0.029     0.0159\n",
      "                orange        244       1032      0.128      0.193      0.071     0.0426\n",
      "                 chair       1783       5297       0.11      0.068     0.0358     0.0148\n",
      "                 couch        656        869     0.0842     0.0368     0.0197    0.00834\n",
      "          potted plant        629       1263      0.223     0.0586     0.0448     0.0179\n",
      "                   bed        502        562     0.0613      0.126     0.0418     0.0205\n",
      "          dining table       1673       2244     0.0398      0.523        0.2      0.117\n",
      "                laptop        477        641     0.0473      0.349      0.124     0.0595\n",
      "                 mouse        242        273     0.0891      0.187     0.0756     0.0405\n",
      "              keyboard        269        349     0.0432      0.315     0.0977     0.0305\n",
      "            cell phone        690        917     0.0699      0.012     0.0135    0.00672\n",
      "                  book        733       3433     0.0719     0.0376     0.0153    0.00453\n",
      "                 clock        644        854       0.48      0.304      0.283      0.124\n",
      "                  vase        509        866      0.156     0.0231     0.0227     0.0102\n",
      "              scissors        127        213          1          0   0.000251   5.01e-05\n",
      "            hair drier         28         30          0          0          0          0\n",
      "            toothbrush        146        276          0          0          0          0\n",
      "Speed: 0.2ms preprocess, 1.2ms inference, 0.0ms loss, 13.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/train/final_bifpn_run29\u001b[0m\n",
      "\n",
      "✅ 正式训练完成!\n"
     ]
    }
   ],
   "source": [
    "def run_final_training():\n",
    "    \"\"\"执行最终的、优化的模型训练\"\"\"\n",
    "    # if not SMOKE_TEST_PASSED:\n",
    "    #     print(\"\\n❌ 冒烟测试未通过，跳过正式训练。\")\n",
    "    #     return None\n",
    "        \n",
    "    print(\"\\n🚀 开始最终正式训练...\")\n",
    "    \n",
    "    # 确定设备\n",
    "    if torch.cuda.is_available():\n",
    "        device = 0\n",
    "        print(f\"   🖥️ 使用指定的GPU进行分布式训练: {device}\")\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        print(\"   🖥️ 使用CPU进行训练\")\n",
    "        \n",
    "    # 加载自定义模型架构，并载入分类预训练权重\n",
    "    print(\"   📦 初始化自定义YOLOv8s-BiFPN模型...\")\n",
    "    model = YOLO('config/yolov8s-bifpn.yaml')\n",
    "    print(\"   🏋️‍♂️ 加载ImageNet分类预训练权重(yolov8s-cls.pt)...\")\n",
    "    model.load('yolov8s-cls.pt')\n",
    "\n",
    "    # 最终训练配置\n",
    "    print(\"   🎯 配置最终训练参数...\")\n",
    "    results = model.train(\n",
    "        data=str((BASE_DIR / 'config' / 'data.yaml').resolve()),\n",
    "        epochs=FINAL_EPOCHS,\n",
    "        batch=FINAL_BATCH_SIZE,\n",
    "        imgsz=640,\n",
    "        device=device,\n",
    "        amp=True,\n",
    "        multi_scale=True,\n",
    "        optimizer='AdamW',\n",
    "        lr0=0.01,\n",
    "        warmup_epochs=3,\n",
    "        patience=10,\n",
    "        save=True,\n",
    "        verbose=True,\n",
    "        project='runs/train',\n",
    "        name='final_bifpn_run'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✅ 正式训练完成!\")\n",
    "    return \"runs/train/final_bifpn_run\"\n",
    "\n",
    "# 执行训练\n",
    "BEST_MODEL_RUN_PATH = run_final_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 预测与提交\n",
    "\n",
    " 训练完成后，我们使用在验证集上表现最好的模型（`best.pt`）对测试集进行预测，并生成符合比赛要求的`test.csv`文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 开始测试集预测...\n",
      "   📦 加载最佳模型: runs/train/final_bifpn_run/weights/best.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📷 发现 16362 张测试图片\n",
      "   🤖 正在执行预测...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      -> Predicting: 100%|██████████| 16362/16362 [15:38<00:00, 17.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 预测完成！结果已保存到 `test.csv`\n",
      "   现在可以将 `test.csv` 压缩为 `test.zip` 并提交到Codelab。\n"
     ]
    }
   ],
   "source": [
    "def run_prediction(run_path):\n",
    "    \"\"\"使用训练好的模型生成提交文件\"\"\"\n",
    "    if not run_path:\n",
    "        print(\"\\n❌ 未提供有效模型路径，跳过预测。\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n🔮 开始测试集预测...\")\n",
    "    best_model_path = Path(run_path) / 'weights' / 'best.pt'\n",
    "    \n",
    "    if not best_model_path.exists():\n",
    "        print(f\"❌ 找不到训练好的模型: {best_model_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"   📦 加载最佳模型: {best_model_path}\")\n",
    "    model = YOLO(best_model_path)\n",
    "\n",
    "    test_images = sorted(list(TEST_IMG_DIR.glob('*.jpg')))\n",
    "    print(f\"   📷 发现 {len(test_images)} 张测试图片\")\n",
    "\n",
    "    print(\"   🤖 正在执行预测...\")\n",
    "    predictions = []\n",
    "    for img_path in tqdm(test_images, desc=\"      -> Predicting\"):\n",
    "        image_id = int(img_path.stem)\n",
    "        results = model.predict(img_path, conf=0.01, verbose=False)\n",
    "        \n",
    "        pred_str = \"\"\n",
    "        for res in results:\n",
    "            if res.boxes is not None:\n",
    "                for box in res.boxes:\n",
    "                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "                    conf, cls = box.conf[0].cpu().numpy(), int(box.cls[0].cpu().numpy())\n",
    "                    pred_str += f\"{{{x1:.3f} {y1:.3f} {x2:.3f} {y2:.3f} {conf:.5f} {cls}}}\"\n",
    "        \n",
    "        predictions.append({'image_id': image_id, 'predictions': pred_str})\n",
    "\n",
    "    df = pd.DataFrame(predictions).sort_values('image_id')\n",
    "    output_path = 'test.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✅ 预测完成！结果已保存到 `{output_path}`\")\n",
    "    print(\"   现在可以将 `test.csv` 压缩为 `test.zip` 并提交到Codelab。\")\n",
    "\n",
    "BEST_MODEL_RUN_PATH = \"runs/train/final_bifpn_run\"\n",
    "# 执行预测\n",
    "run_prediction(BEST_MODEL_RUN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 最终总结与思考\n",
    "\n",
    " 在这次大作业中，我完整地体验了深度学习目标检测项目的全流程，从数据分析、预处理，到模型选型、创新、训练，再到最终的预测与提交。这是一个充满挑战但收获颇丰的过程。\n",
    "\n",
    " **关于模型改进**:\n",
    " 我最大的创新尝试是引入了BiFPN颈部结构。从理论上讲，它对于我们任务中的小物体和复杂场景应该能提供更好的多尺度特征表达。虽然最终的mAP提升需要通过实验结果来验证，但这个尝试本身让我对YOLO系列模型的可定制性和特征金字塔网络（FPN）的发展有了更深入的理解。从PANet到BiFPN，我看到了学术界在追求更高效、更强大的特征融合网络上所做的努力。\n",
    "\n",
    " **关于工程实践**:\n",
    " 本次项目让我深刻体会到，一个成功的深度学习项目远不止模型本身。**冒烟测试**的价值不言而喻，它帮我们提前规避了环境问题，节省了大量时间。**代码的可复现性**也至关重要，将整个流程整合到一个线性的Notebook中，不仅方便自己回顾，也让其他人能够轻松复现我的工作。\n",
    "\n",
    " **未来的可优化方向**:\n",
    " 如果时间允许，我还会尝试以下几个方向：\n",
    " 1.  **损失函数**：尝试使用如Focal Loss或EIoU Loss等更先进的损失函数，可能会对难样本和边界框回归有更好的效果。\n",
    " 2.  **数据增强**：尝试更复杂的数据增强策略，如Albumentations库。\n",
    " 3.  **模型蒸馏**：训练一个更大的模型作为教师模型，来指导我们当前这个轻量级模型的训练。\n",
    "\n",
    " 总之，这次大作业是一次宝贵的实战经历，感谢老师和助教的指导！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhgsl_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
