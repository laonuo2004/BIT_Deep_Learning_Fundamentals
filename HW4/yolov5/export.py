# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
Export a YOLOv5 PyTorch model to other formats. TensorFlow exports authored by https://github.com/zldrobit.

Format                      | `export.py --include`         | Model
---                         | ---                           | ---
PyTorch                     | -                             | yolov5s.pt
TorchScript                 | `torchscript`                 | yolov5s.torchscript
ONNX                        | `onnx`                        | yolov5s.onnx
OpenVINO                    | `openvino`                    | yolov5s_openvino_model/
TensorRT                    | `engine`                      | yolov5s.engine
CoreML                      | `coreml`                      | yolov5s.mlmodel
TensorFlow SavedModel       | `saved_model`                 | yolov5s_saved_model/
TensorFlow GraphDef         | `pb`                          | yolov5s.pb
TensorFlow Lite             | `tflite`                      | yolov5s.tflite
TensorFlow Edge TPU         | `edgetpu`                     | yolov5s_edgetpu.tflite
TensorFlow.js               | `tfjs`                        | yolov5s_web_model/
PaddlePaddle                | `paddle`                      | yolov5s_paddle_model/

Requirements:
    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime openvino-dev tensorflow-cpu  # CPU
    $ pip install -r requirements.txt coremltools onnx onnx-simplifier onnxruntime-gpu openvino-dev tensorflow  # GPU

Usage:
    $ python export.py --weights yolov5s.pt --include torchscript onnx openvino engine coreml tflite ...

Inference:
    $ python detect.py --weights yolov5s.pt                 # PyTorch
                                 yolov5s.torchscript        # TorchScript
                                 yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn
                                 yolov5s_openvino_model     # OpenVINO
                                 yolov5s.engine             # TensorRT
                                 yolov5s.mlmodel            # CoreML (macOS-only)
                                 yolov5s_saved_model        # TensorFlow SavedModel
                                 yolov5s.pb                 # TensorFlow GraphDef
                                 yolov5s.tflite             # TensorFlow Lite
                                 yolov5s_edgetpu.tflite     # TensorFlow Edge TPU
                                 yolov5s_paddle_model       # PaddlePaddle

TensorFlow.js:
    $ cd .. && git clone https://github.com/zldrobit/tfjs-yolov5-example.git && cd tfjs-yolov5-example
    $ npm install
    $ ln -s ../../yolov5/yolov5s_web_model public/yolov5s_web_model
    $ npm start
"""

import argparse
import contextlib
import json
import os
import platform
import re
import subprocess
import sys
import time
import warnings
from pathlib import Path

import pandas as pd
import torch
from torch.utils.mobile_optimizer import optimize_for_mobile

FILE = Path(__file__).resolve()
ROOT = FILE.parents[0]  # YOLOv5æ ¹ç›®å½•
if str(ROOT) not in sys.path:
    sys.path.append(str(ROOT))  # å°†ROOTæ·»åŠ åˆ°PATH
if platform.system() != "Windows":
    ROOT = Path(os.path.relpath(ROOT, Path.cwd()))  # ç›¸å¯¹è·¯å¾„

from models.experimental import attempt_load
from models.yolo import ClassificationModel, Detect, DetectionModel, SegmentationModel
from utils.dataloaders import LoadImages
from utils.general import (
    LOGGER,
    Profile,
    check_dataset,
    check_img_size,
    check_requirements,
    check_version,
    check_yaml,
    colorstr,
    file_size,
    get_default_args,
    print_args,
    url2file,
    yaml_save,
)
from utils.torch_utils import select_device, smart_inference_mode

MACOS = platform.system() == "Darwin"  # macOSç¯å¢ƒ


class iOSModel(torch.nn.Module):
    """ä¸€ä¸ªiOSå…¼å®¹çš„YOLOv5æ¨¡å‹åŒ…è£…å™¨ï¼Œæ ¹æ®å›¾åƒå°ºå¯¸å¯¹è¾“å…¥å›¾åƒè¿›è¡Œå½’ä¸€åŒ–ã€‚"""

    def __init__(self, model, im):
        """
        ä½¿ç”¨åŸºäºå›¾åƒå°ºå¯¸çš„å½’ä¸€åŒ–åˆå§‹åŒ–iOSå…¼å®¹æ¨¡å‹ã€‚

        Args:
            model (torch.nn.Module): è¦é€‚é…iOSå…¼å®¹æ€§çš„PyTorchæ¨¡å‹ã€‚
            im (torch.Tensor): è¡¨ç¤ºæ‰¹å¤„ç†å›¾åƒçš„è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚

        Returns:
            None: æ­¤æ–¹æ³•ä¸è¿”å›ä»»ä½•å€¼ã€‚

        Notes:
            æ­¤åˆå§‹åŒ–å™¨æ ¹æ®è¾“å…¥å›¾åƒå°ºå¯¸é…ç½®å½’ä¸€åŒ–ï¼Œè¿™å¯¹äºç¡®ä¿æ¨¡å‹åœ¨iOSè®¾å¤‡ä¸Šçš„å…¼å®¹æ€§å’Œæ­£å¸¸åŠŸèƒ½è‡³å…³é‡è¦ã€‚
            å½’ä¸€åŒ–æ­¥éª¤æ¶‰åŠå¦‚æœå›¾åƒæ˜¯æ­£æ–¹å½¢åˆ™é™¤ä»¥å›¾åƒå®½åº¦ï¼›å¦åˆ™ï¼Œå¯èƒ½é€‚ç”¨å…¶ä»–æ¡ä»¶ã€‚
        """
        super().__init__()
        b, c, h, w = im.shape  # æ‰¹æ¬¡ï¼Œé€šé“ï¼Œé«˜åº¦ï¼Œå®½åº¦
        self.model = model
        self.nc = model.nc  # ç±»åˆ«æ•°é‡
        if w == h:
            self.normalize = 1.0 / w
        else:
            self.normalize = torch.tensor([1.0 / w, 1.0 / h, 1.0 / w, 1.0 / h])  # å¹¿æ’­ (è¾ƒæ…¢ï¼Œè¾ƒå°)
            # np = model(im)[0].shape[1]  # ç‚¹çš„æ•°é‡
            # self.normalize = torch.tensor([1. / w, 1. / h, 1. / w, 1. / h]).expand(np, 4)  # æ˜¾å¼ (è¾ƒå¿«ï¼Œè¾ƒå¤§)

    def forward(self, x):
        """
        å¯¹è¾“å…¥å¼ é‡è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œè¿”å›ç±»åˆ«ç½®ä¿¡åº¦å’Œå½’ä¸€åŒ–åæ ‡ã€‚

        Args:
            x (torch.Tensor): åŒ…å«å›¾åƒæ•°æ®çš„è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (æ‰¹æ¬¡, é€šé“, é«˜åº¦, å®½åº¦)ã€‚

        Returns:
            torch.Tensor: åŒ…å«å½’ä¸€åŒ–åæ ‡ (xywh)ã€ç½®ä¿¡åº¦åˆ†æ•° (conf) å’Œç±»åˆ«æ¦‚ç‡ (cls) çš„æ‹¼æ¥å¼ é‡ï¼Œ
            å½¢çŠ¶ä¸º (N, 4 + 1 + C)ï¼Œå…¶ä¸­ N æ˜¯é¢„æµ‹æ•°é‡ï¼ŒC æ˜¯ç±»åˆ«æ•°é‡ã€‚

        Examples:
            ```python
            model = iOSModel(pretrained_model, input_image)
            output = model.forward(torch_input_tensor)
            ```
        """
        xywh, conf, cls = self.model(x)[0].squeeze().split((4, 1, self.nc), 1)
        return cls * conf, xywh * self.normalize  # ç½®ä¿¡åº¦ (3780, 80), åæ ‡ (3780, 4)


def export_formats():
    r"""
    è¿”å›æ”¯æŒçš„YOLOv5æ¨¡å‹å¯¼å‡ºæ ¼å¼åŠå…¶å±æ€§çš„DataFrameã€‚

    Returns:
        pandas.DataFrame: åŒ…å«æ”¯æŒçš„å¯¼å‡ºæ ¼å¼åŠå…¶å±æ€§çš„DataFrameã€‚DataFrameåŒ…æ‹¬æ ¼å¼åç§°ã€CLIå‚æ•°åç¼€ã€
        æ–‡ä»¶æ‰©å±•åæˆ–ç›®å½•åç§°ä»¥åŠæŒ‡ç¤ºå¯¼å‡ºæ ¼å¼æ˜¯å¦æ”¯æŒè®­ç»ƒå’Œæ£€æµ‹çš„å¸ƒå°”æ ‡å¿—ã€‚

    Examples:
        ```python
        formats = export_formats()
        print(f"Supported export formats:\n{formats}")
        ```

    Notes:
        DataFrameåŒ…å«ä»¥ä¸‹åˆ—ï¼š
        - Format: æ¨¡å‹æ ¼å¼çš„åç§° (ä¾‹å¦‚ï¼ŒPyTorch, TorchScript, ONNXç­‰)ã€‚
        - Include Argument: ç”¨äºå¯¼å‡ºè„šæœ¬ä»¥åŒ…å«æ­¤æ ¼å¼çš„å‚æ•°ã€‚
        - File Suffix: ä¸æ ¼å¼å…³è”çš„æ–‡ä»¶æ‰©å±•åæˆ–ç›®å½•åç§°ã€‚
        - Supports Training: æ ¼å¼æ˜¯å¦æ”¯æŒè®­ç»ƒã€‚
        - Supports Detection: æ ¼å¼æ˜¯å¦æ”¯æŒæ£€æµ‹ã€‚
    """
    x = [
        ["PyTorch", "-", ".pt", True, True],
        ["TorchScript", "torchscript", ".torchscript", True, True],
        ["ONNX", "onnx", ".onnx", True, True],
        ["OpenVINO", "openvino", "_openvino_model", True, False],
        ["TensorRT", "engine", ".engine", False, True],
        ["CoreML", "coreml", ".mlpackage", True, False],
        ["TensorFlow SavedModel", "saved_model", "_saved_model", True, True],
        ["TensorFlow GraphDef", "pb", ".pb", True, True],
        ["TensorFlow Lite", "tflite", ".tflite", True, False],
        ["TensorFlow Edge TPU", "edgetpu", "_edgetpu.tflite", False, False],
        ["TensorFlow.js", "tfjs", "_web_model", False, False],
        ["PaddlePaddle", "paddle", "_paddle_model", True, True],
    ]
    return pd.DataFrame(x, columns=["Format", "Argument", "Suffix", "CPU", "GPU"])


def try_export(inner_func):
    """
    è®°å½•æˆåŠŸæˆ–å¤±è´¥ã€æ‰§è¡Œæ—¶é—´ä»¥åŠæ–‡ä»¶å¤§å°ï¼Œç”¨äºä½¿ç”¨@try_exportåŒ…è£…çš„YOLOv5æ¨¡å‹å¯¼å‡ºå‡½æ•°ã€‚

    Args:
        inner_func (Callable): è¦ç”±è£…é¥°å™¨åŒ…è£…çš„æ¨¡å‹å¯¼å‡ºå‡½æ•°ã€‚

    Returns:
        Callable: è®°å½•æ‰§è¡Œç»†èŠ‚çš„åŒ…è£…å‡½æ•°ã€‚æ‰§è¡Œæ—¶ï¼Œæ­¤åŒ…è£…å‡½æ•°è¿”å›ï¼š
            - å…ƒç»„ (str | torch.nn.Module): æˆåŠŸæ—¶ â€” å¯¼å‡ºæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„å’Œæ¨¡å‹å®ä¾‹ã€‚
            - å…ƒç»„ (None, None): å¤±è´¥æ—¶ â€” Noneå€¼è¡¨ç¤ºå¯¼å‡ºå¤±è´¥ã€‚

    Examples:
        ```python
        @try_export
        def export_onnx(model, filepath):
            # implementation here
            pass

        exported_file, exported_model = export_onnx(yolo_model, 'path/to/save/model.onnx')
        ```

    Notes:
        æœ‰å…³å…¶ä»–è¦æ±‚å’Œæ¨¡å‹å¯¼å‡ºæ ¼å¼ï¼Œè¯·å‚é˜…
        [Ultralytics YOLOv5 GitHubä»“åº“](https://github.com/ultralytics/ultralytics)ã€‚
    """
    inner_args = get_default_args(inner_func)

    def outer_func(*args, **kwargs):
        """è®°å½•ä½¿ç”¨@try_exportè£…é¥°å™¨åŒ…è£…çš„æ¨¡å‹å¯¼å‡ºå‡½æ•°çš„æˆåŠŸ/å¤±è´¥å’Œæ‰§è¡Œç»†èŠ‚ã€‚"""
        prefix = inner_args["prefix"]
        try:
            with Profile() as dt:
                f, model = inner_func(*args, **kwargs)
            LOGGER.info(f"{prefix} export success âœ… {dt.t:.1f}s, saved as {f} ({file_size(f):.1f} MB)")
            return f, model
        except Exception as e:
            LOGGER.info(f"{prefix} export failure âŒ {dt.t:.1f}s: {e}")
            return None, None

    return outer_func


@try_export
def export_torchscript(model, im, file, optimize, prefix=colorstr("TorchScript:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºTorchScriptæ ¼å¼ã€‚

    Args:
        model (torch.nn.Module): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºè·Ÿè¸ªTorchScriptæ¨¡å‹çš„ç¤ºä¾‹è¾“å…¥å¼ é‡ã€‚
        file (Path): å¯¼å‡ºTorchScriptæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
        optimize (bool): å¦‚æœä¸ºTrueï¼Œåˆ™åº”ç”¨ç§»åŠ¨éƒ¨ç½²ä¼˜åŒ–ã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯çš„å¯é€‰å‰ç¼€ã€‚é»˜è®¤ä¸º'TorchScript:'ã€‚

    Returns:
        (str | None, torch.jit.ScriptModule | None): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«å¯¼å‡ºæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„ (å­—ç¬¦ä¸²)
            å’ŒTorchScriptæ¨¡å‹ (torch.jit.ScriptModule)ã€‚å¦‚æœå¯¼å‡ºå¤±è´¥ï¼Œå…ƒç»„çš„ä¸¤ä¸ªå…ƒç´ éƒ½å°†ä¸ºNoneã€‚

    Notes:
        - æ­¤å‡½æ•°ä½¿ç”¨è·Ÿè¸ªæ¥åˆ›å»ºTorchScriptæ¨¡å‹ã€‚
        - å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬è¾“å…¥å›¾åƒå½¢çŠ¶ã€æ¨¡å‹æ­¥é•¿å’Œç±»åˆ«åç§°ï¼Œä¿å­˜åœ¨TorchScriptæ¨¡å‹åŒ…å†…çš„é¢å¤–æ–‡ä»¶ (`config.txt`) ä¸­ã€‚
        - æœ‰å…³ç§»åŠ¨ä¼˜åŒ–çš„ä¿¡æ¯ï¼Œè¯·å‚é˜…PyTorchæ•™ç¨‹: https://pytorch.org/tutorials/recipes/mobile_interpreter.html

    Example:
        ```python
        from pathlib import Path
        import torch
        from models.experimental import attempt_load
        from utils.torch_utils import select_device

        # Load model
        weights = 'yolov5s.pt'
        device = select_device('')
        model = attempt_load(weights, device=device)

        # Example input tensor
        im = torch.zeros(1, 3, 640, 640).to(device)

        # Export model
        file = Path('yolov5s.torchscript')
        export_torchscript(model, im, file, optimize=False)
        ```
    """
    LOGGER.info(f"\n{prefix} starting export with torch {torch.__version__}...")
    f = file.with_suffix(".torchscript")

    ts = torch.jit.trace(model, im, strict=False) # è·Ÿè¸ªæ¨¡å‹ä»¥åˆ›å»ºTorchScript
    d = {"shape": im.shape, "stride": int(max(model.stride)), "names": model.names}
    extra_files = {"config.txt": json.dumps(d)}  # torch._C.ExtraFilesMap()
    if optimize:  # https://pytorch.org/tutorials/recipes/mobile_interpreter.html
        optimize_for_mobile(ts)._save_for_lite_interpreter(str(f), _extra_files=extra_files) # ä¼˜åŒ–å¹¶ä¿å­˜ç”¨äºç§»åŠ¨ç«¯
    else:
        ts.save(str(f), _extra_files=extra_files) # ä¿å­˜TorchScriptæ¨¡å‹
    return f, None


@try_export
def export_onnx(model, im, file, opset, dynamic, simplify, prefix=colorstr("ONNX:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œæ”¯æŒåŠ¨æ€è½´å’Œå¯é€‰çš„æ¨¡å‹ç®€åŒ–ã€‚

    Args:
        model (torch.nn.Module): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºæ¨¡å‹è·Ÿè¸ªçš„ç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œé€šå¸¸å½¢çŠ¶ä¸º (1, 3, é«˜åº¦, å®½åº¦)ã€‚
        file (pathlib.Path | str): ONNXæ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        opset (int): ç”¨äºå¯¼å‡ºçš„ONNX opsetç‰ˆæœ¬ã€‚
        dynamic (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä¸ºæ‰¹æ¬¡ã€é«˜åº¦å’Œå®½åº¦ç»´åº¦å¯ç”¨åŠ¨æ€è½´ã€‚
        simplify (bool): å¦‚æœä¸ºTrueï¼Œåˆ™åº”ç”¨ONNXæ¨¡å‹ç®€åŒ–ä»¥è¿›è¡Œä¼˜åŒ–ã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯çš„å‰ç¼€å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º'ONNX:'ã€‚

    Returns:
        tuple[pathlib.Path | str, None]: ä¿å­˜çš„ONNXæ¨¡å‹æ–‡ä»¶è·¯å¾„å’ŒNone (ä¸è£…é¥°å™¨ä¸€è‡´)ã€‚

    Raises:
        ImportError: å¦‚æœæœªå®‰è£…å¯¼å‡ºæ‰€éœ€çš„åº“ (ä¾‹å¦‚ï¼Œ'onnx', 'onnx-simplifier')ã€‚
        AssertionError: å¦‚æœç®€åŒ–æ£€æŸ¥å¤±è´¥ã€‚

    Notes:
        æ­¤å‡½æ•°æ‰€éœ€çš„åŒ…å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®‰è£…ï¼š
        ```
        pip install onnx onnx-simplifier onnxruntime onnxruntime-gpu
        ```

    Example:
        ```python
        from pathlib import Path
        import torch
        from models.experimental import attempt_load
        from utils.torch_utils import select_device

        # Load model
        weights = 'yolov5s.pt'
        device = select_device('')
        model = attempt_load(weights, map_location=device)

        # Example input tensor
        im = torch.zeros(1, 3, 640, 640).to(device)

        # Export model
        file_path = Path('yolov5s.onnx')
        export_onnx(model, im, file_path, opset=12, dynamic=True, simplify=True)
        ```
    """
    check_requirements("onnx>=1.12.0")
    import onnx

    LOGGER.info(f"\n{prefix} starting export with onnx {onnx.__version__}...")
    f = str(file.with_suffix(".onnx"))

    output_names = ["output0", "output1"] if isinstance(model, SegmentationModel) else ["output0"] # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®è¾“å‡ºåç§°
    if dynamic:
        dynamic = {"images": {0: "batch", 2: "height", 3: "width"}}  # å½¢çŠ¶(1,3,640,640)
        if isinstance(model, SegmentationModel):
            dynamic["output0"] = {0: "batch", 1: "anchors"}  # å½¢çŠ¶(1,25200,85)
            dynamic["output1"] = {0: "batch", 2: "mask_height", 3: "mask_width"}  # å½¢çŠ¶(1,32,160,160)
        elif isinstance(model, DetectionModel):
            dynamic["output0"] = {0: "batch", 1: "anchors"}  # å½¢çŠ¶(1,25200,85)

    torch.onnx.export(
        model.cpu() if dynamic else model,  # --dynamicä»…ä¸cpuå…¼å®¹
        im.cpu() if dynamic else im,
        f,
        verbose=False,
        opset_version=opset,
        do_constant_folding=True,  # WARNING: torch>=1.12çš„DNNæ¨ç†å¯èƒ½éœ€è¦do_constant_folding=False
        input_names=["images"],
        output_names=output_names,
        dynamic_axes=dynamic or None,
    )

    # æ£€æŸ¥
    model_onnx = onnx.load(f)  # åŠ è½½onnxæ¨¡å‹
    onnx.checker.check_model(model_onnx)  # æ£€æŸ¥onnxæ¨¡å‹

    # å…ƒæ•°æ®
    d = {"stride": int(max(model.stride)), "names": model.names}
    for k, v in d.items():
        meta = model_onnx.metadata_props.add()
        meta.key, meta.value = k, str(v)
    onnx.save(model_onnx, f)

    # ç®€åŒ–
    if simplify:
        try:
            cuda = torch.cuda.is_available()
            check_requirements(("onnxruntime-gpu" if cuda else "onnxruntime", "onnxslim"))
            import onnxslim

            LOGGER.info(f"{prefix} slimming with onnxslim {onnxslim.__version__}...")
            model_onnx = onnxslim.slim(model_onnx)
            onnx.save(model_onnx, f)
        except Exception as e:
            LOGGER.info(f"{prefix} simplifier failure: {e}")
    return f, model_onnx


@try_export
def export_openvino(file, metadata, half, int8, data, prefix=colorstr("OpenVINO:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºOpenVINOæ ¼å¼ï¼Œå¯é€‰FP16å’ŒINT8é‡åŒ–ã€‚

    Args:
        file (Path): OpenVINOæ¨¡å‹çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        metadata (dict): åŒ…å«æ¨¡å‹å…ƒæ•°æ® (å¦‚åç§°å’Œæ­¥é•¿) çš„å­—å…¸ã€‚
        half (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä»¥FP16ç²¾åº¦å¯¼å‡ºæ¨¡å‹ã€‚
        int8 (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä»¥INT8é‡åŒ–å¯¼å‡ºæ¨¡å‹ã€‚
        data (str): INT8é‡åŒ–æ‰€éœ€çš„æ•°æ®é›†YAMLæ–‡ä»¶è·¯å¾„ã€‚
        prefix (str): ç”¨äºæ—¥å¿—è®°å½•çš„å‰ç¼€å­—ç¬¦ä¸² (é»˜è®¤ä¸º"OpenVINO:")ã€‚

    Returns:
        (str, openvino.runtime.Model | None): OpenVINOæ¨¡å‹æ–‡ä»¶è·¯å¾„å’Œopenvino.runtime.Modelå¯¹è±¡ (å¦‚æœå¯¼å‡ºæˆåŠŸ)ï¼›
        å¦åˆ™ä¸ºNoneã€‚

    Notes:
        - éœ€è¦`openvino-dev`åŒ…ç‰ˆæœ¬2023.0æˆ–æ›´é«˜ã€‚å®‰è£…å‘½ä»¤:
          `$ pip install openvino-dev>=2023.0`
        - å¯¹äºINT8é‡åŒ–ï¼Œè¿˜éœ€è¦`nncf`åº“ç‰ˆæœ¬2.5.0æˆ–æ›´é«˜ã€‚å®‰è£…å‘½ä»¤:
          `$ pip install nncf>=2.5.0`

    Examples:
        ```python
        from pathlib import Path
        from ultralytics import YOLOv5

        model = YOLOv5('yolov5s.pt')
        export_openvino(Path('yolov5s.onnx'), metadata={'names': model.names, 'stride': model.stride}, half=True,
                        int8=False, data='data.yaml')
        ```

        è¿™å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºOpenVINOï¼Œä½¿ç”¨FP16ç²¾åº¦ä½†æ²¡æœ‰INT8é‡åŒ–ï¼Œå¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶è·¯å¾„ã€‚
    """
    check_requirements("openvino-dev>=2023.0")  # éœ€è¦openvino-dev: https://pypi.org/project/openvino-dev/
    import openvino.runtime as ov  # noqa
    from openvino.tools import mo  # noqa

    LOGGER.info(f"\n{prefix} starting export with openvino {ov.__version__}...")
    f = str(file).replace(file.suffix, f"_{'int8_' if int8 else ''}openvino_model{os.sep}")
    f_onnx = file.with_suffix(".onnx")
    f_ov = str(Path(f) / file.with_suffix(".xml").name)

    ov_model = mo.convert_model(f_onnx, model_name=file.stem, framework="onnx", compress_to_fp16=half)  # å¯¼å‡º

    if int8:
        check_requirements("nncf>=2.5.0")  # éœ€è¦è‡³å°‘2.5.0ç‰ˆæœ¬æ‰èƒ½ä½¿ç”¨åè®­ç»ƒé‡åŒ–
        import nncf
        import numpy as np

        from utils.dataloaders import create_dataloader

        def gen_dataloader(yaml_path, task="train", imgsz=640, workers=4):
            """æ ¹æ®ç»™å®šçš„YAMLæ•°æ®é›†é…ç½®ç”Ÿæˆç”¨äºæ¨¡å‹è®­ç»ƒæˆ–éªŒè¯çš„DataLoaderã€‚"""
            data_yaml = check_yaml(yaml_path)
            data = check_dataset(data_yaml)
            dataloader = create_dataloader(
                data[task], imgsz=imgsz, batch_size=1, stride=32, pad=0.5, single_cls=False, rect=False, workers=workers
            )[0]
            return dataloader

        # noqa: F811

        def transform_fn(data_item):
            """
            é‡åŒ–è½¬æ¢å‡½æ•°ã€‚

            ä»dataloaderé¡¹ä¸­æå–å’Œé¢„å¤„ç†è¾“å…¥æ•°æ®ä»¥è¿›è¡Œé‡åŒ–ã€‚

            Args:
               data_item: DataLoaderåœ¨è¿­ä»£æœŸé—´ç”Ÿæˆçš„æ•°æ®é¡¹å…ƒç»„

            Returns:
                input_tensor: ç”¨äºé‡åŒ–çš„è¾“å…¥æ•°æ®
            """
            assert data_item[0].dtype == torch.uint8, "input image must be uint8 for the quantization preprocessing"

            img = data_item[0].numpy().astype(np.float32)  # uint8è½¬fp16/32
            img /= 255.0  # 0 - 255è½¬0.0 - 1.0
            return np.expand_dims(img, 0) if img.ndim == 3 else img

        ds = gen_dataloader(data)
        quantization_dataset = nncf.Dataset(ds, transform_fn)
        ov_model = nncf.quantize(ov_model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)

    ov.serialize(ov_model, f_ov)  # ä¿å­˜
    yaml_save(Path(f) / file.with_suffix(".yaml").name, metadata)  # æ·»åŠ metadata.yaml
    return f, None


@try_export
def export_paddle(model, im, file, metadata, prefix=colorstr("PaddlePaddle:")):
    """
    ä½¿ç”¨X2Paddleå°†YOLOv5 PyTorchæ¨¡å‹å¯¼å‡ºä¸ºPaddlePaddleæ ¼å¼ï¼Œä¿å­˜è½¬æ¢åçš„æ¨¡å‹å’Œå…ƒæ•°æ®ã€‚

    Args:
        model (torch.nn.Module): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºæ¨¡å‹è·Ÿè¸ªçš„è¾“å…¥å¼ é‡ã€‚
        file (pathlib.Path): è¦è½¬æ¢çš„æºæ–‡ä»¶è·¯å¾„ã€‚
        metadata (dict): è¦ä¸æ¨¡å‹ä¸€èµ·ä¿å­˜çš„é¢å¤–å…ƒæ•°æ®ã€‚
        prefix (str): æ—¥å¿—ä¿¡æ¯çš„å‰ç¼€ã€‚

    Returns:
        tuple (str, None): ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ä¿å­˜çš„PaddlePaddleæ¨¡å‹çš„è·¯å¾„ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯Noneã€‚

    Examples:
        ```python
        from pathlib import Path
        import torch

        # Assume 'model' is a pre-trained YOLOv5 model and 'im' is an example input tensor
        model = ...  # Load your model here
        im = torch.randn((1, 3, 640, 640))  # Dummy input tensor for tracing
        file = Path("yolov5s.pt")
        metadata = {"stride": 32, "names": ["person", "bicycle", "car", "motorbike"]}

        export_paddle(model=model, im=im, file=file, metadata=metadata)
        ```

    Notes:
        ç¡®ä¿å·²å®‰è£…`paddlepaddle`å’Œ`x2paddle`ï¼Œå› ä¸ºè¿™äº›æ˜¯å¯¼å‡ºå‡½æ•°æ‰€éœ€çš„ã€‚å¯ä»¥é€šè¿‡pipå®‰è£…ï¼š
        ```
        $ pip install paddlepaddle x2paddle
        ```
    """
    check_requirements(("paddlepaddle>=3.0.0", "x2paddle"))
    import x2paddle
    from x2paddle.convert import pytorch2paddle

    LOGGER.info(f"\n{prefix} starting export with X2Paddle {x2paddle.__version__}...")
    f = str(file).replace(".pt", f"_paddle_model{os.sep}")

    pytorch2paddle(module=model, save_dir=f, jit_type="trace", input_examples=[im])  # å¯¼å‡º
    yaml_save(Path(f) / file.with_suffix(".yaml").name, metadata)  # æ·»åŠ metadata.yaml
    return f, None


@try_export
def export_coreml(model, im, file, int8, half, nms, mlmodel, prefix=colorstr("CoreML:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºCoreMLæ ¼å¼ï¼Œå¯é€‰NMSã€INT8å’ŒFP16æ”¯æŒã€‚

    Args:
        model (torch.nn.Module): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºè·Ÿè¸ªæ¨¡å‹çš„ç¤ºä¾‹è¾“å…¥å¼ é‡ã€‚
        file (pathlib.Path): ä¿å­˜CoreMLæ¨¡å‹çš„è·¯å¾„å¯¹è±¡ã€‚
        int8 (bool): æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨INT8é‡åŒ–çš„æ ‡å¿— (é»˜è®¤ä¸ºFalse)ã€‚
        half (bool): æŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨FP16é‡åŒ–çš„æ ‡å¿— (é»˜è®¤ä¸ºFalse)ã€‚
        nms (bool): æŒ‡ç¤ºæ˜¯å¦åŒ…å«éæå¤§å€¼æŠ‘åˆ¶çš„æ ‡å¿— (é»˜è®¤ä¸ºFalse)ã€‚
        mlmodel (bool): æŒ‡ç¤ºæ˜¯å¦å¯¼å‡ºä¸ºæ—§çš„*.mlmodelæ ¼å¼çš„æ ‡å¿— (é»˜è®¤ä¸ºFalse)ã€‚
        prefix (str): ç”¨äºæ—¥å¿—è®°å½•çš„å‰ç¼€å­—ç¬¦ä¸² (é»˜è®¤ä¸º'CoreML:')ã€‚

    Returns:
        tuple[pathlib.Path | None, None]: ä¿å­˜çš„CoreMLæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå‡ºç°é”™è¯¯åˆ™ä¸º (None, None)ã€‚

    Notes:
        å¯¼å‡ºçš„CoreMLæ¨¡å‹å°†ä»¥.mlmodelæ‰©å±•åä¿å­˜ã€‚
        é‡åŒ–ä»…åœ¨macOSä¸Šæ”¯æŒã€‚

    Example:
        ```python
        from pathlib import Path
        import torch
        from models.yolo import Model
        model = Model(cfg, ch=3, nc=80)
        im = torch.randn(1, 3, 640, 640)
        file = Path("yolov5s_coreml")
        export_coreml(model, im, file, int8=False, half=False, nms=True, mlmodel=False)
        ```
    """
    check_requirements("coremltools")
    import coremltools as ct

    LOGGER.info(f"\n{prefix} starting export with coremltools {ct.__version__}...")
    if mlmodel:
        f = file.with_suffix(".mlmodel")
        convert_to = "neuralnetwork"
        precision = None
    else:
        f = file.with_suffix(".mlpackage")
        convert_to = "mlprogram"
        precision = ct.precision.FLOAT16 if half else ct.precision.FLOAT32
    if nms:
        model = iOSModel(model, im) # å¦‚æœå¯ç”¨NMSï¼Œåˆ™ä½¿ç”¨iOSModelåŒ…è£…å™¨
    ts = torch.jit.trace(model, im, strict=False)  # TorchScriptæ¨¡å‹
    ct_model = ct.convert(
        ts,
        inputs=[ct.ImageType("image", shape=im.shape, scale=1 / 255, bias=[0, 0, 0])],
        convert_to=convert_to,
        compute_precision=precision,
    )
    bits, mode = (8, "kmeans") if int8 else (16, "linear") if half else (32, None)
    if bits < 32:
        if mlmodel:
            with warnings.catch_warnings():
                warnings.filterwarnings(
                    "ignore", category=DeprecationWarning
                )  # æŠ‘åˆ¶numpy==1.20æµ®ç‚¹è­¦å‘Šï¼Œåœ¨coremltools==7.0ä¸­å·²ä¿®å¤
                ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)
        elif bits == 8:
            op_config = ct.optimize.coreml.OpPalettizerConfig(mode=mode, nbits=bits, weight_threshold=512)
            config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)
            ct_model = ct.optimize.coreml.palettize_weights(ct_model, config)
    ct_model.save(f) # ä¿å­˜CoreMLæ¨¡å‹
    return f, ct_model


@try_export
def export_engine(
    model, im, file, half, dynamic, simplify, workspace=4, verbose=False, cache="", prefix=colorstr("TensorRT:")
):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºTensorRTå¼•æ“æ ¼å¼ï¼Œéœ€è¦GPUå’ŒTensorRT>=7.0.0ã€‚

    Args:
        model (torch.nn.Module): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹ã€‚
        im (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚
        file (pathlib.Path): ä¿å­˜å¯¼å‡ºæ¨¡å‹çš„è·¯å¾„ã€‚
        half (bool): è®¾ç½®ä¸ºTrueä»¥FP16ç²¾åº¦å¯¼å‡ºã€‚
        dynamic (bool): è®¾ç½®ä¸ºTrueä»¥å¯ç”¨åŠ¨æ€è¾“å…¥å½¢çŠ¶ã€‚
        simplify (bool): è®¾ç½®ä¸ºTrueä»¥åœ¨å¯¼å‡ºæœŸé—´ç®€åŒ–æ¨¡å‹ã€‚
        workspace (int): å·¥ä½œç©ºé—´å¤§å° (GB) (é»˜è®¤ä¸º4)ã€‚
        verbose (bool): è®¾ç½®ä¸ºTrueä»¥å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡ºã€‚
        cache (str): TensorRTè®¡æ—¶ç¼“å­˜è·¯å¾„ã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯å‰ç¼€ã€‚

    Returns:
        (pathlib.Path, None): åŒ…å«å¯¼å‡ºæ¨¡å‹è·¯å¾„å’ŒNoneçš„å…ƒç»„ã€‚

    Raises:
        AssertionError: å¦‚æœåœ¨CPUè€Œä¸æ˜¯GPUä¸Šæ‰§è¡Œã€‚
        RuntimeError: å¦‚æœè§£æONNXæ–‡ä»¶å¤±è´¥ã€‚

    Example:
        ```python
        from ultralytics import YOLOv5
        import torch
        from pathlib import Path

        model = YOLOv5('yolov5s.pt')  # åŠ è½½é¢„è®­ç»ƒçš„YOLOv5æ¨¡å‹
        input_tensor = torch.randn(1, 3, 640, 640).cuda()  # GPUä¸Šçš„ç¤ºä¾‹è¾“å…¥å¼ é‡
        export_path = Path('yolov5s.engine')  # å¯¼å‡ºç›®æ ‡

        export_engine(model.model, input_tensor, export_path, half=True, dynamic=True, simplify=True, workspace=8, verbose=True)
        ```
    """
    assert im.device.type != "cpu", "å¯¼å‡ºåœ¨CPUä¸Šè¿è¡Œï¼Œä½†å¿…é¡»åœ¨GPUä¸Šï¼Œä¾‹å¦‚ `python export.py --device 0`"
    try:
        import tensorrt as trt
    except Exception:
        if platform.system() == "Linux":
            check_requirements("nvidia-tensorrt", cmds="-U --index-url https://pypi.ngc.nvidia.com")
        import tensorrt as trt

    if trt.__version__[0] == "7":  # TensorRT 7å¤„ç† https://github.com/ultralytics/yolov5/issues/6012
        grid = model.model[-1].anchor_grid
        model.model[-1].anchor_grid = [a[..., :1, :1, :] for a in grid]
        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12
        model.model[-1].anchor_grid = grid
    else:  # TensorRT >= 8
        check_version(trt.__version__, "8.0.0", hard=True)  # éœ€è¦tensorrt>=8.0.0
        export_onnx(model, im, file, 12, dynamic, simplify)  # opset 12
    onnx = file.with_suffix(".onnx")

    LOGGER.info(f"\n{prefix} starting export with TensorRT {trt.__version__}...")
    is_trt10 = int(trt.__version__.split(".")[0]) >= 10  # æ˜¯å¦ä¸ºTensorRT >= 10
    assert onnx.exists(), f"æœªèƒ½å¯¼å‡ºONNXæ–‡ä»¶: {onnx}"
    f = file.with_suffix(".engine")  # TensorRTå¼•æ“æ–‡ä»¶
    logger = trt.Logger(trt.Logger.INFO)
    if verbose:
        logger.min_severity = trt.Logger.Severity.VERBOSE

    builder = trt.Builder(logger)
    config = builder.create_builder_config()
    if is_trt10:
        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, workspace << 30)
    else:  # TensorRTç‰ˆæœ¬7, 8
        config.max_workspace_size = workspace * 1 << 30
    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    network = builder.create_network(flag)
    parser = trt.OnnxParser(network, logger)
    if not parser.parse_from_file(str(onnx)):
        raise RuntimeError(f"æœªèƒ½åŠ è½½ONNXæ–‡ä»¶: {onnx}")

    inputs = [network.get_input(i) for i in range(network.num_inputs)]
    outputs = [network.get_output(i) for i in range(network.num_outputs)]
    for inp in inputs:
        LOGGER.info(f'{prefix} input "{inp.name}" with shape{inp.shape} {inp.dtype}')
    for out in outputs:
        LOGGER.info(f'{prefix} output "{out.name}" with shape{out.shape} {out.dtype}')

    if dynamic:
        if im.shape[0] <= 1:
            LOGGER.warning(f"{prefix} WARNING âš ï¸ --dynamicæ¨¡å‹éœ€è¦æœ€å¤§--batch-sizeå‚æ•°")
        profile = builder.create_optimization_profile()
        for inp in inputs:
            profile.set_shape(inp.name, (1, *im.shape[1:]), (max(1, im.shape[0] // 2), *im.shape[1:]), im.shape)
        config.add_optimization_profile(profile)

    LOGGER.info(f"{prefix} building FP{16 if builder.platform_has_fast_fp16 and half else 32} engine as {f}")
    if builder.platform_has_fast_fp16 and half:
        config.set_flag(trt.BuilderFlag.FP16)

    build = builder.build_serialized_network if is_trt10 else builder.build_engine
    with build(network, config) as engine, open(f, "wb") as t:
        t.write(engine if is_trt10 else engine.serialize())
    if cache:  # ä¿å­˜è®¡æ—¶ç¼“å­˜
        with open(cache, "wb") as c:
            c.write(config.get_timing_cache().serialize())
    return f, None


@try_export
def export_saved_model(
    model,
    im,
    file,
    dynamic,
    tf_nms=False,
    agnostic_nms=False,
    topk_per_class=100,
    topk_all=100,
    iou_thres=0.45,
    conf_thres=0.25,
    keras=False,
    prefix=colorstr("TensorFlow SavedModel:"),
):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºTensorFlow SavedModelæ ¼å¼ï¼Œæ”¯æŒåŠ¨æ€è½´å’Œéæå¤§å€¼æŠ‘åˆ¶ (NMS)ã€‚

    Args:
        model (torch.nn.Module): è¦è½¬æ¢çš„PyTorchæ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºè·Ÿè¸ªçš„ç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚
        file (pathlib.Path): ä¿å­˜å¯¼å‡ºæ¨¡å‹çš„è·¯å¾„ã€‚
        dynamic (bool): æŒ‡ç¤ºæ˜¯å¦åº”ä½¿ç”¨åŠ¨æ€è½´çš„æ ‡å¿—ã€‚
        tf_nms (bool, optional): å¯ç”¨TensorFlowéæå¤§å€¼æŠ‘åˆ¶ (NMS)ã€‚é»˜è®¤ä¸ºFalseã€‚
        agnostic_nms (bool, optional): å¯ç”¨ç±»åˆ«æ— å…³NMSã€‚é»˜è®¤ä¸ºFalseã€‚
        topk_per_class (int, optional): åœ¨åº”ç”¨NMSä¹‹å‰æ¯ç±»ä¿ç•™çš„Top Kæ£€æµ‹æ•°é‡ã€‚é»˜è®¤ä¸º100ã€‚
        topk_all (int, optional): åœ¨åº”ç”¨NMSä¹‹å‰æ‰€æœ‰ç±»åˆ«ä¿ç•™çš„Top Kæ£€æµ‹æ•°é‡ã€‚é»˜è®¤ä¸º100ã€‚
        iou_thres (float, optional): NMSçš„IoUé˜ˆå€¼ã€‚é»˜è®¤ä¸º0.45ã€‚
        conf_thres (float, optional): æ£€æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚é»˜è®¤ä¸º0.25ã€‚
        keras (bool, optional): å¦‚æœä¸ºTrueï¼Œåˆ™ä»¥Kerasæ ¼å¼ä¿å­˜æ¨¡å‹ã€‚é»˜è®¤ä¸ºFalseã€‚
        prefix (str, optional): æ—¥å¿—æ¶ˆæ¯çš„å‰ç¼€ã€‚é»˜è®¤ä¸º"TensorFlow SavedModel:"ã€‚

    Returns:
        tuple[str, tf.keras.Model | None]: åŒ…å«ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶å¤¹è·¯å¾„å’ŒKerasæ¨¡å‹å®ä¾‹çš„å…ƒç»„ï¼Œ
        å¦‚æœTensorFlowå¯¼å‡ºå¤±è´¥åˆ™ä¸ºNoneã€‚

    Notes:
        - è¯¥æ–¹æ³•æ”¯æŒTensorFlowç‰ˆæœ¬é«˜è¾¾2.15.1ã€‚
        - è¾ƒæ—§çš„TensorFlowç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒTensorFlow NMSã€‚
        - å¦‚æœTensorFlowç‰ˆæœ¬è¶…è¿‡2.13.1ï¼Œå¯¼å‡ºåˆ°TFLiteæ—¶å¯èƒ½ä¼šå¯¼è‡´é—®é¢˜ã€‚
          å‚è€ƒ: https://github.com/ultralytics/yolov5/issues/12489

    Example:
        ```python
        model, im = ...  # åˆå§‹åŒ–PyTorchæ¨¡å‹å’Œè¾“å…¥å¼ é‡
        export_saved_model(model, im, Path("yolov5_saved_model"), dynamic=True)
        ```
    """
    # YOLOv5 TensorFlow SavedModelå¯¼å‡º
    try:
        import tensorflow as tf
    except Exception:
        check_requirements(f"tensorflow{'' if torch.cuda.is_available() else '-macos' if MACOS else '-cpu'}<=2.15.1")

        import tensorflow as tf
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

    from models.tf import TFModel

    LOGGER.info(f"\n{prefix} starting export with tensorflow {tf.__version__}...")
    if tf.__version__ > "2.13.1":
        helper_url = "https://github.com/ultralytics/yolov5/issues/12489"
        LOGGER.info(
            f"WARNING âš ï¸ using Tensorflow {tf.__version__} > 2.13.1 might cause issue when exporting the model to tflite {helper_url}"
        )  # å¤„ç†é—®é¢˜ https://github.com/ultralytics/yolov5/issues/12489
    f = str(file).replace(".pt", "_saved_model")
    batch_size, ch, *imgsz = list(im.shape)  # BCHW

    tf_model = TFModel(cfg=model.yaml, model=model, nc=model.nc, imgsz=imgsz)
    im = tf.zeros((batch_size, *imgsz, ch))  # TensorFlowçš„BHWCé¡ºåº
    _ = tf_model.predict(im, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)
    inputs = tf.keras.Input(shape=(*imgsz, ch), batch_size=None if dynamic else batch_size)
    outputs = tf_model.predict(inputs, tf_nms, agnostic_nms, topk_per_class, topk_all, iou_thres, conf_thres)
    keras_model = tf.keras.Model(inputs=inputs, outputs=outputs)
    keras_model.trainable = False
    keras_model.summary()
    if keras:
        keras_model.save(f, save_format="tf") # ä¿å­˜ä¸ºKerasæ ¼å¼
    else:
        spec = tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype)
        m = tf.function(lambda x: keras_model(x))  # å®Œæ•´æ¨¡å‹
        m = m.get_concrete_function(spec)
        frozen_func = convert_variables_to_constants_v2(m)
        tfm = tf.Module()
        tfm.__call__ = tf.function(lambda x: frozen_func(x)[:4] if tf_nms else frozen_func(x), [spec])
        tfm.__call__(im)
        tf.saved_model.save(
            tfm,
            f,
            options=tf.saved_model.SaveOptions(experimental_custom_gradients=False)
            if check_version(tf.__version__, "2.6")
            else tf.saved_model.SaveOptions(),
        )
    return f, keras_model


@try_export
def export_pb(keras_model, file, prefix=colorstr("TensorFlow GraphDef:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºTensorFlow GraphDef (*.pb) æ ¼å¼ã€‚

    Args:
        keras_model (tf.keras.Model): è¦è½¬æ¢çš„Kerasæ¨¡å‹ã€‚
        file (Path): ä¿å­˜GraphDefçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ã€‚
        prefix (str): å¯é€‰å‰ç¼€å­—ç¬¦ä¸²ï¼›é»˜è®¤ä¸ºæŒ‡ç¤ºTensorFlow GraphDefå¯¼å‡ºçŠ¶æ€çš„å½©è‰²å­—ç¬¦ä¸²ã€‚

    Returns:
        Tuple[Path, None]: ä¿å­˜GraphDefæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„å’ŒNoneå ä½ç¬¦ã€‚

    Notes:
        æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å†»ç»“å›¾æŒ‡å—: https://github.com/leimao/Frozen_Graph_TensorFlow

    Example:
        ```python
        from pathlib import Path
        keras_model = ...  # å‡è®¾å­˜åœ¨ä¸€ä¸ªKerasæ¨¡å‹
        file = Path("model.pb")
        export_pb(keras_model, file)
        ```
    """
    import tensorflow as tf
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2

    LOGGER.info(f"\n{prefix} starting export with tensorflow {tf.__version__}...")
    f = file.with_suffix(".pb")

    m = tf.function(lambda x: keras_model(x))  # å®Œæ•´æ¨¡å‹
    m = m.get_concrete_function(tf.TensorSpec(keras_model.inputs[0].shape, keras_model.inputs[0].dtype))
    frozen_func = convert_variables_to_constants_v2(m)
    frozen_func.graph.as_graph_def()
    tf.io.write_graph(graph_or_graph_def=frozen_func.graph, logdir=str(f.parent), name=f.name, as_text=False) # å†™å…¥GraphDef
    return f, None


@try_export
def export_tflite(
    keras_model, im, file, int8, per_tensor, data, nms, agnostic_nms, prefix=colorstr("TensorFlow Lite:")
):
    # YOLOv5 TensorFlow Liteå¯¼å‡º
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºTensorFlow Liteæ ¼å¼ï¼Œå¯é€‰INT8é‡åŒ–å’ŒNMSæ”¯æŒã€‚

    Args:
        keras_model (tf.keras.Model): è¦å¯¼å‡ºçš„Kerasæ¨¡å‹ã€‚
        im (torch.Tensor): ç”¨äºå½’ä¸€åŒ–å’Œæ¨¡å‹è·Ÿè¸ªçš„è¾“å…¥å›¾åƒå¼ é‡ã€‚
        file (Path): ä¿å­˜TensorFlow Liteæ¨¡å‹çš„è·¯å¾„ã€‚
        int8 (bool): å¦‚æœä¸ºTrueï¼Œåˆ™å¯ç”¨INT8é‡åŒ–ã€‚
        per_tensor (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ç¦ç”¨é€é€šé“é‡åŒ–ã€‚
        data (str): ç”¨äºINT8é‡åŒ–ä¸­ä»£è¡¨æ€§æ•°æ®é›†ç”Ÿæˆçš„è·¯å¾„ã€‚
        nms (bool): å¦‚æœä¸ºTrueï¼Œåˆ™å¯ç”¨éæå¤§å€¼æŠ‘åˆ¶ (NMS)ã€‚
        agnostic_nms (bool): å¦‚æœä¸ºTrueï¼Œåˆ™å¯ç”¨ç±»åˆ«æ— å…³NMSã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯çš„å‰ç¼€ã€‚

    Returns:
        (str | None, tflite.Model | None): å¯¼å‡ºTFLiteæ¨¡å‹çš„æ–‡ä»¶è·¯å¾„å’ŒTFLiteæ¨¡å‹å®ä¾‹ï¼Œå¦‚æœå¯¼å‡ºå¤±è´¥åˆ™ä¸ºNoneã€‚

    Example:
        ```python
        from pathlib import Path
        import torch
        import tensorflow as tf

        # Load a Keras model wrapping a YOLOv5 model
        keras_model = tf.keras.models.load_model('path/to/keras_model.h5')

        # Example input tensor
        im = torch.zeros(1, 3, 640, 640)

        # Export the model
        export_tflite(keras_model, im, Path('model.tflite'), int8=True, per_tensor=False, data='data/coco.yaml',
                      nms=True, agnostic_nms=False)
        ```

    Notes:
        - ç¡®ä¿å·²å®‰è£…TensorFlowå’ŒTensorFlow Liteä¾èµ–é¡¹ã€‚
        - INT8é‡åŒ–éœ€è¦ä»£è¡¨æ€§æ•°æ®é›†ä»¥å®ç°æœ€ä½³ç²¾åº¦ã€‚
        - TensorFlow Liteæ¨¡å‹é€‚ç”¨äºåœ¨ç§»åŠ¨å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šè¿›è¡Œé«˜æ•ˆæ¨ç†ã€‚
    """
    import tensorflow as tf

    LOGGER.info(f"\n{prefix} starting export with tensorflow {tf.__version__}...")
    batch_size, ch, *imgsz = list(im.shape)  # BCHW
    f = str(file).replace(".pt", "-fp16.tflite")

    converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
    converter.target_spec.supported_types = [tf.float16]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    if int8:
        from models.tf import representative_dataset_gen

        dataset = LoadImages(check_dataset(check_yaml(data))["train"], img_size=imgsz, auto=False)
        converter.representative_dataset = lambda: representative_dataset_gen(dataset, ncalib=100)
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
        converter.target_spec.supported_types = []
        converter.inference_input_type = tf.uint8  # or tf.int8
        converter.inference_output_type = tf.uint8  # or tf.int8
        converter.experimental_new_quantizer = True
        if per_tensor:
            converter._experimental_disable_per_channel = True
        f = str(file).replace(".pt", "-int8.tflite")
    if nms or agnostic_nms:
        converter.target_spec.supported_ops.append(tf.lite.OpsSet.SELECT_TF_OPS)

    tflite_model = converter.convert() # è½¬æ¢æ¨¡å‹
    open(f, "wb").write(tflite_model) # å†™å…¥TFLiteæ¨¡å‹
    return f, None


@try_export
def export_edgetpu(file, prefix=colorstr("Edge TPU:")):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºEdge TPUå…¼å®¹çš„TFLiteæ ¼å¼ï¼›éœ€è¦Linuxå’ŒEdge TPUç¼–è¯‘å™¨ã€‚

    Args:
        file (Path): è¦å¯¼å‡ºçš„YOLOv5æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.ptæ ¼å¼)ã€‚
        prefix (str, optional): æ—¥å¿—æ¶ˆæ¯çš„å‰ç¼€ã€‚é»˜è®¤ä¸ºcolorstr("Edge TPU:")ã€‚

    Returns:
        tuple[Path, None]: å¯¼å‡ºåˆ°Edge TPUå…¼å®¹çš„TFLiteæ¨¡å‹çš„è·¯å¾„ï¼ŒNoneã€‚

    Raises:
        AssertionError: å¦‚æœç³»ç»Ÿä¸æ˜¯Linuxã€‚
        subprocess.CalledProcessError: å¦‚æœä»»ä½•è°ƒç”¨Edge TPUç¼–è¯‘å™¨çš„å­è¿›ç¨‹å¤±è´¥ã€‚

    Notes:
        è¦ä½¿ç”¨æ­¤å‡½æ•°ï¼Œè¯·ç¡®ä¿æ‚¨çš„Linuxç³»ç»Ÿä¸Šå®‰è£…äº†Edge TPUç¼–è¯‘å™¨ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„æ‰¾åˆ°å®‰è£…è¯´æ˜ï¼š
        https://coral.ai/docs/edgetpu/compiler/ã€‚

    Example:
        ```python
        from pathlib import Path
        file = Path('yolov5s.pt')
        export_edgetpu(file)
        ```
    """
    cmd = "edgetpu_compiler --version"
    help_url = "https://coral.ai/docs/edgetpu/compiler/"
    assert platform.system() == "Linux", f"å¯¼å‡ºä»…åœ¨Linuxä¸Šæ”¯æŒã€‚è¯·å‚é˜… {help_url}"
    if subprocess.run(f"{cmd} > /dev/null 2>&1", shell=True).returncode != 0:
        LOGGER.info(f"\n{prefix} å¯¼å‡ºéœ€è¦Edge TPUç¼–è¯‘å™¨ã€‚å°è¯•ä» {help_url} å®‰è£…")
        sudo = subprocess.run("sudo --version >/dev/null", shell=True).returncode == 0  # ç³»ç»Ÿæ˜¯å¦å®‰è£…sudo
        for c in (
            "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -",
            'echo "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list',
            "sudo apt-get update",
            "sudo apt-get install edgetpu-compiler",
        ):
            subprocess.run(c if sudo else c.replace("sudo ", ""), shell=True, check=True)
    ver = subprocess.run(cmd, shell=True, capture_output=True, check=True).stdout.decode().split()[-1]

    LOGGER.info(f"\n{prefix} starting export with Edge TPU compiler {ver}...")
    f = str(file).replace(".pt", "-int8_edgetpu.tflite")  # Edge TPUæ¨¡å‹
    f_tfl = str(file).replace(".pt", "-int8.tflite")  # TFLiteæ¨¡å‹

    subprocess.run(
        [
            "edgetpu_compiler",
            "-s",
            "-d",
            "-k",
            "10",
            "--out_dir",
            str(file.parent),
            f_tfl,
        ],
        check=True,
    ) # è¿è¡ŒEdge TPUç¼–è¯‘å™¨
    return f, None


@try_export
def export_tfjs(file, int8, prefix=colorstr("TensorFlow.js:")):
    """
    å°†YOLOv5æ¨¡å‹è½¬æ¢ä¸ºTensorFlow.jsæ ¼å¼ï¼Œå¯é€‰uint8é‡åŒ–ã€‚

    Args:
        file (Path): è¦è½¬æ¢çš„YOLOv5æ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œé€šå¸¸å…·æœ‰".pt"æˆ–".onnx"æ‰©å±•åã€‚
        int8 (bool): å¦‚æœä¸ºTrueï¼Œåˆ™åœ¨è½¬æ¢è¿‡ç¨‹ä¸­åº”ç”¨uint8é‡åŒ–ã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯çš„å¯é€‰å‰ç¼€ï¼Œé»˜è®¤ä¸ºå¸¦é¢œè‰²æ ¼å¼çš„'TensorFlow.js:'ã€‚

    Returns:
        (str, None): åŒ…å«è¾“å‡ºç›®å½•è·¯å¾„çš„å­—ç¬¦ä¸²å’ŒNoneã€‚

    Notes:
        - æ­¤å‡½æ•°éœ€è¦`tensorflowjs`åŒ…ã€‚ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š
          ```shell
          pip install tensorflowjs
          ```
        - è½¬æ¢åçš„TensorFlow.jsæ¨¡å‹å°†ä¿å­˜åœ¨ä¸€ä¸ªç›®å½•ä¸­ï¼Œè¯¥ç›®å½•çš„åç§°åœ¨åŸå§‹æ–‡ä»¶ååé™„åŠ "_web_model"åç¼€ã€‚
        - è½¬æ¢æ¶‰åŠè¿è¡Œè°ƒç”¨TensorFlow.jsè½¬æ¢å™¨å·¥å…·çš„shellå‘½ä»¤ã€‚

    Example:
        ```python
        from pathlib import Path
        file = Path('yolov5.onnx')
        export_tfjs(file, int8=False)
        ```
    """
    check_requirements("tensorflowjs")
    import tensorflowjs as tfjs

    LOGGER.info(f"\n{prefix} starting export with tensorflowjs {tfjs.__version__}...")
    f = str(file).replace(".pt", "_web_model")  # jsç›®å½•
    f_pb = file.with_suffix(".pb")  # *.pbè·¯å¾„
    f_json = f"{f}/model.json"  # *.jsonè·¯å¾„

    args = [
        "tensorflowjs_converter",
        "--input_format=tf_frozen_model",
        "--quantize_uint8" if int8 else "",
        "--output_node_names=Identity,Identity_1,Identity_2,Identity_3",
        str(f_pb),
        f,
    ]
    subprocess.run([arg for arg in args if arg], check=True) # è¿è¡Œtensorflowjsè½¬æ¢å™¨

    json = Path(f_json).read_text()
    with open(f_json, "w") as j:  # æŒ‰å‡åºæ’åºJSON Identity_*
        subst = re.sub(
            r'{"outputs": {"Identity.?.?": {"name": "Identity.?.?"}, '
            r'"Identity.?.?": {"name": "Identity.?.?"}, '
            r'"Identity.?.?": {"name": "Identity.?.?"}, '
            r'"Identity.?.?": {"name": "Identity.?.?"}}}',
            r'{"outputs": {"Identity": {"name": "Identity"}, '
            r'"Identity_1": {"name": "Identity_1"}, '
            r'"Identity_2": {"name": "Identity_2"}, '
            r'"Identity_3": {"name": "Identity_3"}}}',
            json,
        )
        j.write(subst)
    return f, None


def add_tflite_metadata(file, metadata, num_outputs):
    """
    å‘TensorFlow Lite (TFLite) æ¨¡å‹æ–‡ä»¶æ·»åŠ å…ƒæ•°æ®ï¼Œæ ¹æ®TensorFlowæŒ‡å—æ”¯æŒå¤šä¸ªè¾“å‡ºã€‚

    Args:
        file (str): è¦æ·»åŠ å…ƒæ•°æ®çš„TFLiteæ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚
        metadata (dict): è¦æ·»åŠ åˆ°æ¨¡å‹çš„å…ƒæ•°æ®ä¿¡æ¯ï¼ŒæŒ‰ç…§TFLiteå…ƒæ•°æ®æ¨¡å¼çš„è¦æ±‚è¿›è¡Œç»“æ„åŒ–ã€‚
            å¸¸è§é”®åŒ…æ‹¬"name"ã€"description"ã€"version"ã€"author"å’Œ"license"ã€‚
        num_outputs (int): æ¨¡å‹å…·æœ‰çš„è¾“å‡ºå¼ é‡æ•°é‡ï¼Œç”¨äºæ­£ç¡®é…ç½®å…ƒæ•°æ®ã€‚

    Returns:
        None

    Example:
        ```python
        metadata = {
            "name": "yolov5",
            "description": "YOLOv5 object detection model",
            "version": "1.0",
            "author": "Ultralytics",
            "license": "Apache License 2.0"
        }
        add_tflite_metadata("model.tflite", metadata, num_outputs=4)
        ```

    Note:
        TFLiteå…ƒæ•°æ®å¯ä»¥åŒ…å«æ¨¡å‹åç§°ã€ç‰ˆæœ¬ã€ä½œè€…å’Œå…¶ä»–ç›¸å…³è¯¦ç»†ä¿¡æ¯ã€‚
        æœ‰å…³å…ƒæ•°æ®ç»“æ„çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…TensorFlow Lite
        [å…ƒæ•°æ®æŒ‡å—](https://ai.google.dev/edge/litert/models/metadata)ã€‚
    """
    with contextlib.suppress(ImportError):
        # check_requirements('tflite_support')
        from tflite_support import flatbuffers
        from tflite_support import metadata as _metadata
        from tflite_support import metadata_schema_py_generated as _metadata_fb

        tmp_file = Path("/tmp/meta.txt")
        with open(tmp_file, "w") as meta_f:
            meta_f.write(str(metadata))

        model_meta = _metadata_fb.ModelMetadataT()
        label_file = _metadata_fb.AssociatedFileT()
        label_file.name = tmp_file.name
        model_meta.associatedFiles = [label_file]

        subgraph = _metadata_fb.SubGraphMetadataT()
        subgraph.inputTensorMetadata = [_metadata_fb.TensorMetadataT()]
        subgraph.outputTensorMetadata = [_metadata_fb.TensorMetadataT()] * num_outputs
        model_meta.subgraphMetadata = [subgraph]

        b = flatbuffers.Builder(0)
        b.Finish(model_meta.Pack(b), _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)
        metadata_buf = b.Output()

        populator = _metadata.MetadataPopulator.with_model_file(file)
        populator.load_metadata_buffer(metadata_buf)
        populator.load_associated_files([str(tmp_file)])
        populator.populate()
        tmp_file.unlink()


def pipeline_coreml(model, im, file, names, y, mlmodel, prefix=colorstr("CoreML Pipeline:")):
    """
    å°†PyTorch YOLOv5æ¨¡å‹è½¬æ¢ä¸ºCoreMLæ ¼å¼ï¼Œå¸¦éæå¤§å€¼æŠ‘åˆ¶ (NMS)ï¼Œå¤„ç†ä¸åŒçš„è¾“å…¥/è¾“å‡ºå½¢çŠ¶ï¼Œå¹¶ä¿å­˜æ¨¡å‹ã€‚

    Args:
        model (torch.nn.Module): è¦è½¬æ¢çš„YOLOv5 PyTorchæ¨¡å‹ã€‚
        im (torch.Tensor): ç¤ºä¾‹è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (N, C, H, W)ï¼Œå…¶ä¸­Næ˜¯æ‰¹æ¬¡å¤§å°ï¼ŒCæ˜¯é€šé“æ•°ï¼Œ
            Hæ˜¯é«˜åº¦ï¼ŒWæ˜¯å®½åº¦ã€‚
        file (Path): ä¿å­˜è½¬æ¢åçš„CoreMLæ¨¡å‹çš„è·¯å¾„ã€‚
        names (dict[int, str]): ç±»åˆ«ç´¢å¼•åˆ°ç±»åˆ«åç§°çš„å­—å…¸æ˜ å°„ã€‚
        y (torch.Tensor): PyTorchæ¨¡å‹å‰å‘ä¼ æ’­çš„è¾“å‡ºå¼ é‡ã€‚
        mlmodel (bool): æŒ‡ç¤ºæ˜¯å¦å¯¼å‡ºä¸ºæ—§çš„*.mlmodelæ ¼å¼çš„æ ‡å¿— (é»˜è®¤ä¸ºFalse)ã€‚
        prefix (str): æ—¥å¿—æ¶ˆæ¯çš„è‡ªå®šä¹‰å‰ç¼€ã€‚

    Returns:
        (Path): ä¿å­˜çš„CoreMLæ¨¡å‹è·¯å¾„ (.mlmodel)ã€‚

    Raises:
        AssertionError: å¦‚æœç±»åˆ«åç§°çš„æ•°é‡ä¸æ¨¡å‹ä¸­çš„ç±»åˆ«æ•°é‡ä¸åŒ¹é…ã€‚

    Notes:
        - æ­¤å‡½æ•°éœ€è¦å®‰è£…`coremltools`ã€‚
        - åœ¨émacOSç¯å¢ƒä¸‹è¿è¡Œæ­¤å‡½æ•°å¯èƒ½ä¸æ”¯æŒæŸäº›åŠŸèƒ½ã€‚
        - çµæ´»çš„è¾“å…¥å½¢çŠ¶å’Œé¢å¤–çš„NMSé€‰é¡¹å¯ä»¥åœ¨å‡½æ•°å†…éƒ¨è‡ªå®šä¹‰ã€‚

    Examples:
        ```python
        from pathlib import Path
        import torch

        model = torch.load('yolov5s.pt')  # åŠ è½½YOLOv5æ¨¡å‹
        im = torch.zeros((1, 3, 640, 640))  # ç¤ºä¾‹è¾“å…¥å¼ é‡

        names = {0: "person", 1: "bicycle", 2: "car", ...}  # å®šä¹‰ç±»åˆ«åç§°

        y = model(im)  # æ‰§è¡Œå‰å‘ä¼ æ’­ä»¥è·å–æ¨¡å‹è¾“å‡º

        output_file = Path('yolov5s.mlmodel')  # è½¬æ¢ä¸ºCoreML
        pipeline_coreml(model, im, output_file, names, y)
        ```
    """
    import coremltools as ct
    from PIL import Image

    f = file.with_suffix(".mlmodel") if mlmodel else file.with_suffix(".mlpackage")
    print(f"{prefix} starting pipeline with coremltools {ct.__version__}...")
    batch_size, ch, h, w = list(im.shape)  # BCHW
    t = time.time()

    # YOLOv5è¾“å‡ºå½¢çŠ¶
    spec = model.get_spec()
    out0, out1 = iter(spec.description.output)
    if platform.system() == "Darwin":
        img = Image.new("RGB", (w, h))  # img(192 width, 320 height)
        # img = torch.zeros((*opt.img_size, 3)).numpy()  # img size(320,192,3) iDetection
        out = model.predict({"image": img})
        out0_shape, out1_shape = out[out0.name].shape, out[out1.name].shape
    else:  # linuxå’Œwindowsæ— æ³•è¿è¡Œmodel.predict()ï¼Œä»pytorchè¾“å‡ºyè·å–å°ºå¯¸
        s = tuple(y[0].shape)
        out0_shape, out1_shape = (s[1], s[2] - 5), (s[1], 4)  # (3780, 80), (3780, 4)

    # æ£€æŸ¥
    nx, ny = spec.description.input[0].type.imageType.width, spec.description.input[0].type.imageType.height
    na, nc = out0_shape
    # na, nc = out0.type.multiArrayType.shape  # é”šç‚¹æ•°é‡ï¼Œç±»åˆ«æ•°é‡
    assert len(names) == nc, f"{len(names)}ä¸ªåç§°ä¸nc={nc}ä¸åŒ¹é…"  # æ£€æŸ¥

    # å®šä¹‰è¾“å‡ºå½¢çŠ¶ (ç¼ºå¤±)
    out0.type.multiArrayType.shape[:] = out0_shape  # (3780, 80)
    out1.type.multiArrayType.shape[:] = out1_shape  # (3780, 4)
    # spec.neuralNetwork.preprocessing[0].featureName = '0'

    # çµæ´»è¾“å…¥å½¢çŠ¶
    # from coremltools.models.neural_network import flexible_shape_utils
    # s = [] # å½¢çŠ¶
    # s.append(flexible_shape_utils.NeuralNetworkImageSize(320, 192))
    # s.append(flexible_shape_utils.NeuralNetworkImageSize(640, 384))  # (é«˜åº¦, å®½åº¦)
    # flexible_shape_utils.add_enumerated_image_sizes(spec, feature_name='image', sizes=s)
    # r = flexible_shape_utils.NeuralNetworkImageSizeRange()  # å½¢çŠ¶èŒƒå›´
    # r.add_height_range((192, 640))
    # r.add_width_range((192, 640))
    # flexible_shape_utils.update_image_size_range(spec, feature_name='image', size_range=r)

    # æ‰“å°
    print(spec.description)

    # ä»specåˆ›å»ºæ¨¡å‹
    weights_dir = None
    weights_dir = None if mlmodel else str(f / "Data/com.apple.CoreML/weights")
    model = ct.models.MLModel(spec, weights_dir=weights_dir)

    # 3. åˆ›å»ºNMS protobuf
    nms_spec = ct.proto.Model_pb2.Model()
    nms_spec.specificationVersion = 5
    for i in range(2):
        decoder_output = model._spec.description.output[i].SerializeToString()
        nms_spec.description.input.add()
        nms_spec.description.input[i].ParseFromString(decoder_output)
        nms_spec.description.output.add()
        nms_spec.description.output[i].ParseFromString(decoder_output)

    nms_spec.description.output[0].name = "confidence"
    nms_spec.description.output[1].name = "coordinates"

    output_sizes = [nc, 4]
    for i in range(2):
        ma_type = nms_spec.description.output[i].type.multiArrayType
        ma_type.shapeRange.sizeRanges.add()
        ma_type.shapeRange.sizeRanges[0].lowerBound = 0
        ma_type.shapeRange.sizeRanges[0].upperBound = -1
        ma_type.shapeRange.sizeRanges.add()
        ma_type.shapeRange.sizeRanges[1].lowerBound = output_sizes[i]
        ma_type.shapeRange.sizeRanges[1].upperBound = output_sizes[i]
        del ma_type.shape[:]

    nms = nms_spec.nonMaximumSuppression
    nms.confidenceInputFeatureName = out0.name  # 1x507x80
    nms.coordinatesInputFeatureName = out1.name  # 1x507x4
    nms.confidenceOutputFeatureName = "confidence"
    nms.coordinatesOutputFeatureName = "coordinates"
    nms.iouThresholdInputFeatureName = "iouThreshold"
    nms.confidenceThresholdInputFeatureName = "confidenceThreshold"
    nms.iouThreshold = 0.45
    nms.confidenceThreshold = 0.25
    nms.pickTop.perClass = True
    nms.stringClassLabels.vector.extend(names.values())
    nms_model = ct.models.MLModel(nms_spec)

    # 4. å°†æ¨¡å‹ç»„åˆæˆç®¡é“
    pipeline = ct.models.pipeline.Pipeline(
        input_features=[
            ("image", ct.models.datatypes.Array(3, ny, nx)),
            ("iouThreshold", ct.models.datatypes.Double()),
            ("confidenceThreshold", ct.models.datatypes.Double()),
        ],
        output_features=["confidence", "coordinates"],
    )
    pipeline.add_model(model)
    pipeline.add_model(nms_model)

    # ä¿®æ­£æ•°æ®ç±»å‹
    pipeline.spec.description.input[0].ParseFromString(model._spec.description.input[0].SerializeToString())
    pipeline.spec.description.output[0].ParseFromString(nms_model._spec.description.output[0].SerializeToString())
    pipeline.spec.description.output[1].ParseFromString(nms_model._spec.description.output[1].SerializeToString())

    # æ›´æ–°å…ƒæ•°æ®
    pipeline.spec.specificationVersion = 5
    pipeline.spec.description.metadata.versionString = "https://github.com/ultralytics/yolov5"
    pipeline.spec.description.metadata.shortDescription = "https://github.com/ultralytics/yolov5"
    pipeline.spec.description.metadata.author = "glenn.jocher@ultralytics.com"
    pipeline.spec.description.metadata.license = "https://github.com/ultralytics/yolov5/blob/master/LICENSE"
    pipeline.spec.description.metadata.userDefined.update(
        {
            "classes": ",".join(names.values()),
            "iou_threshold": str(nms.iouThreshold),
            "confidence_threshold": str(nms.confidenceThreshold),
        }
    )

    # ä¿å­˜æ¨¡å‹
    model = ct.models.MLModel(pipeline.spec, weights_dir=weights_dir)
    model.input_description["image"] = "è¾“å…¥å›¾åƒ"
    model.input_description["iouThreshold"] = f"(å¯é€‰) IoUé˜ˆå€¼è¦†ç›– (é»˜è®¤: {nms.iouThreshold})"
    model.input_description["confidenceThreshold"] = (
        f"(å¯é€‰) ç½®ä¿¡åº¦é˜ˆå€¼è¦†ç›– (é»˜è®¤: {nms.confidenceThreshold})"
    )
    model.output_description["confidence"] = 'æ¡† Ã— ç±»åˆ«ç½®ä¿¡åº¦ (å‚è§ç”¨æˆ·å®šä¹‰å…ƒæ•°æ® "classes")'
    model.output_description["coordinates"] = "æ¡† Ã— [x, y, å®½åº¦, é«˜åº¦] (ç›¸å¯¹äºå›¾åƒå°ºå¯¸)"
    model.save(f)  # ç®¡é“åŒ–æ¨¡å‹
    print(f"{prefix} ç®¡é“æˆåŠŸ ({time.time() - t:.2f}s), ä¿å­˜ä¸º {f} ({file_size(f):.1f} MB)")


@smart_inference_mode()
def run(
    data=ROOT / "data/coco128.yaml",  # 'dataset.yamlè·¯å¾„'
    weights=ROOT / "yolov5s.pt",  # æƒé‡è·¯å¾„
    imgsz=(640, 640),  # å›¾åƒ (é«˜, å®½)
    batch_size=1,  # æ‰¹æ¬¡å¤§å°
    device="cpu",  # cudaè®¾å¤‡ï¼Œä¾‹å¦‚ 0 æˆ– 0,1,2,3 æˆ– cpu
    include=("torchscript", "onnx"),  # åŒ…å«æ ¼å¼
    half=False,  # FP16åŠç²¾åº¦å¯¼å‡º
    inplace=False,  # è®¾ç½®YOLOv5 Detect() inplace=True
    keras=False,  # ä½¿ç”¨Keras
    optimize=False,  # TorchScript: ä¼˜åŒ–ç”¨äºç§»åŠ¨ç«¯
    int8=False,  # CoreML/TF INT8é‡åŒ–
    per_tensor=False,  # TFé€å¼ é‡é‡åŒ–
    dynamic=False,  # ONNX/TF/TensorRT: åŠ¨æ€è½´
    cache="",  # TensorRT: è®¡æ—¶ç¼“å­˜è·¯å¾„
    simplify=False,  # ONNX: ç®€åŒ–æ¨¡å‹
    mlmodel=False,  # CoreML: å¯¼å‡ºä¸º*.mlmodelæ ¼å¼
    opset=12,  # ONNX: opsetç‰ˆæœ¬
    verbose=False,  # TensorRT: è¯¦ç»†æ—¥å¿—
    workspace=4,  # TensorRT: å·¥ä½œç©ºé—´å¤§å° (GB)
    nms=False,  # TF: æ·»åŠ NMSåˆ°æ¨¡å‹
    agnostic_nms=False,  # TF: æ·»åŠ ç±»åˆ«æ— å…³NMSåˆ°æ¨¡å‹
    topk_per_class=100,  # TF.js NMS: æ¯ç±»ä¿ç•™çš„topk
    topk_all=100,  # TF.js NMS: æ‰€æœ‰ç±»åˆ«ä¿ç•™çš„topk
    iou_thres=0.45,  # TF.js NMS: IoUé˜ˆå€¼
    conf_thres=0.25,  # TF.js NMS: ç½®ä¿¡åº¦é˜ˆå€¼
):
    """
    å°†YOLOv5æ¨¡å‹å¯¼å‡ºä¸ºæŒ‡å®šæ ¼å¼ï¼ŒåŒ…æ‹¬ONNXã€TensorRTã€CoreMLå’ŒTensorFlowã€‚

    Args:
        data (str | Path): æ•°æ®é›†YAMLé…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤ä¸º'data/coco128.yaml'ã€‚
        weights (str | Path): é¢„è®­ç»ƒæ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„ã€‚é»˜è®¤ä¸º'yolov5s.pt'ã€‚
        imgsz (tuple): å›¾åƒå°ºå¯¸ï¼Œæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)ã€‚é»˜è®¤ä¸º (640, 640)ã€‚
        batch_size (int): å¯¼å‡ºæ¨¡å‹çš„æ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤ä¸º1ã€‚
        device (str): è¿è¡Œå¯¼å‡ºçš„è®¾å¤‡ï¼Œä¾‹å¦‚'0'è¡¨ç¤ºGPUï¼Œ'cpu'è¡¨ç¤ºCPUã€‚é»˜è®¤ä¸º'cpu'ã€‚
        include (tuple): å¯¼å‡ºä¸­åŒ…å«çš„æ ¼å¼ã€‚é»˜è®¤ä¸º ('torchscript', 'onnx')ã€‚
        half (bool): æ ‡å¿—ï¼Œç”¨äºä»¥FP16åŠç²¾åº¦å¯¼å‡ºæ¨¡å‹ã€‚é»˜è®¤ä¸ºFalseã€‚
        inplace (bool): è®¾ç½®YOLOv5 Detect()æ¨¡å—inplace=Trueã€‚é»˜è®¤ä¸ºFalseã€‚
        keras (bool): æ ‡å¿—ï¼Œç”¨äºTensorFlow SavedModelå¯¼å‡ºæ—¶ä½¿ç”¨Kerasã€‚é»˜è®¤ä¸ºFalseã€‚
        optimize (bool): ä¼˜åŒ–TorchScriptæ¨¡å‹ä»¥ç”¨äºç§»åŠ¨éƒ¨ç½²ã€‚é»˜è®¤ä¸ºFalseã€‚
        int8 (bool): å¯¹CoreMLæˆ–TensorFlowæ¨¡å‹åº”ç”¨INT8é‡åŒ–ã€‚é»˜è®¤ä¸ºFalseã€‚
        per_tensor (bool): å¯¹TensorFlowæ¨¡å‹åº”ç”¨é€å¼ é‡é‡åŒ–ã€‚é»˜è®¤ä¸ºFalseã€‚
        dynamic (bool): ä¸ºONNXã€TensorFlowæˆ–TensorRTå¯¼å‡ºå¯ç”¨åŠ¨æ€è½´ã€‚é»˜è®¤ä¸ºFalseã€‚
        cache (str): TensorRTè®¡æ—¶ç¼“å­˜è·¯å¾„ã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ã€‚
        simplify (bool): åœ¨å¯¼å‡ºæœŸé—´ç®€åŒ–ONNXæ¨¡å‹ã€‚é»˜è®¤ä¸ºFalseã€‚
        opset (int): ONNX opsetç‰ˆæœ¬ã€‚é»˜è®¤ä¸º12ã€‚
        verbose (bool): ä¸ºTensorRTå¯¼å‡ºå¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•ã€‚é»˜è®¤ä¸ºFalseã€‚
        workspace (int): TensorRTå·¥ä½œç©ºé—´å¤§å° (GB)ã€‚é»˜è®¤ä¸º4ã€‚
        nms (bool): å‘TensorFlowæ¨¡å‹æ·»åŠ éæå¤§å€¼æŠ‘åˆ¶ (NMS)ã€‚é»˜è®¤ä¸ºFalseã€‚
        agnostic_nms (bool): å‘TensorFlowæ¨¡å‹æ·»åŠ ç±»åˆ«æ— å…³NMSã€‚é»˜è®¤ä¸ºFalseã€‚
        topk_per_class (int): TensorFlow.js NMSä¸­æ¯ç±»ä¿ç•™çš„Top-Kæ¡†ã€‚é»˜è®¤ä¸º100ã€‚
        topk_all (int): TensorFlow.js NMSä¸­æ‰€æœ‰ç±»åˆ«ä¿ç•™çš„Top-Kæ¡†ã€‚é»˜è®¤ä¸º100ã€‚
        iou_thres (float): NMSçš„IoUé˜ˆå€¼ã€‚é»˜è®¤ä¸º0.45ã€‚
        conf_thres (float): NMSçš„ç½®ä¿¡åº¦é˜ˆå€¼ã€‚é»˜è®¤ä¸º0.25ã€‚
        mlmodel (bool): æ ‡å¿—ï¼Œç”¨äºCoreMLå¯¼å‡ºæ—¶ä½¿ç”¨*.mlmodelã€‚é»˜è®¤ä¸ºFalseã€‚

    Returns:
        None

    Notes:
        - æ¨¡å‹å¯¼å‡ºåŸºäº'include'å‚æ•°ä¸­æŒ‡å®šçš„æ ¼å¼ã€‚
        - è¯·æ³¨æ„æŸäº›æ ‡å¿—ç›¸äº’æ’æ–¥çš„ç»„åˆï¼Œä¾‹å¦‚`--half`å’Œ`--dynamic`ã€‚

    Example:
        ```python
        run(
            data="data/coco128.yaml",
            weights="yolov5s.pt",
            imgsz=(640, 640),
            batch_size=1,
            device="cpu",
            include=("torchscript", "onnx"),
            half=False,
            inplace=False,
            keras=False,
            optimize=False,
            int8=False,
            per_tensor=False,
            dynamic=False,
            cache="",
            simplify=False,
            opset=12,
            verbose=False,
            mlmodel=False,
            workspace=4,
            nms=False,
            agnostic_nms=False,
            topk_per_class=100,
            topk_all=100,
            iou_thres=0.45,
            conf_thres=0.25,
        )
        ```
    """
    t = time.time()
    include = [x.lower() for x in include]  # è½¬æ¢ä¸ºå°å†™
    fmts = tuple(export_formats()["Argument"][1:])  # --includeå‚æ•°
    flags = [x in include for x in fmts]
    assert sum(flags) == len(include), f"é”™è¯¯: æ— æ•ˆçš„--include {include}, æœ‰æ•ˆçš„--includeå‚æ•°æ˜¯ {fmts}"
    jit, onnx, xml, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle = flags  # å¯¼å‡ºå¸ƒå°”å€¼
    file = Path(url2file(weights) if str(weights).startswith(("http:/", "https:/")) else weights)  # PyTorchæƒé‡

    # åŠ è½½PyTorchæ¨¡å‹
    device = select_device(device)
    if half:
        assert device.type != "cpu" or coreml, "--halfä»…ä¸GPUå¯¼å‡ºå…¼å®¹ï¼Œä¾‹å¦‚ä½¿ç”¨--device 0"
        assert not dynamic, "--halfä¸--dynamicä¸å…¼å®¹ï¼Œä¾‹å¦‚ä½¿ç”¨--halfæˆ–--dynamicä½†ä¸èƒ½åŒæ—¶ä½¿ç”¨"
    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # åŠ è½½FP32æ¨¡å‹

    # æ£€æŸ¥
    imgsz *= 2 if len(imgsz) == 1 else 1  # æ‰©å±•
    if optimize:
        assert device.type == "cpu", "--optimizeä¸cudaè®¾å¤‡ä¸å…¼å®¹ï¼Œä¾‹å¦‚ä½¿ç”¨--device cpu"

    # è¾“å…¥
    gs = int(max(model.stride))  # ç½‘æ ¼å¤§å° (æœ€å¤§æ­¥é•¿)
    imgsz = [check_img_size(x, gs) for x in imgsz]  # éªŒè¯img_sizeæ˜¯gsçš„å€æ•°
    ch = next(model.parameters()).size(1)  # éœ€è¦è¾“å…¥å›¾åƒé€šé“
    im = torch.zeros(batch_size, ch, *imgsz).to(device)  # å›¾åƒå°ºå¯¸(1,3,320,192) BCHW iDetection

    # æ›´æ–°æ¨¡å‹
    model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    for k, m in model.named_modules():
        if isinstance(m, Detect):
            m.inplace = inplace
            m.dynamic = dynamic
            m.export = True

    for _ in range(2):
        y = model(im)  # ç©ºè¿è¡Œ
    if half and not coreml:
        im, model = im.half(), model.half()  # è½¬æ¢ä¸ºFP16
    shape = tuple((y[0] if isinstance(y, tuple) else y).shape)  # æ¨¡å‹è¾“å‡ºå½¢çŠ¶
    metadata = {"stride": int(max(model.stride)), "names": model.names}  # æ¨¡å‹å…ƒæ•°æ®
    LOGGER.info(f"\n{colorstr('PyTorch:')} starting from {file} with output shape {shape} ({file_size(file):.1f} MB)")

    # å¯¼å‡º
    f = [""] * len(fmts)  # å¯¼å‡ºæ–‡ä»¶å
    warnings.filterwarnings(action="ignore", category=torch.jit.TracerWarning)  # æŠ‘åˆ¶TracerWarning
    if jit:  # TorchScript
        f[0], _ = export_torchscript(model, im, file, optimize)
    if engine:  # TensorRTåœ¨ONNXä¹‹å‰éœ€è¦
        f[1], _ = export_engine(model, im, file, half, dynamic, simplify, workspace, verbose, cache)
    if onnx or xml:  # OpenVINOéœ€è¦ONNX
        f[2], _ = export_onnx(model, im, file, opset, dynamic, simplify)
    if xml:  # OpenVINO
        f[3], _ = export_openvino(file, metadata, half, int8, data)
    if coreml:  # CoreML
        f[4], ct_model = export_coreml(model, im, file, int8, half, nms, mlmodel)
        if nms:
            pipeline_coreml(ct_model, im, file, model.names, y, mlmodel)
    if any((saved_model, pb, tflite, edgetpu, tfjs)):  # TensorFlowæ ¼å¼
        assert not tflite or not tfjs, "TFLiteå’ŒTF.jsæ¨¡å‹å¿…é¡»å•ç‹¬å¯¼å‡ºï¼Œè¯·ä»…ä¼ é€’ä¸€ç§ç±»å‹ã€‚"
        assert not isinstance(model, ClassificationModel), "åˆ†ç±»æ¨¡å‹å¯¼å‡ºåˆ°TFæ ¼å¼å°šä¸æ”¯æŒã€‚"
        f[5], s_model = export_saved_model(
            model.cpu(),
            im,
            file,
            dynamic,
            tf_nms=nms or agnostic_nms or tfjs,
            agnostic_nms=agnostic_nms or tfjs,
            topk_per_class=topk_per_class,
            topk_all=topk_all,
            iou_thres=iou_thres,
            conf_thres=conf_thres,
            keras=keras,
        )
        if pb or tfjs:  # pbæ˜¯tfjsçš„å…ˆå†³æ¡ä»¶
            f[6], _ = export_pb(s_model, file)
        if tflite or edgetpu:
            f[7], _ = export_tflite(
                s_model, im, file, int8 or edgetpu, per_tensor, data=data, nms=nms, agnostic_nms=agnostic_nms
            )
            if edgetpu:
                f[8], _ = export_edgetpu(file)
            add_tflite_metadata(f[8] or f[7], metadata, num_outputs=len(s_model.outputs))
        if tfjs:
            f[9], _ = export_tfjs(file, int8)
    if paddle:  # PaddlePaddle
        f[10], _ = export_paddle(model, im, file, metadata)

    # å®Œæˆ
    f = [str(x) for x in f if x]  # è¿‡æ»¤æ‰''å’ŒNone
    if any(f):
        cls, det, seg = (isinstance(model, x) for x in (ClassificationModel, DetectionModel, SegmentationModel))  # ç±»å‹
        det &= not seg  # åˆ†å‰²æ¨¡å‹ç»§æ‰¿è‡ªSegmentationModel(DetectionModel)
        dir = Path("segment" if seg else "classify" if cls else "")
        h = "--half" if half else ""  # --half FP16æ¨ç†å‚æ•°
        s = (
            "# WARNING âš ï¸ ClassificationModel not yet supported for PyTorch Hub AutoShape inference"
            if cls
            else "# WARNING âš ï¸ SegmentationModel not yet supported for PyTorch Hub AutoShape inference"
            if seg
            else ""
        )
        LOGGER.info(
            f"\nå¯¼å‡ºå®Œæˆ ({time.time() - t:.1f}s)"
            f"\nç»“æœä¿å­˜åˆ° {colorstr('bold', file.parent.resolve())}"
            f"\næ£€æµ‹:          python {dir / ('detect.py' if det else 'predict.py')} --weights {f[-1]} {h}"
            f"\néªŒè¯:        python {dir / 'val.py'} --weights {f[-1]} {h}"
            f"\nPyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '{f[-1]}')  {s}"
            f"\nå¯è§†åŒ–:       https://netron.app"
        )
    return f  # è¿”å›å¯¼å‡ºæ–‡ä»¶/ç›®å½•åˆ—è¡¨


def parse_opt(known=False):
    """
    è§£æYOLOv5æ¨¡å‹å¯¼å‡ºé…ç½®çš„å‘½ä»¤è¡Œé€‰é¡¹ã€‚

    Args:
        known (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨`argparse.ArgumentParser.parse_known_args`ï¼›å¦åˆ™ï¼Œä½¿ç”¨`argparse.ArgumentParser.parse_args`ã€‚
                      é»˜è®¤ä¸ºFalseã€‚

    Returns:
        argparse.Namespace: åŒ…å«è§£æåçš„å‘½ä»¤è¡Œå‚æ•°çš„å¯¹è±¡ã€‚

    Example:
        ```python
        opts = parse_opt()
        print(opts.data)
        print(opts.weights)
        ```
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, default=ROOT / "data/coco128.yaml", help="dataset.yaml path")
    parser.add_argument("--weights", nargs="+", type=str, default=ROOT / "yolov5s.pt", help="model.pt path(s)")
    parser.add_argument("--imgsz", "--img", "--img-size", nargs="+", type=int, default=[640, 640], help="image (h, w)")
    parser.add_argument("--batch-size", type=int, default=1, help="batch size")
    parser.add_argument("--device", default="cpu", help="cuda device, i.e. 0 or 0,1,2,3 or cpu")
    parser.add_argument("--half", action="store_true", help="FP16 half-precision export")
    parser.add_argument("--inplace", action="store_true", help="set YOLOv5 Detect() inplace=True")
    parser.add_argument("--keras", action="store_true", help="TF: use Keras")
    parser.add_argument("--optimize", action="store_true", help="TorchScript: optimize for mobile")
    parser.add_argument("--int8", action="store_true", help="CoreML/TF/OpenVINO INT8 quantization")
    parser.add_argument("--per-tensor", action="store_true", help="TF per-tensor quantization")
    parser.add_argument("--dynamic", action="store_true", help="ONNX/TF/TensorRT: dynamic axes")
    parser.add_argument("--cache", type=str, default="", help="TensorRT: timing cache file path")
    parser.add_argument("--simplify", action="store_true", help="ONNX: simplify model")
    parser.add_argument("--mlmodel", action="store_true", help="CoreML: Export in *.mlmodel format")
    parser.add_argument("--opset", type=int, default=17, help="ONNX: opset version")
    parser.add_argument("--verbose", action="store_true", help="TensorRT: verbose log")
    parser.add_argument("--workspace", type=int, default=4, help="TensorRT: workspace size (GB)")
    parser.add_argument("--nms", action="store_true", help="TF: add NMS to model")
    parser.add_argument("--agnostic-nms", action="store_true", help="TF: add agnostic NMS to model")
    parser.add_argument("--topk-per-class", type=int, default=100, help="TF.js NMS: topk per class to keep")
    parser.add_argument("--topk-all", type=int, default=100, help="TF.js NMS: topk for all classes to keep")
    parser.add_argument("--iou-thres", type=float, default=0.45, help="TF.js NMS: IoU threshold")
    parser.add_argument("--conf-thres", type=float, default=0.25, help="TF.js NMS: confidence threshold")
    parser.add_argument(
        "--include",
        nargs="+",
        default=["torchscript"],
        help="torchscript, onnx, openvino, engine, coreml, saved_model, pb, tflite, edgetpu, tfjs, paddle",
    )
    opt = parser.parse_known_args()[0] if known else parser.parse_args()
    print_args(vars(opt)) # æ‰“å°å‚æ•°
    return opt


def main(opt):
    """Run(**vars(opt))  # ä½¿ç”¨è§£æçš„é€‰é¡¹æ‰§è¡Œrunå‡½æ•°ã€‚"""
    for opt.weights in opt.weights if isinstance(opt.weights, list) else [opt.weights]:
        run(**vars(opt))


if __name__ == "__main__":
    opt = parse_opt() # è§£æå‘½ä»¤è¡Œå‚æ•°
    main(opt) # è°ƒç”¨ä¸»å‡½æ•°
