{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 笔记\n",
    "\n",
    "## 概括(AI 生成+自己完善)\n",
    "\n",
    "本教程聚焦于 PyTorch 的核心数据结构——**张量 (Tensor)**。它详细介绍了张量的基本概念、多种初始化方法（包括从 Python 数据结构、NumPy 数组创建，或根据形状创建特定类型的张量），以及张量的重要属性（如形状 `shape`、数据类型 `dtype` 和存储设备 `device`）。此外，教程还演示了丰富的张量操作，涵盖了类似 NumPy 的索引与切片、张量的拼接 (`torch.cat`)、算术运算（如矩阵乘法 `torch.matmul` 和逐元素乘法 `torch.mul`）、聚合操作 (`tensor.sum()`) 以及原地操作的特性和注意事项。最后，教程特别强调了 PyTorch 张量与 NumPy 数组之间的桥梁作用，解释了它们在 CPU 上如何共享内存以及相互转换的方法 (`tensor.numpy()` 和 `torch.from_numpy()`)。\n",
    "\n",
    "## 关键函数、语法(自己总结+AI 优化)\n",
    "\n",
    "- `torch.tensor(data, dtype=None, device=None, requires_grad=False)`: 从 Python 列表、序列或 NumPy 数组等数据直接创建张量。可以显式指定数据类型 (`dtype`)、张量所在的设备 (`device`) 以及是否需要计算梯度 (`requires_grad`)。数据类型会自动推断（如果未指定）。\n",
    "- `torch.from_numpy(ndarray)`: 将 NumPy 数组转换为 PyTorch 张量。**重要特性**：当 NumPy 数组和转换后的张量都位于 **CPU** 时，它们将共享底层内存，此时修改其中一个会影响另一个。\n",
    "- `torch.ones_like(input_tensor, dtype=None, ...)`: 根据 `input_tensor` 的形状创建一个全为1的新张量。可以覆盖 `input_tensor` 的数据类型等属性。\n",
    "- `torch.rand_like(input_tensor, dtype=None, ...)`: 根据 `input_tensor` 的形状创建一个新张量，其元素是在区间 $[0, 1)$ 上均匀分布的随机数。可以覆盖数据类型等属性。\n",
    "- `torch.rand(*size, dtype=None, ...)`: 创建一个指定形状 `size` (一个整数序列或元组) 的张量，其元素是在区间 $[0, 1)$ 上均匀分布的随机数。\n",
    "- `torch.ones(*size, dtype=None, ...)`: 创建一个指定形状 `size` 的全1张量。\n",
    "- `torch.zeros(*size, dtype=None, ...)`: 创建一个指定形状 `size` 的全0张量。\n",
    "- `tensor.shape` 或 `tensor.size()`: 张量的属性或方法，返回一个 `torch.Size` 对象 (类似于元组)，表示张量在各个维度上的大小。\n",
    "- `tensor.dtype`: 张量的属性，返回张量中元素的数据类型，例如 `torch.float32` (32位浮点数)、`torch.int64` (64位整数) 等。\n",
    "- `tensor.device`: 张量的属性，返回张量当前存储的设备，例如 `cpu` 或 `cuda:0` (表示第一个GPU)。\n",
    "- **张量索引与切片 (Tensor Indexing and Slicing)**: PyTorch 张量支持类似 NumPy 的索引和切片机制，可以用来方便地访问和修改张量的部分元素。\n",
    "    -   例如：`tensor[0]` 获取第一行（对于2D张量），`tensor[:, 0]` 获取第一列，`tensor[..., -1]` 获取最后一个维度的最后一个元素。\n",
    "    -   切片赋值也是支持的，如 `tensor[:, 1] = 0` 将第二列所有元素置0。\n",
    "- `torch.cat(tensors, dim=0, *, out=None)`: 将一个张量序列 `tensors` (例如 Python 列表中的多个张量) 沿着指定的维度 `dim` 进行拼接。如果各个张量在**非拼接维度**上的大小不一致，**会报错**！没有广播机制。可以先使用 `torch.Tensor.resize_(*size)` 调整大小。\n",
    "- `tensor.T`: 张量的属性，返回该张量的转置。对于一个 $n$ 维张量，如果 $n \\ge 2$，则交换最后两个维度；如果 $n < 2$，则返回原张量。对于二维张量（矩阵），这等同于传统的矩阵转置。\n",
    "- `torch.matmul(input, other, *, out=None)` 或 `input @ other`: 计算两个张量的矩阵乘积。需要注意参与运算的张量的维度必须符合矩阵乘法的要求。\n",
    "- `torch.mul(input, other, *, out=None)` 或 `input * other`: 计算两个张量的逐元素乘积 (element-wise product)。要求两个张量的形状是可广播 (broadcastable) 的。\n",
    "- `tensor.sum(dim=None, keepdim=False, dtype=None)`: 计算张量中所有元素的和。如果指定了 `dim` 参数 (一个整数或整数元组)，则沿该维度/些维度求和。`keepdim=True` 会在结果中保留被求和的维度，其大小为1，否则结果的维度会降低。\n",
    "- **原地操作 (In-place operations)**: PyTorch中，许多操作都有一个以 `_` (下划线) 结尾的版本，例如 `tensor.add_(value)`、`tensor.copy_(other_tensor)`、`tensor.t_()`。这些操作会直接修改调用它们的张量本身，而不是返回一个新的张量。这样做可以节省内存，但在需要保留原始张量或涉及自动求导历史时需要谨慎使用，因为它们会丢失原始张量的历史记录。\n",
    "- `tensor.numpy()`: 将一个存储在 **CPU** 上的 PyTorch 张量转换为 NumPy 数组。转换后的 NumPy 数组与原始的 CPU 张量共享底层内存，因此修改一方会直接影响另一方。如果张量存储在 GPU 上，需要先用 `.cpu()` 方法将其转移到 CPU 才能调用 `.numpy()`。\n",
    "\n",
    "## 提问与解答(自己提问+AI 回答)\n",
    "\n",
    "- **问题1 (对于 \"Initializing a Tensor\" 从 NumPy 数组创建张量这一部分):**\n",
    "  教程提到 `torch.from_numpy(ndarray)` 创建的张量在 CPU 上会与 NumPy 数组共享内存。那么，使用 `torch.tensor(numpy_array)` 从 NumPy 数组创建张量时，是否也会共享内存？这两种方法在从 NumPy 数组创建张量时，主要区别是什么？各自的典型使用场景是什么？\n",
    "\n",
    "  - **解答:**\n",
    "    1.  **内存共享**:\n",
    "        -   `torch.from_numpy(ndarray)`: 当 NumPy 数组在 CPU 上且其数据类型与 PyTorch 兼容时，此函数创建的张量将**共享** NumPy 数组的内存。修改一方会影响另一方。\n",
    "        -   `torch.tensor(numpy_array)`: 此函数**总是复制** NumPy 数组的数据来创建一个新的张量。因此，新张量拥有独立的内存，修改一方不会影响另一方。\n",
    "\n",
    "    2.  **主要区别与使用场景**:\n",
    "        -   **`torch.from_numpy(ndarray)`**:\n",
    "            -   **优点**: 由于共享内存（在满足条件时），转换非常高效，避免了数据复制的开销。\n",
    "            -   **缺点**: 因为共享内存，如果原始 NumPy 数组在其他地方被修改，张量的值也会改变，反之亦然，这可能导致意外行为。此外，它继承 NumPy 数组的数据类型，不能在创建时直接指定新的 `dtype`。\n",
    "            -   **使用场景**: 当你确定不需要独立的副本，并且希望在 NumPy 和 PyTorch 之间高效传递数据（尤其是大型数组）时，例如，在数据预处理阶段使用 NumPy，然后将数据传递给 PyTorch 模型而不需要修改原始 NumPy 数据。\n",
    "        -   **`torch.tensor(numpy_array)`**:\n",
    "            -   **优点**: 创建一个独立的数据副本，因此后续对张量或 NumPy 数组的修改互不影响，更安全。允许在创建时通过 `dtype` 参数指定张量的数据类型，即使它与 NumPy 数组的类型不同。功能更通用，不仅限于 NumPy 数组，还可以从 Python 列表等创建。\n",
    "            -   **缺点**: 涉及数据复制，对于非常大的数组可能会有性能和内存开销。\n",
    "            -   **使用场景**: 当你需要一个与原始 NumPy 数组解耦的张量副本时，或者当你需要从 NumPy 数组转换并同时改变数据类型时。作为通用的张量创建函数，它的适用性更广。\n",
    "\n",
    "    简而言之，如果首要考虑的是避免数据复制的效率且能接受数据共享的副作用，选择 `torch.from_numpy()`。如果需要数据隔离的安全性或在创建时指定数据类型，则选择 `torch.tensor()`。\n",
    "\n",
    "    参考资料：\n",
    "    - PyTorch 官方文档: `torch.tensor` ([https://pytorch.org/docs/stable/generated/torch.tensor.html](https://pytorch.org/docs/stable/generated/torch.tensor.html)) 和 `torch.from_numpy` ([https://pytorch.org/docs/stable/generated/torch.from_numpy.html](https://pytorch.org/docs/stable/generated/torch.from_numpy.html))\n",
    "\n",
    "- **问题2 (基于教程中 \"Joining tensors\" 部分的 `torch.cat` 与 `torch.stack`):**\n",
    "  教程中介绍了使用 `torch.cat` 来拼接张量，并提到 `torch.stack` 是一个与之“微妙不同”(subtly different)的张量连接操作。能否详细解释一下 `torch.cat` 和 `torch.stack` 之间的主要区别是什么？它们各自适用于什么场景？最好能给出一个简单的例子来说明。\n",
    "\n",
    "  - **解答:**\n",
    "    `torch.cat` 和 `torch.stack` 都是用于组合多个张量的函数，但它们的工作方式和结果有所不同：\n",
    "\n",
    "    1.  **`torch.cat(tensors, dim=0)` (Concatenate - 拼接):**\n",
    "        -   **作用**: 沿着一个**已存在的维度**将一系列张量连接起来。\n",
    "        -   **要求**: 输入的张量序列在非拼接维度上必须具有相同的大小。\n",
    "        -   **结果**: 输出张量的维度数量与输入张量相同（或更准确地说，与具有最多维度的输入张量相同，但通常输入张量维度一致）。拼接维度的大小是输入张量在该维度大小的总和。\n",
    "        -   **场景**: 当你想要扩展一个已有的维度时。例如，将多个批次的数据合并成一个更大的批次，或者将特征图在通道维度上拼接。\n",
    "\n",
    "    2.  **`torch.stack(tensors, dim=0)` (Stack - 堆叠):**\n",
    "        -   **作用**: 沿着一个**新的维度**将一系列张量堆叠起来。\n",
    "        -   **要求**: 输入的张量序列必须具有完全相同的形状。\n",
    "        -   **结果**: 输出张量的维度数量比输入张量多一。新的维度被插入到 `dim` 参数指定的位置。\n",
    "        -   **场景**: 当你有一系列独立的、相同形状的样本或组件，并希望将它们组合成一个更高维的集合时。例如，将多张单独的图片（每张都是一个张量）堆叠成一个批次的图片（一个新的维度代表批次中的索引）。\n",
    "\n",
    "    **简单示例:**\n",
    "    假设有两个形状为 `(2, 3)` 的张量：\n",
    "    ```python\n",
    "    import torch\n",
    "    t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "    ```\n",
    "    -   **使用 `torch.cat`:**\n",
    "        ```python\n",
    "        # 沿着维度 0 (行) 拼接\n",
    "        cat_dim0 = torch.cat((t1, t2), dim=0)\n",
    "        # cat_dim0 的形状会是 (4, 3)\n",
    "        # tensor([[ 1,  2,  3],\n",
    "        #         [ 4,  5,  6],\n",
    "        #         [ 7,  8,  9],\n",
    "        #         [10, 11, 12]])\n",
    "\n",
    "        # 沿着维度 1 (列) 拼接\n",
    "        cat_dim1 = torch.cat((t1, t2), dim=1)\n",
    "        # cat_dim1 的形状会是 (2, 6)\n",
    "        # tensor([[ 1,  2,  3,  7,  8,  9],\n",
    "        #         [ 4,  5,  6, 10, 11, 12]])\n",
    "        ```\n",
    "    -   **使用 `torch.stack`:**\n",
    "        ```python\n",
    "        # 沿着新的维度 0 堆叠\n",
    "        stack_dim0 = torch.stack((t1, t2), dim=0)\n",
    "        # stack_dim0 的形状会是 (2, 2, 3)\n",
    "        # tensor([[[ 1,  2,  3],\n",
    "        #          [ 4,  5,  6]],\n",
    "        #\n",
    "        #         [[ 7,  8,  9],\n",
    "        #          [10, 11, 12]]])\n",
    "\n",
    "        # 沿着新的维度 1 堆叠\n",
    "        stack_dim1 = torch.stack((t1, t2), dim=1)\n",
    "        # stack_dim1 的形状会是 (2, 2, 3)\n",
    "        # tensor([[[ 1,  2,  3],\n",
    "        #          [ 7,  8,  9]],\n",
    "        #\n",
    "        #         [[ 4,  5,  6],\n",
    "        #          [10, 11, 12]]])\n",
    "        ```\n",
    "\n",
    "    总结：`cat` 是在现有维度上“追加”内容，而 `stack` 是创建一个新的维度来“组织”这些张量。\n",
    "\n",
    "    参考资料：\n",
    "    - PyTorch 官方文档: `torch.cat` ([https://pytorch.org/docs/stable/generated/torch.cat.html](https://pytorch.org/docs/stable/generated/torch.cat.html)) 和 `torch.stack` ([https://pytorch.org/docs/stable/generated/torch.stack.html](https://pytorch.org/docs/stable/generated/torch.stack.html))\n",
    "\n",
    "- **问题3 (关于 \"In-place operations\" 的说明):**\n",
    "  教程中提到原地操作（例如 `tensor.add_()`）会节省一些内存，但“在计算导数时可能会因为立即丢失历史记录而产生问题”，因此不鼓励使用。能否更详细地解释一下“丢失历史记录”在 PyTorch 的自动求导 (Autograd) 系统中具体指什么？为什么这会对导数计算产生问题？是否存在一些即使张量 `requires_grad=True`，也可以使用原地操作的场景？\n",
    "\n",
    "  - **解答:**\n",
    "    1.  **“丢失历史记录”与 Autograd:**\n",
    "        PyTorch 的自动求导系统 (Autograd) 通过构建一个**计算图 (computation graph)** 来跟踪对设置了 `requires_grad=True` 的张量所执行的操作。这个图是一个有向无环图 (DAG)，其中叶节点是输入张量，根节点是最终的输出（通常是损失函数）。当调用 `loss.backward()` 时，Autograd 会沿着这个图反向传播，计算梯度。\n",
    "        每个操作（如加法、乘法）在计算图中都会成为一个节点，它会存储足够的信息以便在反向传播时计算其输入的梯度。这些信息可能包括操作本身的类型、其输入张量的值或形状等。\n",
    "        **原地操作** (in-place operation) 直接修改了张量自身的内容。如果一个张量 $A$ 经过某个操作 $f$ 得到张量 $B$，然后张量 $A$ 被一个原地操作修改了（例如 `A.add_(1)`），那么当 Autograd 需要计算依赖于原始 $A$ 值的梯度时（比如通过操作 $f$ 间接依赖），它会发现 $A$ 的内容已经变了。原始的 $A$ 值（即参与操作 $f$ 时的那个值）就“丢失”了，Autograd 可能无法正确计算梯度。这就是所谓的“丢失历史记录”。\n",
    "\n",
    "    2.  **为什么对导数计算产生问题:**\n",
    "        梯度计算通常依赖于前向传播过程中的中间值。例如，对于 $y = w \\cdot x$，$\\frac{\\partial y}{\\partial x} = w$ 和 $\\frac{\\partial y}{\\partial w} = x$。如果 $x$ 在计算 $y$ 之后被原地修改，那么在计算 $\\frac{\\partial y}{\\partial w}$ 时就无法获取到正确的原始 $x$ 值，导致梯度计算错误。PyTorch 的 Autograd 引擎会尝试检测这种情况，并通常会抛出 `RuntimeError`，提示一个用于梯度计算的变量已经被原地修改。\n",
    "\n",
    "    3.  **谨慎使用原地操作的场景 (当 `requires_grad=True`):**\n",
    "        尽管通常不鼓励，但在某些严格控制的条件下，原地操作可能被使用：\n",
    "        -   **叶子节点**: 如果一个张量是计算图的叶子节点（即它不是任何其他需要梯度的操作的输出），并且你确定在它被原地修改后，它的原始值不再需要用于任何后续的梯度计算，那么对它进行原地操作可能是安全的。然而，即使是叶子节点，如果它的值被用于多个后续分支，且其中一个分支在原地修改它之前完成了前向传播，而另一个分支在其被修改后才使用它，也会导致问题。\n",
    "        -   **不依赖于被修改值的梯度**: 如果某个操作的梯度计算不依赖于其输入张量的原始值（这种情况比较少见），那么对该输入进行原地修改可能不会影响梯度。\n",
    "        -   **明确的计算图分离**: 在一些高级用法中，用户可能通过 `torch.no_grad()` 或 `.detach()` 等方式显式地将某部分计算从梯度跟踪中分离出来，然后在这些部分谨慎使用原地操作。\n",
    "\n",
    "        PyTorch 的 Autograd 引擎会进行一些检查。如果一个操作需要保存其输入用于反向传播，而该输入被原地修改了，Autograd 会报错。例如：\n",
    "        ```python\n",
    "        # a = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "        # b = a.clone() # b 依赖 a\n",
    "        # c = b * 2\n",
    "        # b.add_(1) # 原地修改 b，b 是 c 的输入，c.backward() 需要原始的 b\n",
    "        # # c.sum().backward() # 这通常会引发 RuntimeError\n",
    "        ```\n",
    "        **总的来说，除非你对计算图和 Autograd 的内部机制有非常深入的理解，并且有强烈的内存优化需求，否则对于 `requires_grad=True` 的张量，最佳实践是避免使用原地操作，以确保梯度计算的正确性和代码的清晰性。** 使用非原地操作（如 `new_tensor = old_tensor + 1` 而不是 `old_tensor.add_(1)`）通常更安全。\n",
    "\n",
    "    参考资料：\n",
    "    - PyTorch 官方文档中关于 Autograd 的说明: [https://pytorch.org/docs/stable/notes/autograd.html](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "    - PyTorch 官方文档中关于原地操作的警告: [https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\|\n",
    "[Quickstart](quickstart_tutorial.html) \\|\\| **Tensors** \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
    "Model](buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Tensors\n",
    "=======\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays\n",
    "and matrices. In PyTorch, we use tensors to encode the inputs and\n",
    "outputs of a model, as well as the model's parameters.\n",
    "\n",
    "Tensors are similar to [NumPy's](https://numpy.org/) ndarrays, except\n",
    "that tensors can run on GPUs or other hardware accelerators. In fact,\n",
    "tensors and NumPy arrays can often share the same underlying memory,\n",
    "eliminating the need to copy data (see\n",
    "`bridge-to-np-label`{.interpreted-text role=\"ref\"}). Tensors are also\n",
    "optimized for automatic differentiation (we\\'ll see more about that\n",
    "later in the [Autograd](autogradqs_tutorial.html) section). If you're\n",
    "familiar with ndarrays, you'll be right at home with the Tensor API. If\n",
    "not, follow along!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a Tensor\n",
    "=====================\n",
    "\n",
    "Tensors can be initialized in various ways. Take a look at the following\n",
    "examples:\n",
    "\n",
    "**Directly from data**\n",
    "\n",
    "Tensors can be created directly from data. The data type is\n",
    "automatically inferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From a NumPy array**\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see\n",
    "`bridge-to-np-label`{.interpreted-text role=\"ref\"}).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From another tensor:**\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument\n",
    "tensor, unless explicitly overridden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.7227, 0.7035],\n",
      "        [0.2945, 0.4366]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With random or constant values:**\n",
    "\n",
    "`shape` is a tuple of tensor dimensions. In the functions below, it\n",
    "determines the dimensionality of the output tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Tensor: \n",
      " tensor([[0.9597, 0.4914, 0.4755],\n",
      "        [0.4816, 0.7301, 0.1664]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attributes of a Tensor\n",
    "======================\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on\n",
    "which they are stored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations on Tensors\n",
    "=====================\n",
    "\n",
    "Over 1200 tensor operations, including arithmetic, linear algebra,\n",
    "matrix manipulation (transposing, indexing, slicing), sampling and more\n",
    "are comprehensively described\n",
    "[here](https://pytorch.org/docs/stable/torch.html).\n",
    "\n",
    "Each of these operations can be run on the CPU and\n",
    "[Accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
    "such as CUDA, MPS, MTIA, or XPU. If you're using Colab, allocate an\n",
    "accelerator by going to Runtime \\> Change runtime type \\> GPU.\n",
    "\n",
    "By default, tensors are created on the CPU. We need to explicitly move\n",
    "tensors to the accelerator using `.to` method (after checking for\n",
    "accelerator availability). Keep in mind that copying large tensors\n",
    "across devices can be expensive in terms of time and memory!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the current accelerator if available\n",
    "# if torch.accelerator.is_available():\n",
    "#     tensor = tensor.to(torch.accelerator.current_accelerator())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out some of the operations from the list. If you\\'re familiar with\n",
    "the NumPy API, you\\'ll find the Tensor API a breeze to use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard numpy-like indexing and slicing:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row: tensor([1., 1., 1., 1.])\n",
      "First column: tensor([1., 1., 1., 1.])\n",
      "Last column: tensor([1., 1., 1., 1.])\n",
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:, 0]}\")\n",
    "print(f\"Last column: {tensor[..., -1]}\")\n",
    "tensor[:,1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joining tensors** You can use `torch.cat` to concatenate a sequence of\n",
    "tensors along a given dimension. See also\n",
    "[torch.stack](https://pytorch.org/docs/stable/generated/torch.stack.html),\n",
    "another tensor joining operator that is subtly different from\n",
    "`torch.cat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arithmetic operations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.],\n",
       "        [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value\n",
    "# ``tensor.T`` returns the transpose of a tensor\n",
    "y1 = tensor @ tensor.T\n",
    "y2 = tensor.matmul(tensor.T)\n",
    "\n",
    "y3 = torch.rand_like(y1)\n",
    "torch.matmul(tensor, tensor.T, out=y3)\n",
    "\n",
    "\n",
    "# This computes the element-wise product. z1, z2, z3 will have the same value\n",
    "z1 = tensor * tensor\n",
    "z2 = tensor.mul(tensor)\n",
    "\n",
    "z3 = torch.rand_like(tensor)\n",
    "torch.mul(tensor, tensor, out=z3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Single-element tensors** If you have a one-element tensor, for example\n",
    "by aggregating all values of a tensor into one value, you can convert it\n",
    "to a Python numerical value using `item()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "agg = tensor.sum()\n",
    "agg_item = agg.item()\n",
    "print(agg_item, type(agg_item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In-place operations** Operations that store the result into the\n",
    "operand are called in-place. They are denoted by a `_` suffix. For\n",
    "example: `x.copy_(y)`, `x.t_()`, will change `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]]) \n",
      "\n",
      "tensor([[6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.],\n",
      "        [6., 5., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tensor} \\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>In-place operations save some memory, but can be problematic when computing derivatives because of an immediate lossof history. Hence, their use is discouraged.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bridge with NumPy {#bridge-to-np-label}\n",
    "=================\n",
    "\n",
    "Tensors on the CPU and NumPy arrays can share their underlying memory\n",
    "locations, and changing one will change the other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor to NumPy array\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A change in the tensor reflects in the NumPy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy array to Tensor\n",
    "=====================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes in the NumPy array reflects in the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
