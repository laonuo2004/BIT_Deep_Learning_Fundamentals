{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 笔记\n",
    "\n",
    "## 概括(AI 生成+自己完善)\n",
    "\n",
    "该教程通过一个具体的**图像分类任务**——对 **FashionMNIST 数据集**中的衣物图片进行分类——来引导初学者入门 PyTorch。其核心目标是展示如何利用 PyTorch 完成一个典型的深度学习项目流程：从数据准备、模型构建，到模型训练、评估，以及最终的模型保存与加载。\n",
    "\n",
    "教程主要介绍了以下几个方面的 PyTorch 核心概念和操作：\n",
    "\n",
    "- **数据处理与加载**：如何使用 `Dataset` 和 `DataLoader` 来高效地准备和供给数据。\n",
    "- **模型构建**：如何通过继承 `nn.Module` 来定义神经网络结构，并使用预定义的层 (如线性层、激活函数)。\n",
    "- **模型训练**：如何定义损失函数、选择优化器，并实现标准的训练循环 (前向传播、计算损失、反向传播、更新参数)。\n",
    "- **模型评估**：如何在测试集上评估模型性能。\n",
    "- **模型保存与加载**：如何保存训练好的模型参数，并在需要时重新加载使用。\n",
    "\n",
    "## 关键函数、语法(自己总结+AI 优化)\n",
    "\n",
    "- `torch.utils.data.Dataset`: 数据集的抽象基类。自定义数据集时，需继承并实现 `__len__` (返回数据集大小) 和 `__getitem__` (按索引获取数据样本) 两个核心方法。\n",
    "- `torch.utils.data.DataLoader`: 数据加载器。封装 `Dataset` 以提供数据的迭代访问，支持批处理 (batching)、打乱 (shuffling) 及多进程加载等功能。详细的可以参考官方文档：[https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader](https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- `torchvision.datasets`: `torchvision` 库的一部分，提供常用的计算机视觉数据集，如本教程中使用的 `FashionMNIST`。可通过参数指定训练/测试集、下载及预处理转换。\n",
    "- `torchvision.transforms.ToTensor`: 一种数据预处理转换。将 PIL.Image 或 `numpy.ndarray` (H x W x C) 格式的图像转换为 `torch.Tensor` (C x H x W)，并将像素值从 `[0, 255]` 归一化到 `[0.0, 1.0]`。\n",
    "- `torch.cuda.is_available()`: 返回布尔值的函数，检测当前系统是否安装并配置了 NVIDIA CUDA 支持，用于判断是否可以将计算迁移到 GPU。\n",
    "- `torch.nn.Module`: 所有神经网络层的基类。自定义网络模型必须继承此类。核心功能包括参数注册与管理、子模块嵌套、设备迁移 (`.to(device)`) 及状态保存/加载 (`state_dict`, `load_state_dict`)。详细的可以参考官方文档：[https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module)\n",
    "- `__init__(self)` (在 `nn.Module` 子类中): 构造方法。在此处实例化并注册网络层 (如 `nn.Linear`, `nn.Conv2d`) 及其他子模块作为类的属性。掌握了面向对象的相关基础知识就不难理解。\n",
    "- `forward(self, input_data)` (在 `nn.Module` 子类中): 定义前向传播的计算流程。输入数据 `input_data` 通过在 `__init__` 中定义的网络层和操作，最终返回模型的输出。\n",
    "- `torch.nn.Flatten(start_dim=1, end_dim=-1)`: 将输入张量从 `start_dim` 到 `end_dim` 的维度展平。常用于将卷积层的多维输出转换为适合全连接层的二维张量 `(batch_size, features)`。此处 `end_dim=-1` 表示展平操作将进行到张量的最后一个维度（包含该维度）。\n",
    "- `torch.nn.Sequential(*modules)`: 一个有序的模块容器。数据会按照模块在构造函数中传入的顺序依次通过所有模块。是快速构建简单线性堆叠模型的便捷方式。\n",
    "- `torch.nn.Linear(in_features, out_features)`: 全连接层或线性层。对输入应用线性变换 $y = xW^T + b$。`in_features` 指输入特征维度，`out_features` 指输出特征维度。\n",
    "\n",
    "![线性层示意图](attachments\\Python_Pytorch_nn_Linear_i3_o2_01.png)\n",
    "\n",
    "- `torch.nn.ReLU()`: 修正线性单元 (Rectified Linear Unit) 激活函数。按元素应用 $f(x) = \\max(0, x)$，为模型引入非线性。除此以外，还有多种非线性激活函数：\n",
    "\n",
    "![non-linear](attachments\\non-linear.jfif)\n",
    "\n",
    "- `module_instance.to(device)`: `nn.Module` 或 `torch.Tensor` 的方法。将模块的所有参数和缓冲区，或张量本身，迁移到指定的计算设备 `device` (如 `'cpu'` 或 `'cuda'`)。对 `nn.Module` 通常是原地操作，即直接对模块本身进行修改。\n",
    "- `torch.nn.CrossEntropyLoss()`: 交叉熵损失函数。常用于多分类问题。内部集成了 `LogSoftmax` 和 `NLLLoss`，因此输入应为原始 logits (未经 softmax 的分数)，而不是经 softmax 的输出，**易错点**！\n",
    "- `torch.optim.SGD(model.parameters(), lr=learning_rate)`: 随机梯度下降 (Stochastic Gradient Descent) 优化器。`model.parameters()` 提供模型中所有可训练的参数，`lr` 为学习率。\n",
    "- `model.parameters()`: `nn.Module` 的方法，返回一个包含模型所有可学习参数 (通常是权重和偏置，即 `requires_grad=True` 的张量) 的迭代器。\n",
    "- `model.train()`: 将模型设置为训练模式。此模式会启用诸如 `Dropout` 和 `BatchNorm` 等特定层的训练期行为。\n",
    "- `tensor_instance.to(device)`: `torch.Tensor` 的方法，将张量的数据复制到指定的 `device`。如果张量已在目标设备，则返回自身。\n",
    "- `optimizer.zero_grad()`: 优化器的方法。在计算每个批次的梯度之前，清除先前累积的梯度。防止梯度在多次反向传播中叠加。\n",
    "- `loss_tensor.backward()`: 在标量损失张量上调用此方法，以启动反向传播。PyTorch 的自动微分引擎 (Autograd) 会计算损失相对于所有 `requires_grad=True` 的参数的梯度，并将其累积存储在这些参数的 `.grad` 属性中。\n",
    "- `optimizer.step()`: 优化器的方法。在梯度计算完成 (`loss.backward()`) 后调用，根据选定的优化算法 (如 SGD) 和参数的 `.grad` 中存储的梯度来更新模型参数。\n",
    "- `scalar_tensor.item()`: 对于仅包含单个元素的张量 (如平均损失值)，返回其对应的 Python 原生数值 (如 `float` 或 `int`)。\n",
    "- `model.eval()`: 将模型设置为评估（或推理）模式。此模式会禁用诸如 `Dropout` 层的随机失活行为，并使 `BatchNorm` 层使用其在训练期间学习到的固定统计量。\n",
    "- `with torch.no_grad():`: 上下文管理器。在其作用域内执行的 PyTorch 运算不会构建计算图，也不会计算梯度。用于模型评估或推理阶段，以减少内存消耗并加速计算。\n",
    "- `output_tensor.argmax(dim)`: `torch.Tensor` 的方法。返回在指定维度 `dim` 上最大元素的索引。常用于从模型的 logits 输出中获取预测类别。\n",
    "- `torch.save(object_to_save, 'filepath.pth')`: 将 Python 对象 (如模型的 `state_dict` 或整个模型) 序列化并保存到指定路径的文件中。通常使用 `.pth` 或 `.pt` 作为扩展名。\n",
    "- `model.state_dict()`: `nn.Module` 的方法。返回一个 Python 字典，其中键是参数名，值是参数张量。它包含了模型所有可学习的参数以及持久化的缓冲区 (如 BatchNorm 的运行均值和方差)。\n",
    "- `torch.load('filepath.pth', weights_only=True)`: 从文件中加载之前由 `torch.save()` 保存的对象。`weights_only=True` (PyTorch 1.13+) 是一个安全选项，一般在仅加载模型权重（即 `state_dict`）时使用，以防止加载恶意代码。\n",
    "- `model.load_state_dict(loaded_state_dict)`: `nn.Module` 的方法。将从文件加载并可能经过处理的 `state_dict` 中的参数和缓冲区加载到当前模型实例中。默认情况下，`state_dict` 的键必须与模型自身的键完全匹配 (`strict=True`)。\n",
    "\n",
    "## 提问与解答(自己提问+AI 回答)\n",
    "\n",
    "- **问题1 (对于 \"Working with data\" 当中 `DataLoader` 的使用):**\n",
    "  该教程展示了 `DataLoader` 的基本用法，并设置了 `batch_size`。除了 `batch_size`，`DataLoader` 初始化时还有哪些其他的参数？它们各自的作用是什么，我们一般在哪些情况下需要调整它们？\n",
    "\n",
    "  - **解答:**\n",
    "    `DataLoader` 除了 `dataset` 和 `batch_size` 外，还有几个常用且重要的参数：\n",
    "    1.  `shuffle` (布尔类型, 默认为 `False`): 设置为 `True` 时，会在每个 epoch 开始时打乱数据集的顺序。这有助于模型学习到更鲁棒的特征，防止模型因数据顺序而产生偏见，尤其是在训练集中样本类别分布不均或存在某种序列模式时非常有用。在训练阶段通常建议设置为 `True`，而在验证和测试阶段则应为 `False` 以确保评估的一致性。\n",
    "    2.  `num_workers` (整型, 默认为 `0`): 指定用于数据加载的子进程数量。当 `num_workers > 0` 时，数据加载将在后台由多个子进程并行处理，这可以显著加速数据准备过程，尤其是在数据预处理比较复杂或磁盘 I/O 成为瓶颈时。CPU 核心数是一个常见的参考值，但最佳值需要根据具体硬件和任务进行调整。如果设置为 `0`，则数据将在主进程中加载。\n",
    "    3.  `pin_memory` (布尔类型, 默认为 `False`): 如果设置为 `True` 并且正在使用 GPU (CUDA)，`DataLoader` 会将加载的张量数据复制到 CUDA 的固定内存 (pinned memory) 中。这可以加速数据从 CPU 到 GPU 的传输。当 GPU 成为训练瓶颈时，开启此选项可能会有性能提升。\n",
    "    4.  `drop_last` (布尔类型, 默认为 `False`): 如果数据集大小不能被 `batch_size` 整除，最后一个批次的数据量会小于 `batch_size`。如果设置为 `True`，则这个不完整的批次将被丢弃。在一些需要固定批次大小的场景（例如某些特定的网络结构或分布式训练设置）下，这个参数可能需要设置为 `True`。\n",
    "\n",
    "    参考资料：\n",
    "    - PyTorch 官方文档 `torch.utils.data.DataLoader`: [https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "\n",
    "- **问题2 (对于 \"Creating Models\" 部分 `NeuralNetwork` 定义中 `nn.ReLU` 的使用):**\n",
    "  该教程的 `NeuralNetwork` 类使用了 `nn.ReLU` 作为激活函数。除了 ReLU，深度学习中还有哪些常见的激活函数？它们各自有什么主要的优点和缺点？为什么我们通常使用 ReLU 而不是其他激活函数？\n",
    "\n",
    "  - **解答:**\n",
    "    除了 ReLU (Rectified Linear Unit)，常见的激活函数还包括：\n",
    "    1.  **Sigmoid (Logistic):**\n",
    "        -   公式: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "        -   输出范围: (0, 1)\n",
    "        -   优点: 输出在 (0,1) 之间，可以用作概率表示；平滑，处处可导。\n",
    "        -   缺点:\n",
    "            -   **梯度消失**: 当输入值过大或过小时，梯度趋近于0，导致深层网络训练困难。\n",
    "            -   输出不是零中心的 (zero-centered)，可能影响后续层参数更新的效率。\n",
    "            -   计算复杂度相对较高 (涉及指数运算)。\n",
    "        -   常用于二分类问题的输出层或需要输出概率的场景。\n",
    "\n",
    "    2.  **Tanh (Hyperbolic Tangent):**\n",
    "        -   公式: $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = 2 \\cdot \\text{sigmoid}(2x) - 1 $\n",
    "        -   输出范围: (-1, 1)\n",
    "        -   优点: 输出是零中心的，通常比 Sigmoid 收敛更快。\n",
    "        -   缺点: 仍然存在梯度消失问题 (虽然比 Sigmoid 略好，因为梯度范围更大)。计算复杂度相对较高。\n",
    "\n",
    "    3.  **LeakyReLU (Leaky Rectified Linear Unit):**\n",
    "        -   公式: $ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{if } x \\le 0 \\end{cases} $，其中 $ \\alpha $ 是一个小的正常数 (如 0.01)。\n",
    "        -   输出范围: $ (-\\infty, \\infty) $\n",
    "        -   优点: 解决了 ReLU 在负输入区间的神经元“死亡”问题 (dead neuron problem)，允许负值区域有小的非零梯度。\n",
    "        -   缺点: $ \\alpha $ 的选择可能需要调整；性能提升不总能保证。\n",
    "\n",
    "    4.  **ELU (Exponential Linear Unit):**\n",
    "        -   公式: $ f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha (e^x - 1) & \\text{if } x \\le 0 \\end{cases} $，其中 $ \\alpha > 0 $。\n",
    "        -   优点: 结合了 ReLU 和 LeakyReLU 的优点，输出均值接近零，具有一定的抗噪声能力。\n",
    "        -   缺点: 计算复杂度高于 ReLU (涉及指数运算)。\n",
    "\n",
    "    **为什么 ReLU 是一个良好且常用的默认选择？**\n",
    "    -   **计算高效**: ReLU 的计算非常简单 (一个 `max(0, x)` 操作)，比 Sigmoid 和 Tanh 的指数运算快得多。\n",
    "    -   **缓解梯度消失**: 在正数区间，ReLU 的梯度恒为1，这有助于梯度在网络中传播，从而在一定程度上缓解了梯度消失问题，使得训练更深的网络成为可能。\n",
    "    -   **稀疏性**: ReLU 会将负输入置为0，这使得网络中的一些神经元输出为0，从而引入了稀疏性。这种稀疏性可以使网络更高效，并且可能具有更好的泛化能力（类似于正则化的效果）。\n",
    "\n",
    "    尽管 ReLU 有“死亡神经元” (当输入恒为负时，神经元不再激活且梯度为0) 的问题，但在实践中，通过合理的初始化和学习率设置，ReLU 通常表现良好，并且是许多现代神经网络架构的默认激活函数。\n",
    "\n",
    "    参考资料：\n",
    "    - \"Deep Learning\" book by Goodfellow, Bengio, and Courville (Chapter 6 on Deep Feedforward Networks, specifically activation functions).\n",
    "    - Stanford CS231n Course Notes on Activation Functions: [https://cs231n.github.io/neural-networks-1/#actfun](https://cs231n.github.io/neural-networks-1/#actfun)\n",
    "    - PyTorch documentation on non-linear activations: [https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)\n",
    "\n",
    "- **问题3 (关于 \"Optimizing the Model Parameters\" 部分优化器的选择和学习率的设置):**\n",
    "  该教程使用了 `torch.optim.SGD` 优化器，并设置了固定的学习率 `lr=1e-3`。对此，我有以下几个问题：\n",
    "    1. 如果学习率设置得过大或过小，分别可能会导致什么问题？\n",
    "    2. 除了 SGD，还有哪些在实践中常用的优化器？它们相对于 SGD 有哪些改进？\n",
    "    3. 教程中使用了固定的学习率，在更复杂的任务中，我们可能会使用学习率调度器 (Learning Rate Scheduler)。它的作用是什么？你能否举例说明一种常见的学习率调整策略？\n",
    "\n",
    "  - **解答:**\n",
    "    1. **学习率的影响:**\n",
    "        -   **学习率过大**: 可能导致优化算法在损失函数的最小值附近“震荡”甚至“发散”，无法收敛到最优解。想象一下你下山，步子迈得太大，可能会直接跨过山谷的最低点，甚至跑到对面的山坡上去了。\n",
    "        -   **学习率过小**: 可能导致收敛速度非常缓慢，需要大量的训练迭代才能达到较好的性能。同时，也更容易陷入局部最小值 (local minima) 或鞍点 (saddle points)。想象一下你下山，每一步都迈得非常小，虽然最终能到山脚，但会花费非常长的时间。\n",
    "\n",
    "    2. **其他常用优化器及其改进:**\n",
    "        除了 SGD (随机梯度下降)，还有许多更先进的优化器，它们通常通过自适应地调整每个参数的学习率或引入动量 (momentum) 来加速收敛并改善 SGD 的一些缺点。\n",
    "        1.  **SGD with Momentum (带动量的SGD):**\n",
    "            -   改进: 在 SGD 的基础上引入了动量项，该动量项累积了历史梯度的方向。这有助于加速在梯度方向一致的维度上的学习，并抑制震荡，使得优化路径更平滑，更容易跳出局部最优。\n",
    "            -   PyTorch 实现: `torch.optim.SGD(params, lr, momentum=0.9)` (momentum 通常设为0.9左右)。\n",
    "        2.  **Adagrad (Adaptive Gradient Algorithm):**\n",
    "            -   改进: 对每个参数使用不同的学习率。对于不经常更新的参数，它会使用较大的学习率；对于经常更新的参数，则使用较小的学习率。适合处理稀疏数据。\n",
    "            -   缺点: 学习率会单调递减，可能导致学习率过早变得太小，从而提前停止学习。\n",
    "            -   PyTorch 实现: `torch.optim.Adagrad(params, lr)`\n",
    "        3.  **RMSprop (Root Mean Square Propagation):**\n",
    "            -   改进: Adagrad 的一个改进版本，通过引入一个衰减系数来解决 Adagrad 学习率急剧下降的问题。它不累积所有历史平方梯度，而是关注最近一段时间的梯度大小。\n",
    "            -   PyTorch 实现: `torch.optim.RMSprop(params, lr)`\n",
    "        4.  **Adam (Adaptive Moment Estimation):**\n",
    "            -   改进: 结合了 Momentum 和 RMSprop 的思想。它既计算了梯度的一阶矩估计 (均值，即动量项)，也计算了梯度的二阶矩估计 (方差的无偏估计，用于自适应调整学习率)。Adam 通常被认为在各种任务和网络结构上都表现良好，并且对初始学习率的选择不如 SGD 敏感，是目前非常流行和常用的优化器之一。\n",
    "            -   PyTorch 实现: `torch.optim.Adam(params, lr)`\n",
    "\n",
    "    3. **学习率调度器 (Learning Rate Scheduler):**\n",
    "        - 学习率调度器的作用是在训练过程中动态地调整学习率。通常在训练开始时使用较大的学习率以快速接近最优解，然后在训练后期逐渐减小学习率，以便更精细地调整参数，帮助模型收敛到更好的局部最优值并避免震荡。\n",
    "        - 一种常见的学习率调整策略是 **StepLR (阶梯式衰减)**：\n",
    "            -   工作原理: 在预设的训练轮数 (epoch) 间隔 (`step_size`) 后，将当前学习率乘以一个衰减因子 (`gamma`)。\n",
    "            -   例如，初始学习率为 0.1，`step_size=30`，`gamma=0.1`。那么在第30个 epoch 结束后，学习率会变成 0.01；在第60个 epoch 结束后，学习率会变成 0.001，以此类推。\n",
    "            -   PyTorch 实现: `torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma)`\n",
    "        - 其他常见的策略还包括 `MultiStepLR` (在指定的多个 epoch 处衰减学习率)，`ExponentialLR` (指数衰减)，以及 `CosineAnnealingLR` (余弦退火，学习率按余弦函数周期性变化) 等。\n",
    "\n",
    "    参考资料：\n",
    "    - \"An overview of gradient descent optimization algorithms\" by Sebastian Ruder: [https://ruder.io/optimizing-gradient-descent/](https://ruder.io/optimizing-gradient-descent/) (这篇博客文章对各种优化器有非常好的总结)\n",
    "    - PyTorch 官方文档 `torch.optim`: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html) (包含了所有优化器和学习率调度器的详细信息)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Learn the Basics](intro.html) \\|\\| **Quickstart** \\|\\|\n",
    "[Tensors](tensorqs_tutorial.html) \\|\\| [Datasets &\n",
    "DataLoaders](data_tutorial.html) \\|\\|\n",
    "[Transforms](transforms_tutorial.html) \\|\\| [Build\n",
    "Model](buildmodel_tutorial.html) \\|\\|\n",
    "[Autograd](autogradqs_tutorial.html) \\|\\|\n",
    "[Optimization](optimization_tutorial.html) \\|\\| [Save & Load\n",
    "Model](saveloadrun_tutorial.html)\n",
    "\n",
    "Quickstart\n",
    "==========\n",
    "\n",
    "This section runs through the API for common tasks in machine learning.\n",
    "Refer to the links in each section to dive deeper.\n",
    "\n",
    "Working with data\n",
    "-----------------\n",
    "\n",
    "PyTorch has two [primitives to work with\n",
    "data](https://pytorch.org/docs/stable/data.html):\n",
    "`torch.utils.data.DataLoader` and `torch.utils.data.Dataset`. `Dataset`\n",
    "stores the samples and their corresponding labels, and `DataLoader`\n",
    "wraps an iterable around the `Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch offers domain-specific libraries such as\n",
    "[TorchText](https://pytorch.org/text/stable/index.html),\n",
    "[TorchVision](https://pytorch.org/vision/stable/index.html), and\n",
    "[TorchAudio](https://pytorch.org/audio/stable/index.html), all of which\n",
    "include datasets. For this tutorial, we will be using a TorchVision\n",
    "dataset.\n",
    "\n",
    "The `torchvision.datasets` module contains `Dataset` objects for many\n",
    "real-world vision data like CIFAR, COCO ([full list\n",
    "here](https://pytorch.org/vision/stable/datasets.html)). In this\n",
    "tutorial, we use the FashionMNIST dataset. Every TorchVision `Dataset`\n",
    "includes two arguments: `transform` and `target_transform` to modify the\n",
    "samples and labels respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass the `Dataset` as an argument to `DataLoader`. This wraps an\n",
    "iterable over our dataset, and supports automatic batching, sampling,\n",
    "shuffling and multiprocess data loading. Here we define a batch size of\n",
    "64, i.e. each element in the dataloader iterable will return a batch of\n",
    "64 features and labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [loading data in PyTorch](data_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Models\n",
    "===============\n",
    "\n",
    "To define a neural network in PyTorch, we create a class that inherits\n",
    "from\n",
    "[nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "We define the layers of the network in the `__init__` function and\n",
    "specify how data will pass through the network in the `forward`\n",
    "function. To accelerate operations in the neural network, we move it to\n",
    "the\n",
    "[accelerator](https://pytorch.org/docs/stable/torch.html#accelerators)\n",
    "such as CUDA, MPS, MTIA, or XPU. If the current accelerator is\n",
    "available, we will use it. Otherwise, we use the CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "\n",
    "# 检查是否有可用的 CUDA (NVIDIA GPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "# 如果没有，则使用 CPU\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [building neural networks in\n",
    "PyTorch](buildmodel_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the Model Parameters\n",
    "===============================\n",
    "\n",
    "To train a model, we need a [loss\n",
    "function](https://pytorch.org/docs/stable/nn.html#loss-functions) and an\n",
    "[optimizer](https://pytorch.org/docs/stable/optim.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training\n",
    "dataset (fed to it in batches), and backpropagates the prediction error\n",
    "to adjust the model\\'s parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model\\'s performance against the test dataset to\n",
    "ensure it is learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process is conducted over several iterations (*epochs*).\n",
    "During each epoch, the model learns parameters to make better\n",
    "predictions. We print the model\\'s accuracy and loss at each epoch;\n",
    "we\\'d like to see the accuracy increase and the loss decrease with every\n",
    "epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.305108  [   64/60000]\n",
      "loss: 2.298959  [ 6464/60000]\n",
      "loss: 2.283111  [12864/60000]\n",
      "loss: 2.279013  [19264/60000]\n",
      "loss: 2.248929  [25664/60000]\n",
      "loss: 2.232475  [32064/60000]\n",
      "loss: 2.238815  [38464/60000]\n",
      "loss: 2.204337  [44864/60000]\n",
      "loss: 2.202091  [51264/60000]\n",
      "loss: 2.178800  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 44.1%, Avg loss: 2.171968 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.178049  [   64/60000]\n",
      "loss: 2.175273  [ 6464/60000]\n",
      "loss: 2.122789  [12864/60000]\n",
      "loss: 2.136841  [19264/60000]\n",
      "loss: 2.080745  [25664/60000]\n",
      "loss: 2.036564  [32064/60000]\n",
      "loss: 2.058276  [38464/60000]\n",
      "loss: 1.980273  [44864/60000]\n",
      "loss: 1.982165  [51264/60000]\n",
      "loss: 1.926575  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.2%, Avg loss: 1.917008 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.944713  [   64/60000]\n",
      "loss: 1.924238  [ 6464/60000]\n",
      "loss: 1.804566  [12864/60000]\n",
      "loss: 1.845308  [19264/60000]\n",
      "loss: 1.741824  [25664/60000]\n",
      "loss: 1.695971  [32064/60000]\n",
      "loss: 1.721411  [38464/60000]\n",
      "loss: 1.614625  [44864/60000]\n",
      "loss: 1.639374  [51264/60000]\n",
      "loss: 1.547863  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Avg loss: 1.558414 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.617583  [   64/60000]\n",
      "loss: 1.590591  [ 6464/60000]\n",
      "loss: 1.436131  [12864/60000]\n",
      "loss: 1.512209  [19264/60000]\n",
      "loss: 1.397504  [25664/60000]\n",
      "loss: 1.386347  [32064/60000]\n",
      "loss: 1.402694  [38464/60000]\n",
      "loss: 1.321551  [44864/60000]\n",
      "loss: 1.357418  [51264/60000]\n",
      "loss: 1.257103  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Avg loss: 1.289425 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.360452  [   64/60000]\n",
      "loss: 1.347070  [ 6464/60000]\n",
      "loss: 1.181291  [12864/60000]\n",
      "loss: 1.284331  [19264/60000]\n",
      "loss: 1.161196  [25664/60000]\n",
      "loss: 1.183428  [32064/60000]\n",
      "loss: 1.200264  [38464/60000]\n",
      "loss: 1.137249  [44864/60000]\n",
      "loss: 1.175647  [51264/60000]\n",
      "loss: 1.083855  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Avg loss: 1.116828 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Training your model](optimization_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Models\n",
    "=============\n",
    "\n",
    "A common way to save a model is to serialize the internal state\n",
    "dictionary (containing the model parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Models\n",
    "==============\n",
    "\n",
    "The process for loading a model includes re-creating the model structure\n",
    "and loading the state dictionary into it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can now be used to make predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read more about [Saving & Loading your\n",
    "model](saveloadrun_tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
