{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验四：Seq2seq 机器翻译\n",
    "\n",
    " 本次实验的目标是构建一个基于GRU的序列到序列（Seq2seq）模型，来完成简单的英-中机器翻译任务。\n",
    "\n",
    " **学生视角思考**：\n",
    " Seq2seq是处理序列转换问题的经典模型，比如翻译、对话系统等。它由两部分组成：\n",
    " 1. **编码器 (Encoder)**: 读取并理解整个输入句子，将其压缩成一个固定长度的“思想”向量（上下文向量）。\n",
    " 2. **解码器 (Decoder)**: 根据这个“思想”向量，一个词一个词地生成输出句子。\n",
    " 这个任务的挑战在于处理变长的序列数据，以及如何有效地在编码器和解码器之间传递信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备与库导入\n",
    "\n",
    " 导入所有必需的库，并设置MindSpore的运行环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据准备与预处理\n",
    "\n",
    " ### 2.1 下载并解析数据\n",
    "\n",
    " 我们首先下载英-中平行语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": [
     "command"
    ]
   },
   "outputs": [],
   "source": [
    "!wget -N https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/datasets/eng-fra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng-fra.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().strip().split('\\\\n')\n",
    "\n",
    "pairs = [[s for s in l.split('\\\\t')] for l in lines]\n",
    "# For this experiment, we only use Chinese instead of French\n",
    "for pair in pairs:\n",
    "    pair[1] = pair[2]\n",
    "    del pair[2]\n",
    "\n",
    "print(\"数据样本: \", random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义配置和词典类\n",
    "\n",
    " 我们定义一个配置类来管理超参数，并创建一个词典（Vocabulary）类来处理词汇的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class Config:\n",
    "    data_path = 'eng-fra.txt'\n",
    "    vocab_path = 'vocab.json'\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "    hidden_size = 256\n",
    "    encoder_embedding_dim = 256\n",
    "    decoder_embedding_dim = 256\n",
    "    learning_rate = 0.01\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\\\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数据过滤与词典构建\n",
    "\n",
    " 为了简化模型，我们只使用长度小于10的简单句，并过滤掉非 \"I am\", \"He is\" 等开头的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\"i am \", \"i m \", \"he is\", \"she is\", \"they are\", \"we are\", \"you are\")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "    with open('eng-fra.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\\\n')\n",
    "    \n",
    "    pairs = [[s for s in l.split('\\\\t')] for l in lines]\n",
    "    for pair in pairs:\n",
    "        pair[1] = pair[2]\n",
    "        del pair[2]\n",
    "        pair[0] = normalizeString(pair[0])\n",
    "\n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "\n",
    "    input_lang = Vocabulary(lang1)\n",
    "    output_lang = Vocabulary(lang2)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'cmn', False)\n",
    "print(\"Random pair:\", random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数据转换与Dataset创建\n",
    "\n",
    " 将文本句子转换为模型可以处理的Tensor，并创建MindSpore Dataset。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return indexes\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def create_dataset(pairs, batch_size):\n",
    "    tensors = [tensorsFromPair(p) for p in pairs]\n",
    "    \n",
    "    def generator():\n",
    "        for pair_tensor in tensors:\n",
    "            yield pair_tensor[0], len(pair_tensor[0]), pair_tensor[1], len(pair_tensor[1])\n",
    "    \n",
    "    dataset = GeneratorDataset(generator, column_names=[\"input\", \"input_len\", \"target\", \"target_len\"])\n",
    "    \n",
    "    # Padding\n",
    "    dataset = dataset.padded_batch(batch_size, \n",
    "                                   pad_info={\"input\": ([MAX_LENGTH + 1], 0), \n",
    "                                             \"target\": ([MAX_LENGTH + 1], 0)})\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(pairs, Config.batch_size)#| ## 3. 模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在来定义Seq2seq模型的三个核心组件：编码器、解码器和主控模型。\n",
    "\n",
    " **学生视角思考**：\n",
    " - **Encoder**: 它的工作是“阅读”整个英文句子，并将句子的含义压缩到一个`hidden_state`向量中。我使用了GRU，它比LSTM结构更简单，但效果同样强大。\n",
    " - **Decoder**: 它的工作是“写出”中文句子。它会接收Encoder的`hidden_state`作为初始“思想”，然后一个字一个字地生成翻译。在每个时间步，它都会看着Encoder的输出和自己上一步生成的字，来决定下一步要生成什么。\n",
    " - **Seq2Seq**: 这个类像一个“指挥官”，它首先命令Encoder去理解句子，然后将理解的结果（`hidden_state`）传递给Decoder，让Decoder开始生成翻译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return ops.Zeros()((1, batch_size, self.hidden_size), mindspore.float32)\n",
    "\n",
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, hidden_size, output_size, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Dense(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(axis=1)\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        output = self.embedding(x)\n",
    "        output = ops.ReLU()(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, encoder, decoder, max_length=MAX_LENGTH):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def construct(self, enc_input, dec_input, teacher_forcing_ratio=0.5):\n",
    "        batch_size = enc_input.shape[1]\n",
    "        enc_hidden = self.encoder.initHidden(batch_size)\n",
    "        \n",
    "        _, enc_hidden = self.encoder(enc_input, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input_t = dec_input[0]\n",
    "        \n",
    "        decoder_outputs = ops.Zeros()((self.max_length, batch_size, self.decoder.out.out_features), mindspore.float32)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(self.max_length):\n",
    "                decoder_output, dec_hidden = self.decoder(dec_input_t, dec_hidden)\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                dec_input_t = dec_input[di+1] # Teacher forcing\n",
    "        else:\n",
    "            for di in range(self.max_length):\n",
    "                decoder_output, dec_hidden = self.decoder(dec_input_t, dec_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                dec_input_t = topi.squeeze().detach()\n",
    "                decoder_outputs[di] = decoder_output\n",
    "\n",
    "        return decoder_outputs#| ## 4. 训练与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们定义损失函数、训练逻辑和评估函数，并启动训练。\n",
    "\n",
    " **学生视角思考**：\n",
    " - **Masked Loss**: 我们的输入数据经过了填充（Padding）以对齐长度。在计算损失时，我们必须“忽略”这些填充位，否则模型会去学习拟合这些无意义的填充符。因此，我需要一个带掩码（Mask）的损失函数，它只计算非填充部分的损失。\n",
    " - **Teacher Forcing**: 这是训练Seq2seq模型的一个技巧。在训练初期，我们强制解码器使用真实的上一时间步的输出来预测当前步，而不是它自己生成的（可能错误的）输出。这能让模型更快地收敛。随着训练的进行，我们会逐渐减小Teacher Forcing的比例，让模型学会依赖自己的预测。\n",
    " - **评估**: 训练完成后，我会用几个英文句子来测试模型，看看它能否生成合理的中文翻译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropy(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(MaskedCrossEntropy, self).__init__()\n",
    "        self.criterion = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def construct(self, logits, target, mask):\n",
    "        mask = mask.astype(mindspore.float32)\n",
    "        loss = self.criterion(logits, target)\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "def train_step(input_tensor, target_tensor, seq2seq_model, optimizer, criterion):\n",
    "    def forward_fn():\n",
    "        decoder_outputs = seq2seq_model(input_tensor, target_tensor)\n",
    "        \n",
    "        mask = ops.ones_like(target_tensor)\n",
    "        mask[target_tensor == 0] = 0 # Mask padding\n",
    "        \n",
    "        loss = criterion(decoder_outputs.view(-1, output_lang.n_words), \n",
    "                         target_tensor.view(-1), \n",
    "                         mask.view(-1))\n",
    "        return loss\n",
    "\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "    loss, grads = grad_fn()\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train_model():\n",
    "    encoder = Encoder(input_lang.n_words, Config.hidden_size, Config.encoder_embedding_dim)\n",
    "    decoder = Decoder(Config.hidden_size, output_lang.n_words, Config.decoder_embedding_dim)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "    optimizer = nn.Adam(seq2seq_model.trainable_params(), learning_rate=Config.learning_rate)\n",
    "    criterion = MaskedCrossEntropy()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (input_tensor, _, target_tensor, _) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            loss = train_step(input_tensor, target_tensor, seq2seq_model, optimizer, criterion)\n",
    "            total_loss += loss.asnumpy()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{Config.num_epochs}, Loss: {total_loss / (i + 1):.4f}')\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    return seq2seq_model\n",
    "\n",
    "def evaluate(seq2seq_model, sentence):\n",
    "    print(\"Input:\", sentence)\n",
    "    with mindspore.context.set_context(mode=mindspore.context.PYNATIVE_MODE):\n",
    "        input_tensor = Tensor([tensorFromSentence(input_lang, sentence)], mindspore.int32).T\n",
    "        \n",
    "        batch_size = input_tensor.shape[1]\n",
    "        enc_hidden = seq2seq_model.encoder.initHidden(batch_size)\n",
    "        _, enc_hidden = seq2seq_model.encoder(input_tensor, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        decoder_input = Tensor([[SOS_token]], mindspore.int32)\n",
    "        \n",
    "        decoded_words = []\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            decoder_output, dec_hidden = seq2seq_model.decoder(decoder_input, dec_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.asnumpy()[0] == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[topi.asnumpy()[0]])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        print(\"Output:\", ' '.join(decoded_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 执行训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(trained_model, \"i m ok .\")\n",
    "evaluate(trained_model, \"he is a reporter .\")\n",
    "evaluate(trained_model, \"she is sad .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
