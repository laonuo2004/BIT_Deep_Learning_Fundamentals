{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验四：Seq2seq 机器翻译\n",
    "\n",
    " 本次实验的目标是构建一个基于GRU的序列到序列（Seq2seq）模型，来完成简单的英-中机器翻译任务。\n",
    "\n",
    " **学生视角思考**：\n",
    " Seq2seq是处理序列转换问题的经典模型，比如翻译、对话系统等。它由两部分组成：\n",
    " 1. **编码器 (Encoder)**: 读取并理解整个输入句子，将其压缩成一个固定长度的“思想”向量（上下文向量）。\n",
    " 2. **解码器 (Decoder)**: 根据这个“思想”向量，一个词一个词地生成输出句子。\n",
    " 这个任务的挑战在于处理变长的序列数据，以及如何有效地在编码器和解码器之间传递信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备与库导入\n",
    "\n",
    " 导入所有必需的库，并设置MindSpore的运行环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "import mindspore.nn as nn\n",
    "import mindspore.ops as ops\n",
    "from mindspore.ops import operations as P\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.dataset import GeneratorDataset\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"Ascend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据准备与预处理\n",
    "\n",
    " ### 2.1 下载并解析数据\n",
    "\n",
    " 我们首先从国内可访问的镜像下载英-中平行语料库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "tags": [
     "command"
    ]
   },
   "outputs": [],
   "source": [
    "!wget -N http://www.manythings.org/anki/cmn-eng.zip -O cmn-eng.zip\n",
    "\n",
    "!unzip -o cmn-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义配置和词典类\n",
    "\n",
    " 我们定义一个配置类来管理超参数，并创建一个词典（Vocabulary）类来处理词汇的映射。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "class Config:\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "    hidden_size = 256\n",
    "    encoder_embedding_dim = 256\n",
    "    decoder_embedding_dim = 256\n",
    "    learning_rate = 0.01\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\\\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数据过滤与词典构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\"i am \", \"i m \", \"he is\", \"she is\", \"they are\", \"we are\", \"you are\")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2):\n",
    "    print(\"Reading lines...\")\n",
    "    with open('cmn.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\\\n')\n",
    "    \n",
    "    pairs = [[s for s in l.split('\\\\t')] for l in lines]\n",
    "    for p in pairs:\n",
    "        p[0] = normalizeString(p[0])\n",
    "    \n",
    "    print(f\"Read {len(pairs)} sentence pairs\")\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(f\"Trimmed to {len(pairs)} sentence pairs\")\n",
    "\n",
    "    input_lang = Vocabulary(lang1)\n",
    "    output_lang = Vocabulary(lang2)\n",
    "\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    \n",
    "    print(\"Counted words:\")\n",
    "    print(f\"{input_lang.name}: {input_lang.n_words}\")\n",
    "    print(f\"{output_lang.name}: {output_lang.n_words}\")\n",
    "    \n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'cmn')\n",
    "print(\"\\\\nRandom pair:\", random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 数据转换与Dataset创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return indexes\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def create_dataset(pairs, batch_size):\n",
    "    tensors = [tensorsFromPair(p) for p in pairs]\n",
    "    \n",
    "    def generator():\n",
    "        for pair_tensor in tensors:\n",
    "            yield pair_tensor[0], len(pair_tensor[0]), pair_tensor[1], len(pair_tensor[1])\n",
    "    \n",
    "    dataset = GeneratorDataset(generator, column_names=[\"input\", \"input_len\", \"target\", \"target_len\"])\n",
    "    \n",
    "    dataset = dataset.padded_batch(batch_size, \n",
    "                                   pad_info={\"input\": ([MAX_LENGTH + 1], 0), \n",
    "                                             \"target\": ([MAX_LENGTH + 1], 0)})\n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset(pairs, Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Cell):\n",
    "    def __init__(self, input_size, hidden_size, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        embedded = self.embedding(x).view(1, x.shape[0], -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        return ops.Zeros()((1, batch_size, self.hidden_size), mindspore.float32)\n",
    "\n",
    "class Decoder(nn.Cell):\n",
    "    def __init__(self, hidden_size, output_size, embedding_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size)\n",
    "        self.out = nn.Dense(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(axis=1)\n",
    "\n",
    "    def construct(self, x, hidden):\n",
    "        output = self.embedding(x).view(1, x.shape[0], -1)\n",
    "        output = ops.ReLU()(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "class Seq2Seq(nn.Cell):\n",
    "    def __init__(self, encoder, decoder, max_length=MAX_LENGTH):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def construct(self, enc_input, dec_input, teacher_forcing_ratio=0.5):\n",
    "        batch_size = enc_input.shape[1]\n",
    "        enc_hidden = self.encoder.initHidden(batch_size)\n",
    "        \n",
    "        encoder_outputs = ops.Zeros()((self.max_length, batch_size, self.encoder.hidden_size), mindspore.float32)\n",
    "\n",
    "        for ei in range(enc_input.shape[0]):\n",
    "            encoder_output, enc_hidden = self.encoder(enc_input[ei], enc_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input_t = Tensor(np.array([SOS_token] * batch_size), mindspore.int32)\n",
    "        \n",
    "        decoder_outputs = ops.Zeros()((self.max_length, batch_size, self.decoder.out.out_features), mindspore.float32)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(self.max_length):\n",
    "                decoder_output, dec_hidden = self.decoder(dec_input_t, dec_hidden)\n",
    "                decoder_outputs[di] = decoder_output\n",
    "                dec_input_t = dec_input[di]\n",
    "        else:\n",
    "            for di in range(self.max_length):\n",
    "                decoder_output, dec_hidden = self.decoder(dec_input_t, dec_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                dec_input_t = topi.squeeze().detach()\n",
    "                decoder_outputs[di] = decoder_output\n",
    "\n",
    "        return decoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练与评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedNLLLoss(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(MaskedNLLLoss, self).__init__()\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "    \n",
    "    def construct(self, inp, target, mask):\n",
    "        mask = mask.astype(mindspore.float32)\n",
    "        loss = self.loss(inp, target) * mask\n",
    "        return loss.sum() / mask.sum()\n",
    "\n",
    "def train_step(input_tensor, target_tensor, seq2seq, optimizer, criterion):\n",
    "    def forward_fn():\n",
    "        decoder_outputs = seq2seq(input_tensor, target_tensor)\n",
    "        mask = ops.ones_like(target_tensor)\n",
    "        mask[target_tensor == 0] = 0\n",
    "        loss = criterion(decoder_outputs.view(-1, output_lang.n_words), \n",
    "                         target_tensor.view(-1), \n",
    "                         mask.view(-1))\n",
    "        return loss\n",
    "    \n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "    loss, grads = grad_fn()\n",
    "    optimizer(grads)\n",
    "    return loss.asnumpy()\n",
    "\n",
    "def train_model():\n",
    "    encoder = Encoder(input_lang.n_words, Config.hidden_size, Config.encoder_embedding_dim)\n",
    "    decoder = Decoder(Config.hidden_size, output_lang.n_words, Config.decoder_embedding_dim)\n",
    "    seq2seq_model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "    optimizer = nn.Adam(seq2seq_model.trainable_params(), learning_rate=Config.learning_rate)\n",
    "    criterion = MaskedNLLLoss()\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (inp, inp_len, tar, tar_len) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            loss = train_step(inp.T, tar.T, seq2seq_model, optimizer, criterion)\n",
    "            total_loss += loss\n",
    "        print(f'Epoch {epoch + 1}/{Config.num_epochs}, Loss: {total_loss / (i + 1):.4f}')\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    return seq2seq_model\n",
    "\n",
    "def evaluate(seq2seq_model, sentence):\n",
    "    print(\"Input:\", sentence)\n",
    "    with mindspore.context.set_context(mode=mindspore.context.PYNATIVE_MODE):\n",
    "        input_tensor = Tensor([tensorFromSentence(input_lang, sentence)], mindspore.int32).T\n",
    "        \n",
    "        batch_size = input_tensor.shape[1]\n",
    "        enc_hidden = seq2seq_model.encoder.initHidden(batch_size)\n",
    "\n",
    "        for ei in range(input_tensor.shape[0]):\n",
    "            _, enc_hidden = seq2seq_model.encoder(input_tensor[ei], enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        decoder_input = Tensor([[SOS_token]], mindspore.int32)\n",
    "        \n",
    "        decoded_words = []\n",
    "        for _ in range(MAX_LENGTH):\n",
    "            decoder_output, dec_hidden = seq2seq_model.decoder(decoder_input, dec_hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            if topi.asnumpy()[0] == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[topi.asnumpy()[0][0]])\n",
    "            decoder_input = topi.squeeze().detach().view(1, -1)\n",
    "\n",
    "        print(\"Output:\", ' '.join(decoded_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 执行训练和评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(trained_model, \"i m ok .\")\n",
    "evaluate(trained_model, \"he is a reporter .\")\n",
    "evaluate(trained_model, \"she is sad .\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
