{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验三：Fashion MNIST 正则化前后对比\n",
    "\n",
    " 本次实验旨在通过构建两个卷积神经网络（CNN）来对比正则化技术的效果。\n",
    " 我们将使用Fashion-MNIST数据集，一个模型不使用正则化，另一个模型使用Dropout和BatchNorm作为正则化手段。\n",
    "\n",
    " **我的思考**：\n",
    " 正则化是防止模型过拟合的关键技术。过拟合指的是模型在训练集上表现很好，但在未见过的测试集上表现较差。\n",
    " - **Dropout**: 在训练过程中随机“丢弃”一部分神经元的输出，可以强制网络学习更加鲁棒的特征，因为它不能依赖于任何单个神经元。\n",
    " - **BatchNorm**: 对每一层的输入进行归一化，可以加速模型收敛，并在一定程度上起到正则化的作用。\n",
    " 我的预期是看到带有正则化的模型在测试集上能获得更高的准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备与库导入\n",
    "\n",
    " 导入所有必需的库，并设置MindSpore的运行环境。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import sys\n",
    "from easydict import EasyDict as edict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import context, Tensor\n",
    "from mindspore.train import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target='Ascend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据准备与预处理\n",
    "\n",
    " ### 2.1 下载并解压数据集\n",
    "\n",
    " 首先，我们通过命令行下载并解压Fashion-MNIST数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": [
     "command"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-05 23:13:39--  https://ascend-professional-construction-dataset.obs.myhuaweicloud.com/deep-learning/fashion-mnist.zip\n",
      "Resolving ascend-professional-construction-dataset.obs.myhuaweicloud.com (ascend-professional-construction-dataset.obs.myhuaweicloud.com)... 100.125.83.133, 100.125.83.5, 100.125.76.5\n",
      "Connecting to ascend-professional-construction-dataset.obs.myhuaweicloud.com (ascend-professional-construction-dataset.obs.myhuaweicloud.com)|100.125.83.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 30874889 (29M) [application/zip]\n",
      "Saving to: ‘fashion-mnist.zip’\n",
      "\n",
      "fashion-mnist.zip   100%[===================>]  29.44M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-07-05 23:13:39 (229 MB/s) - ‘fashion-mnist.zip’ saved [30874889/30874889]\n",
      "\n",
      "Archive:  fashion-mnist.zip\n",
      "   creating: fashion-mnist/\n",
      "   creating: fashion-mnist/test/\n",
      "  inflating: fashion-mnist/test/t10k-images-idx3-ubyte  \n",
      "  inflating: fashion-mnist/test/t10k-labels-idx1-ubyte  \n",
      "   creating: fashion-mnist/train/\n",
      "  inflating: fashion-mnist/train/train-images-idx3-ubyte  \n",
      "  inflating: fashion-mnist/train/train-labels-idx1-ubyte  \n"
     ]
    }
   ],
   "source": [
    "!wget https://ascend-professional-construction-dataset.obs.myhuaweicloud.com/deep-learning/fashion-mnist.zip\n",
    "\n",
    "!unzip fashion-mnist.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 定义常量和数据读取函数\n",
    "\n",
    " 我们定义一些常量来管理数据集，并编写函数来从二进制文件中读取图像和标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = edict({\n",
    "    'train_size': 60000,\n",
    "    'test_size': 10000,\n",
    "    'channel': 1,\n",
    "    'image_height': 28,\n",
    "    'image_width': 28,\n",
    "    'batch_size': 64,\n",
    "    'num_classes': 10,\n",
    "    'lr': 0.001,\n",
    "    'epoch_size': 3,\n",
    "    'data_dir_train': os.path.join('fashion-mnist', 'train'),\n",
    "    'data_dir_test': os.path.join('fashion-mnist', 'test'),\n",
    "})\n",
    "\n",
    "def read_image(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        buf = f.read()\n",
    "    magic, img_num, rows, cols = struct.unpack_from('>IIII', buf, 0)\n",
    "    offset = struct.calcsize('>IIII')\n",
    "    imgs = np.frombuffer(buf, dtype=np.uint8, offset=offset).reshape(img_num, rows, cols)\n",
    "    return imgs\n",
    "\n",
    "def read_label(file_name):\n",
    "    with open(file_name, \"rb\") as f:\n",
    "        buf = f.read()\n",
    "    magic, label_num = struct.unpack_from('>II', buf, 0)\n",
    "    offset = struct.calcsize('>II')\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8, offset=offset)\n",
    "    return labels\n",
    "\n",
    "def get_data():\n",
    "    train_image = read_image(os.path.join(cfg.data_dir_train, 'train-images-idx3-ubyte'))\n",
    "    train_label = read_label(os.path.join(cfg.data_dir_train, 'train-labels-idx1-ubyte'))\n",
    "    test_image = read_image(os.path.join(cfg.data_dir_test, 't10k-images-idx3-ubyte'))\n",
    "    test_label = read_label(os.path.join(cfg.data_dir_test, 't10k-labels-idx1-ubyte'))\n",
    "    \n",
    "    train_x = train_image.reshape(-1, 1, cfg.image_height, cfg.image_width).astype(np.float32) / 255.0\n",
    "    test_x = test_image.reshape(-1, 1, cfg.image_height, cfg.image_width).astype(np.float32) / 255.0\n",
    "    \n",
    "    train_y = train_label.astype(np.int32)\n",
    "    test_y = test_label.astype(np.int32)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 创建Dataset对象\n",
    "\n",
    " 将numpy数据转换为MindSpore的Dataset对象，以便进行高效的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    train_x, train_y, test_x, test_y = get_data()\n",
    "    \n",
    "    XY_train = list(zip(train_x, train_y))\n",
    "    ds_train = ds.GeneratorDataset(XY_train, ['x', 'y'])\n",
    "    ds_train = ds_train.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    \n",
    "    XY_test = list(zip(test_x, test_y))\n",
    "    ds_test = ds.GeneratorDataset(XY_test, ['x', 'y'])\n",
    "    ds_test = ds_test.shuffle(buffer_size=1000).batch(cfg.batch_size, drop_remainder=True)\n",
    "    \n",
    "    return ds_train, ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型构建\n",
    "\n",
    " ### 3.1 无正则化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardFashion(nn.Cell):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(ForwardFashion, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(128 * 11 * 11, 128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 有正则化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardFashionRegularization(nn.Cell):\n",
    "    def __init__(self, num_class=10):\n",
    "        super(ForwardFashionRegularization, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0, has_bias=False, pad_mode=\"valid\")\n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(3200, 128) # Note: The input size to fc1 changes due to dropout placement\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Dense(128, self.num_class)\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x) # Apply pooling\n",
    "        x = self.dropout(x)   # Apply dropout after pooling\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool2d(x) # Apply pooling\n",
    "        x = self.dropout(x)   # Apply dropout after pooling\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn(x)        # Apply BatchNorm after activation\n",
    "        x = self.dropout(x)   # Apply dropout after activation/bn\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 训练与评估\n",
    "\n",
    " 我们定义一个统一的训练函数来处理两个模型的训练和评估流程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(Net):\n",
    "    ds_train, ds_test = create_dataset()\n",
    "    network = Net(cfg.num_classes)\n",
    "    \n",
    "    net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "    net_opt = nn.Adam(network.trainable_params(), cfg.lr)\n",
    "    \n",
    "    model = Model(network, loss_fn=net_loss, optimizer=net_opt, metrics={'acc': Accuracy()})\n",
    "    \n",
    "    print(f\"============== Starting Training for {Net.__name__} ==============\")\n",
    "    model.train(cfg.epoch_size, ds_train, callbacks=[LossMonitor()], dataset_sink_mode=False)\n",
    "    \n",
    "    metric = model.eval(ds_test)\n",
    "    print(f\"============== Evaluation for {Net.__name__} ==============\")\n",
    "    print(metric)\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 执行并对比结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training without Regularization ---\n",
      "============== Starting Training for ForwardFashion ==============\n",
      "epoch: 1 step: 1, loss is 2.302584648132324\n",
      "epoch: 1 step: 2, loss is 2.3025059700012207\n",
      "epoch: 1 step: 3, loss is 2.3019862174987793\n",
      "epoch: 1 step: 4, loss is 2.3009018898010254\n",
      "epoch: 1 step: 5, loss is 2.2970476150512695\n",
      "epoch: 1 step: 6, loss is 2.280426502227783\n",
      "epoch: 1 step: 7, loss is 2.278353691101074\n",
      "epoch: 1 step: 8, loss is 2.2417538166046143\n",
      "epoch: 1 step: 9, loss is 2.1988327503204346\n",
      "epoch: 1 step: 10, loss is 2.216838836669922\n",
      "epoch: 1 step: 11, loss is 2.1320013999938965\n",
      "epoch: 1 step: 12, loss is 2.010350227355957\n",
      "epoch: 1 step: 13, loss is 1.855591058731079\n",
      "epoch: 1 step: 14, loss is 1.6714884042739868\n",
      "epoch: 1 step: 15, loss is 1.4043824672698975\n",
      "epoch: 1 step: 16, loss is 1.5570423603057861\n",
      "epoch: 1 step: 17, loss is 1.1973830461502075\n",
      "epoch: 1 step: 18, loss is 1.3200874328613281\n",
      "epoch: 1 step: 19, loss is 1.1063528060913086\n",
      "epoch: 1 step: 20, loss is 0.971346378326416\n",
      "epoch: 1 step: 21, loss is 1.134806752204895\n",
      "epoch: 1 step: 22, loss is 1.2777841091156006\n",
      "epoch: 1 step: 23, loss is 0.9166046380996704\n",
      "epoch: 1 step: 24, loss is 1.0835905075073242\n",
      "epoch: 1 step: 25, loss is 1.284921646118164\n",
      "epoch: 1 step: 26, loss is 1.2690224647521973\n",
      "epoch: 1 step: 27, loss is 1.262198567390442\n",
      "epoch: 1 step: 28, loss is 1.1508686542510986\n",
      "epoch: 1 step: 29, loss is 1.0105798244476318\n",
      "epoch: 1 step: 30, loss is 1.1512913703918457\n",
      "epoch: 1 step: 31, loss is 0.975149393081665\n",
      "epoch: 1 step: 32, loss is 1.2453479766845703\n",
      "epoch: 1 step: 33, loss is 1.059248924255371\n",
      "epoch: 1 step: 34, loss is 0.9947094321250916\n",
      "epoch: 1 step: 35, loss is 0.997164785861969\n",
      "epoch: 1 step: 36, loss is 1.0869587659835815\n",
      "epoch: 1 step: 37, loss is 1.0422921180725098\n",
      "epoch: 1 step: 38, loss is 1.11647629737854\n",
      "epoch: 1 step: 39, loss is 1.1530981063842773\n",
      "epoch: 1 step: 40, loss is 0.9130014181137085\n",
      "epoch: 1 step: 41, loss is 1.0321590900421143\n",
      "epoch: 1 step: 42, loss is 1.2923775911331177\n",
      "epoch: 1 step: 43, loss is 0.9495123028755188\n",
      "epoch: 1 step: 44, loss is 0.8234150409698486\n",
      "epoch: 1 step: 45, loss is 0.9591885805130005\n",
      "epoch: 1 step: 46, loss is 0.7587853074073792\n",
      "epoch: 1 step: 47, loss is 0.985808253288269\n",
      "epoch: 1 step: 48, loss is 1.0331454277038574\n",
      "epoch: 1 step: 49, loss is 1.147512435913086\n",
      "epoch: 1 step: 50, loss is 0.8336738348007202\n",
      "epoch: 1 step: 51, loss is 0.868308424949646\n",
      "epoch: 1 step: 52, loss is 0.8282153010368347\n",
      "epoch: 1 step: 53, loss is 0.9290664196014404\n",
      "epoch: 1 step: 54, loss is 0.9973421692848206\n",
      "epoch: 1 step: 55, loss is 0.8636634349822998\n",
      "epoch: 1 step: 56, loss is 0.9409748315811157\n",
      "epoch: 1 step: 57, loss is 0.8636138439178467\n",
      "epoch: 1 step: 58, loss is 0.705443263053894\n",
      "epoch: 1 step: 59, loss is 0.7981452941894531\n",
      "epoch: 1 step: 60, loss is 0.8678174018859863\n",
      "epoch: 1 step: 61, loss is 0.7599840760231018\n",
      "epoch: 1 step: 62, loss is 0.8128488659858704\n",
      "epoch: 1 step: 63, loss is 1.0075535774230957\n",
      "epoch: 1 step: 64, loss is 0.9353087544441223\n",
      "epoch: 1 step: 65, loss is 0.6207699775695801\n",
      "epoch: 1 step: 66, loss is 0.7852556109428406\n",
      "epoch: 1 step: 67, loss is 0.8910566568374634\n",
      "epoch: 1 step: 68, loss is 1.0242122411727905\n",
      "epoch: 1 step: 69, loss is 0.5553841590881348\n",
      "epoch: 1 step: 70, loss is 0.507612943649292\n",
      "epoch: 1 step: 71, loss is 0.7672445774078369\n",
      "epoch: 1 step: 72, loss is 0.9202802181243896\n",
      "epoch: 1 step: 73, loss is 0.7135509848594666\n",
      "epoch: 1 step: 74, loss is 0.6978912353515625\n",
      "epoch: 1 step: 75, loss is 0.7254188656806946\n",
      "epoch: 1 step: 76, loss is 0.8001282811164856\n",
      "epoch: 1 step: 77, loss is 0.9569026231765747\n",
      "epoch: 1 step: 78, loss is 0.6212407350540161\n",
      "epoch: 1 step: 79, loss is 0.8123598098754883\n",
      "epoch: 1 step: 80, loss is 0.7447717189788818\n",
      "epoch: 1 step: 81, loss is 1.0885558128356934\n",
      "epoch: 1 step: 82, loss is 0.8133611083030701\n",
      "epoch: 1 step: 83, loss is 0.8501602411270142\n",
      "epoch: 1 step: 84, loss is 0.6977539658546448\n",
      "epoch: 1 step: 85, loss is 0.8839765787124634\n",
      "epoch: 1 step: 86, loss is 0.8885543942451477\n",
      "epoch: 1 step: 87, loss is 0.7761056423187256\n",
      "epoch: 1 step: 88, loss is 0.9167602062225342\n",
      "epoch: 1 step: 89, loss is 0.8958017230033875\n",
      "epoch: 1 step: 90, loss is 0.7525750398635864\n",
      "epoch: 1 step: 91, loss is 0.8681595325469971\n",
      "epoch: 1 step: 92, loss is 0.6299015283584595\n",
      "epoch: 1 step: 93, loss is 0.821155846118927\n",
      "epoch: 1 step: 94, loss is 0.6843342185020447\n",
      "epoch: 1 step: 95, loss is 0.8052064180374146\n",
      "epoch: 1 step: 96, loss is 0.539121150970459\n",
      "epoch: 1 step: 97, loss is 0.6521576642990112\n",
      "epoch: 1 step: 98, loss is 0.6770565509796143\n",
      "epoch: 1 step: 99, loss is 1.0297088623046875\n",
      "epoch: 1 step: 100, loss is 0.5169363021850586\n",
      "epoch: 1 step: 101, loss is 0.7760071158409119\n",
      "epoch: 1 step: 102, loss is 0.5199961066246033\n",
      "epoch: 1 step: 103, loss is 0.7795811891555786\n",
      "epoch: 1 step: 104, loss is 0.7114057540893555\n",
      "epoch: 1 step: 105, loss is 0.8022569417953491\n",
      "epoch: 1 step: 106, loss is 0.8135969042778015\n",
      "epoch: 1 step: 107, loss is 0.5070033073425293\n",
      "epoch: 1 step: 108, loss is 0.4852033853530884\n",
      "epoch: 1 step: 109, loss is 0.7170591354370117\n",
      "epoch: 1 step: 110, loss is 0.49872246384620667\n",
      "epoch: 1 step: 111, loss is 0.6485438346862793\n",
      "epoch: 1 step: 112, loss is 0.7209184169769287\n",
      "epoch: 1 step: 113, loss is 0.7107676863670349\n",
      "epoch: 1 step: 114, loss is 0.5624498724937439\n",
      "epoch: 1 step: 115, loss is 0.7699045538902283\n",
      "epoch: 1 step: 116, loss is 0.6370594501495361\n",
      "epoch: 1 step: 117, loss is 0.8189327716827393\n",
      "epoch: 1 step: 118, loss is 0.492464154958725\n",
      "epoch: 1 step: 119, loss is 0.5182767510414124\n",
      "epoch: 1 step: 120, loss is 0.7536008358001709\n",
      "epoch: 1 step: 121, loss is 0.7251213788986206\n",
      "epoch: 1 step: 122, loss is 0.6051260232925415\n",
      "epoch: 1 step: 123, loss is 0.504720151424408\n",
      "epoch: 1 step: 124, loss is 0.7928706407546997\n",
      "epoch: 1 step: 125, loss is 0.4814508259296417\n",
      "epoch: 1 step: 126, loss is 0.5237792730331421\n",
      "epoch: 1 step: 127, loss is 0.6059064865112305\n",
      "epoch: 1 step: 128, loss is 0.735806941986084\n",
      "epoch: 1 step: 129, loss is 0.6035118103027344\n",
      "epoch: 1 step: 130, loss is 0.4853244721889496\n",
      "epoch: 1 step: 131, loss is 0.5271649360656738\n",
      "epoch: 1 step: 132, loss is 0.6614543199539185\n",
      "epoch: 1 step: 133, loss is 0.6221015453338623\n",
      "epoch: 1 step: 134, loss is 0.8046249151229858\n",
      "epoch: 1 step: 135, loss is 0.5196520686149597\n",
      "epoch: 1 step: 136, loss is 0.4615115523338318\n",
      "epoch: 1 step: 137, loss is 0.741604208946228\n",
      "epoch: 1 step: 138, loss is 0.6889994144439697\n",
      "epoch: 1 step: 139, loss is 0.6729509830474854\n",
      "epoch: 1 step: 140, loss is 0.46629494428634644\n",
      "epoch: 1 step: 141, loss is 0.6457343697547913\n",
      "epoch: 1 step: 142, loss is 0.5736888647079468\n",
      "epoch: 1 step: 143, loss is 0.5084324479103088\n",
      "epoch: 1 step: 144, loss is 0.6027208566665649\n",
      "epoch: 1 step: 145, loss is 0.6075156927108765\n",
      "epoch: 1 step: 146, loss is 0.6745595932006836\n",
      "epoch: 1 step: 147, loss is 0.9076056480407715\n",
      "epoch: 1 step: 148, loss is 0.7500970363616943\n",
      "epoch: 1 step: 149, loss is 0.5270667672157288\n",
      "epoch: 1 step: 150, loss is 0.7046250104904175\n",
      "epoch: 1 step: 151, loss is 0.6182858347892761\n",
      "epoch: 1 step: 152, loss is 0.6003051996231079\n",
      "epoch: 1 step: 153, loss is 0.6495481133460999\n",
      "epoch: 1 step: 154, loss is 0.4717232286930084\n",
      "epoch: 1 step: 155, loss is 0.5522335767745972\n",
      "epoch: 1 step: 156, loss is 0.530982494354248\n",
      "epoch: 1 step: 157, loss is 0.6076852679252625\n",
      "epoch: 1 step: 158, loss is 0.6088708639144897\n",
      "epoch: 1 step: 159, loss is 0.7276773452758789\n",
      "epoch: 1 step: 160, loss is 0.5671523809432983\n",
      "epoch: 1 step: 161, loss is 0.5786901712417603\n",
      "epoch: 1 step: 162, loss is 0.5223504304885864\n",
      "epoch: 1 step: 163, loss is 0.4750562310218811\n",
      "epoch: 1 step: 164, loss is 0.5505231618881226\n",
      "epoch: 1 step: 165, loss is 0.5966436266899109\n",
      "epoch: 1 step: 166, loss is 0.4706230163574219\n",
      "epoch: 1 step: 167, loss is 0.652308464050293\n",
      "epoch: 1 step: 168, loss is 0.9077680110931396\n",
      "epoch: 1 step: 169, loss is 0.30232617259025574\n",
      "epoch: 1 step: 170, loss is 0.49848756194114685\n",
      "epoch: 1 step: 171, loss is 0.4937308430671692\n",
      "epoch: 1 step: 172, loss is 0.6418777704238892\n",
      "epoch: 1 step: 173, loss is 0.5074379444122314\n",
      "epoch: 1 step: 174, loss is 0.523387610912323\n",
      "epoch: 1 step: 175, loss is 0.5422257781028748\n",
      "epoch: 1 step: 176, loss is 0.6262015700340271\n",
      "epoch: 1 step: 177, loss is 0.4821080267429352\n",
      "epoch: 1 step: 178, loss is 0.5141680836677551\n",
      "epoch: 1 step: 179, loss is 0.5648149251937866\n",
      "epoch: 1 step: 180, loss is 0.5584814548492432\n",
      "epoch: 1 step: 181, loss is 0.5834442377090454\n",
      "epoch: 1 step: 182, loss is 0.7046815752983093\n",
      "epoch: 1 step: 183, loss is 0.605596661567688\n",
      "epoch: 1 step: 184, loss is 0.38246285915374756\n",
      "epoch: 1 step: 185, loss is 0.5152230262756348\n",
      "epoch: 1 step: 186, loss is 0.8528454303741455\n",
      "epoch: 1 step: 187, loss is 0.6038593053817749\n",
      "epoch: 1 step: 188, loss is 0.3794218897819519\n",
      "epoch: 1 step: 189, loss is 0.47808200120925903\n",
      "epoch: 1 step: 190, loss is 0.41097408533096313\n",
      "epoch: 1 step: 191, loss is 0.6358157396316528\n",
      "epoch: 1 step: 192, loss is 0.6216544508934021\n",
      "epoch: 1 step: 193, loss is 0.5265107154846191\n",
      "epoch: 1 step: 194, loss is 0.39309006929397583\n",
      "epoch: 1 step: 195, loss is 0.6173303723335266\n",
      "epoch: 1 step: 196, loss is 0.7300134897232056\n",
      "epoch: 1 step: 197, loss is 0.5665240287780762\n",
      "epoch: 1 step: 198, loss is 0.49838829040527344\n",
      "epoch: 1 step: 199, loss is 0.4525211453437805\n",
      "epoch: 1 step: 200, loss is 0.5351438522338867\n",
      "epoch: 1 step: 201, loss is 0.6744952201843262\n",
      "epoch: 1 step: 202, loss is 0.7153040170669556\n",
      "epoch: 1 step: 203, loss is 0.4666760563850403\n",
      "epoch: 1 step: 204, loss is 0.4306171238422394\n",
      "epoch: 1 step: 205, loss is 0.4333539605140686\n",
      "epoch: 1 step: 206, loss is 0.6297624111175537\n",
      "epoch: 1 step: 207, loss is 0.4147569537162781\n",
      "epoch: 1 step: 208, loss is 0.624565839767456\n",
      "epoch: 1 step: 209, loss is 0.7120126485824585\n",
      "epoch: 1 step: 210, loss is 0.5273729562759399\n",
      "epoch: 1 step: 211, loss is 0.5318301916122437\n",
      "epoch: 1 step: 212, loss is 0.7164583206176758\n",
      "epoch: 1 step: 213, loss is 0.4732002019882202\n",
      "epoch: 1 step: 214, loss is 0.4584749639034271\n",
      "epoch: 1 step: 215, loss is 0.3673974871635437\n",
      "epoch: 1 step: 216, loss is 0.6357823610305786\n",
      "epoch: 1 step: 217, loss is 0.6370006799697876\n",
      "epoch: 1 step: 218, loss is 0.6725255250930786\n",
      "epoch: 1 step: 219, loss is 0.5669087171554565\n",
      "epoch: 1 step: 220, loss is 0.6885867714881897\n",
      "epoch: 1 step: 221, loss is 0.7001160383224487\n",
      "epoch: 1 step: 222, loss is 0.6086002588272095\n",
      "epoch: 1 step: 223, loss is 0.6796718835830688\n",
      "epoch: 1 step: 224, loss is 0.43781211972236633\n",
      "epoch: 1 step: 225, loss is 0.3969789743423462\n",
      "epoch: 1 step: 226, loss is 0.5693894624710083\n",
      "epoch: 1 step: 227, loss is 0.6417038440704346\n",
      "epoch: 1 step: 228, loss is 0.525896430015564\n",
      "epoch: 1 step: 229, loss is 0.37858638167381287\n",
      "epoch: 1 step: 230, loss is 0.5325286388397217\n",
      "epoch: 1 step: 231, loss is 0.49464941024780273\n",
      "epoch: 1 step: 232, loss is 0.4045374095439911\n",
      "epoch: 1 step: 233, loss is 0.6628423929214478\n",
      "epoch: 1 step: 234, loss is 0.6122993230819702\n",
      "epoch: 1 step: 235, loss is 0.42969852685928345\n",
      "epoch: 1 step: 236, loss is 0.32249313592910767\n",
      "epoch: 1 step: 237, loss is 0.5515960454940796\n",
      "epoch: 1 step: 238, loss is 0.41047361493110657\n",
      "epoch: 1 step: 239, loss is 0.5022088885307312\n",
      "epoch: 1 step: 240, loss is 0.7784174680709839\n",
      "epoch: 1 step: 241, loss is 0.5448148250579834\n",
      "epoch: 1 step: 242, loss is 0.5955530405044556\n",
      "epoch: 1 step: 243, loss is 0.4805314540863037\n",
      "epoch: 1 step: 244, loss is 0.37740689516067505\n",
      "epoch: 1 step: 245, loss is 0.5171756744384766\n",
      "epoch: 1 step: 246, loss is 0.4800373911857605\n",
      "epoch: 1 step: 247, loss is 0.4695340394973755\n",
      "epoch: 1 step: 248, loss is 0.3822469115257263\n",
      "epoch: 1 step: 249, loss is 0.5983410477638245\n",
      "epoch: 1 step: 250, loss is 0.5924341678619385\n",
      "epoch: 1 step: 251, loss is 0.48364198207855225\n",
      "epoch: 1 step: 252, loss is 0.6744986772537231\n",
      "epoch: 1 step: 253, loss is 0.49883347749710083\n",
      "epoch: 1 step: 254, loss is 0.28866270184516907\n",
      "epoch: 1 step: 255, loss is 0.6114211678504944\n",
      "epoch: 1 step: 256, loss is 0.44135361909866333\n",
      "epoch: 1 step: 257, loss is 0.6583679914474487\n",
      "epoch: 1 step: 258, loss is 0.5851033926010132\n",
      "epoch: 1 step: 259, loss is 0.6235361695289612\n",
      "epoch: 1 step: 260, loss is 0.3691942095756531\n",
      "epoch: 1 step: 261, loss is 0.3742249011993408\n",
      "epoch: 1 step: 262, loss is 0.39018791913986206\n",
      "epoch: 1 step: 263, loss is 0.4162008464336395\n",
      "epoch: 1 step: 264, loss is 0.4399570822715759\n",
      "epoch: 1 step: 265, loss is 0.3014686703681946\n",
      "epoch: 1 step: 266, loss is 0.4299915134906769\n",
      "epoch: 1 step: 267, loss is 0.6792382001876831\n",
      "epoch: 1 step: 268, loss is 0.3888392746448517\n",
      "epoch: 1 step: 269, loss is 0.45286232233047485\n",
      "epoch: 1 step: 270, loss is 0.308819055557251\n",
      "epoch: 1 step: 271, loss is 0.6521508097648621\n",
      "epoch: 1 step: 272, loss is 0.3720961809158325\n",
      "epoch: 1 step: 273, loss is 0.45649290084838867\n",
      "epoch: 1 step: 274, loss is 0.4263971447944641\n",
      "epoch: 1 step: 275, loss is 0.71712327003479\n",
      "epoch: 1 step: 276, loss is 0.6148738861083984\n",
      "epoch: 1 step: 277, loss is 0.6490228176116943\n",
      "epoch: 1 step: 278, loss is 0.4264899492263794\n",
      "epoch: 1 step: 279, loss is 0.4368765950202942\n",
      "epoch: 1 step: 280, loss is 0.5083897113800049\n",
      "epoch: 1 step: 281, loss is 0.5040990114212036\n",
      "epoch: 1 step: 282, loss is 0.5128475427627563\n",
      "epoch: 1 step: 283, loss is 0.45494869351387024\n",
      "epoch: 1 step: 284, loss is 0.49695488810539246\n",
      "epoch: 1 step: 285, loss is 0.5224338173866272\n",
      "epoch: 1 step: 286, loss is 0.4536738693714142\n",
      "epoch: 1 step: 287, loss is 0.6123940944671631\n",
      "epoch: 1 step: 288, loss is 0.5149576663970947\n",
      "epoch: 1 step: 289, loss is 0.5483332276344299\n",
      "epoch: 1 step: 290, loss is 0.5194336175918579\n",
      "epoch: 1 step: 291, loss is 0.7183877229690552\n",
      "epoch: 1 step: 292, loss is 0.4147607088088989\n",
      "epoch: 1 step: 293, loss is 0.6550652980804443\n",
      "epoch: 1 step: 294, loss is 0.5006102323532104\n",
      "epoch: 1 step: 295, loss is 0.47255080938339233\n",
      "epoch: 1 step: 296, loss is 0.4049736261367798\n",
      "epoch: 1 step: 297, loss is 0.5585421323776245\n",
      "epoch: 1 step: 298, loss is 0.4918828308582306\n",
      "epoch: 1 step: 299, loss is 0.3651443123817444\n",
      "epoch: 1 step: 300, loss is 0.36185741424560547\n",
      "epoch: 1 step: 301, loss is 0.4265183210372925\n",
      "epoch: 1 step: 302, loss is 0.5221580266952515\n",
      "epoch: 1 step: 303, loss is 0.5341858863830566\n",
      "epoch: 1 step: 304, loss is 0.49478501081466675\n",
      "epoch: 1 step: 305, loss is 0.514962375164032\n",
      "epoch: 1 step: 306, loss is 0.37813419103622437\n",
      "epoch: 1 step: 307, loss is 0.389212965965271\n",
      "epoch: 1 step: 308, loss is 0.5207563042640686\n",
      "epoch: 1 step: 309, loss is 0.6184428930282593\n",
      "epoch: 1 step: 310, loss is 0.47254467010498047\n",
      "epoch: 1 step: 311, loss is 0.4466358423233032\n",
      "epoch: 1 step: 312, loss is 0.3548225164413452\n",
      "epoch: 1 step: 313, loss is 0.5560418963432312\n",
      "epoch: 1 step: 314, loss is 0.3168817162513733\n",
      "epoch: 1 step: 315, loss is 0.6769965291023254\n",
      "epoch: 1 step: 316, loss is 0.6150383949279785\n",
      "epoch: 1 step: 317, loss is 0.38693928718566895\n",
      "epoch: 1 step: 318, loss is 0.4859873056411743\n",
      "epoch: 1 step: 319, loss is 0.5112049579620361\n",
      "epoch: 1 step: 320, loss is 0.4851641058921814\n",
      "epoch: 1 step: 321, loss is 0.5690500736236572\n",
      "epoch: 1 step: 322, loss is 0.4306638836860657\n",
      "epoch: 1 step: 323, loss is 0.6265096664428711\n",
      "epoch: 1 step: 324, loss is 0.47654372453689575\n",
      "epoch: 1 step: 325, loss is 0.693050742149353\n",
      "epoch: 1 step: 326, loss is 0.4837982952594757\n",
      "epoch: 1 step: 327, loss is 0.5184380412101746\n",
      "epoch: 1 step: 328, loss is 0.6373450756072998\n",
      "epoch: 1 step: 329, loss is 0.5474866628646851\n",
      "epoch: 1 step: 330, loss is 0.5388063192367554\n",
      "epoch: 1 step: 331, loss is 0.4630347788333893\n",
      "epoch: 1 step: 332, loss is 0.5684691667556763\n",
      "epoch: 1 step: 333, loss is 0.5168792605400085\n",
      "epoch: 1 step: 334, loss is 0.39014995098114014\n",
      "epoch: 1 step: 335, loss is 0.4387701153755188\n",
      "epoch: 1 step: 336, loss is 0.6119018793106079\n",
      "epoch: 1 step: 337, loss is 0.5538501739501953\n",
      "epoch: 1 step: 338, loss is 0.4322069585323334\n",
      "epoch: 1 step: 339, loss is 0.4052220284938812\n",
      "epoch: 1 step: 340, loss is 0.3931812644004822\n",
      "epoch: 1 step: 341, loss is 0.2510821223258972\n",
      "epoch: 1 step: 342, loss is 0.4477198123931885\n",
      "epoch: 1 step: 343, loss is 0.41398781538009644\n",
      "epoch: 1 step: 344, loss is 0.5015286803245544\n",
      "epoch: 1 step: 345, loss is 0.33283576369285583\n",
      "epoch: 1 step: 346, loss is 0.33543890714645386\n",
      "epoch: 1 step: 347, loss is 0.4986063539981842\n",
      "epoch: 1 step: 348, loss is 0.3931085467338562\n",
      "epoch: 1 step: 349, loss is 0.3686508536338806\n",
      "epoch: 1 step: 350, loss is 0.37401479482650757\n",
      "epoch: 1 step: 351, loss is 0.46888265013694763\n",
      "epoch: 1 step: 352, loss is 0.7015564441680908\n",
      "epoch: 1 step: 353, loss is 0.2744455337524414\n",
      "epoch: 1 step: 354, loss is 0.5619689226150513\n",
      "epoch: 1 step: 355, loss is 0.5832059383392334\n",
      "epoch: 1 step: 356, loss is 0.5718385577201843\n",
      "epoch: 1 step: 357, loss is 0.601048469543457\n",
      "epoch: 1 step: 358, loss is 0.779585599899292\n",
      "epoch: 1 step: 359, loss is 0.7180308699607849\n",
      "epoch: 1 step: 360, loss is 0.4584699869155884\n",
      "epoch: 1 step: 361, loss is 0.36989110708236694\n",
      "epoch: 1 step: 362, loss is 0.4529121518135071\n",
      "epoch: 1 step: 363, loss is 0.4811914265155792\n",
      "epoch: 1 step: 364, loss is 0.726723313331604\n",
      "epoch: 1 step: 365, loss is 0.3825404942035675\n",
      "epoch: 1 step: 366, loss is 0.446550190448761\n",
      "epoch: 1 step: 367, loss is 0.6063551306724548\n",
      "epoch: 1 step: 368, loss is 0.5491712093353271\n",
      "epoch: 1 step: 369, loss is 0.41933146119117737\n",
      "epoch: 1 step: 370, loss is 0.49128633737564087\n",
      "epoch: 1 step: 371, loss is 0.47327688336372375\n",
      "epoch: 1 step: 372, loss is 0.32288265228271484\n",
      "epoch: 1 step: 373, loss is 0.48455941677093506\n",
      "epoch: 1 step: 374, loss is 0.4041188359260559\n",
      "epoch: 1 step: 375, loss is 0.3893514573574066\n",
      "epoch: 1 step: 376, loss is 0.6457912921905518\n",
      "epoch: 1 step: 377, loss is 0.3968883752822876\n",
      "epoch: 1 step: 378, loss is 0.5935812592506409\n",
      "epoch: 1 step: 379, loss is 0.3164817690849304\n",
      "epoch: 1 step: 380, loss is 0.3346022665500641\n",
      "epoch: 1 step: 381, loss is 0.5572807788848877\n",
      "epoch: 1 step: 382, loss is 0.34155982732772827\n",
      "epoch: 1 step: 383, loss is 0.43594425916671753\n",
      "epoch: 1 step: 384, loss is 0.4867691397666931\n",
      "epoch: 1 step: 385, loss is 0.38355129957199097\n",
      "epoch: 1 step: 386, loss is 0.7362351417541504\n",
      "epoch: 1 step: 387, loss is 0.3694070875644684\n",
      "epoch: 1 step: 388, loss is 0.6076766848564148\n",
      "epoch: 1 step: 389, loss is 0.5479594469070435\n",
      "epoch: 1 step: 390, loss is 0.4640386700630188\n",
      "epoch: 1 step: 391, loss is 0.5405064225196838\n",
      "epoch: 1 step: 392, loss is 0.4327058494091034\n",
      "epoch: 1 step: 393, loss is 0.3648284077644348\n",
      "epoch: 1 step: 394, loss is 0.5461652278900146\n",
      "epoch: 1 step: 395, loss is 0.4116787910461426\n",
      "epoch: 1 step: 396, loss is 0.2630816698074341\n",
      "epoch: 1 step: 397, loss is 0.5621182918548584\n",
      "epoch: 1 step: 398, loss is 0.556851863861084\n",
      "epoch: 1 step: 399, loss is 0.5317463278770447\n",
      "epoch: 1 step: 400, loss is 0.40605998039245605\n",
      "epoch: 1 step: 401, loss is 0.5126870274543762\n",
      "epoch: 1 step: 402, loss is 0.29500699043273926\n",
      "epoch: 1 step: 403, loss is 0.4857669472694397\n",
      "epoch: 1 step: 404, loss is 0.3915863037109375\n",
      "epoch: 1 step: 405, loss is 0.5114170908927917\n",
      "epoch: 1 step: 406, loss is 0.45066162943840027\n",
      "epoch: 1 step: 407, loss is 0.4239170551300049\n",
      "epoch: 1 step: 408, loss is 0.4868727922439575\n",
      "epoch: 1 step: 409, loss is 0.40336620807647705\n",
      "epoch: 1 step: 410, loss is 0.4127455949783325\n",
      "epoch: 1 step: 411, loss is 0.6077759861946106\n",
      "epoch: 1 step: 412, loss is 0.5187019109725952\n",
      "epoch: 1 step: 413, loss is 0.4201123118400574\n",
      "epoch: 1 step: 414, loss is 0.3184873163700104\n",
      "epoch: 1 step: 415, loss is 0.4310256838798523\n",
      "epoch: 1 step: 416, loss is 0.3744322955608368\n",
      "epoch: 1 step: 417, loss is 0.3682839572429657\n",
      "epoch: 1 step: 418, loss is 0.3989638090133667\n",
      "epoch: 1 step: 419, loss is 0.3503304719924927\n",
      "epoch: 1 step: 420, loss is 0.674758791923523\n",
      "epoch: 1 step: 421, loss is 0.5323059558868408\n",
      "epoch: 1 step: 422, loss is 0.26485326886177063\n",
      "epoch: 1 step: 423, loss is 0.4273325204849243\n",
      "epoch: 1 step: 424, loss is 0.6090786457061768\n",
      "epoch: 1 step: 425, loss is 0.353582501411438\n",
      "epoch: 1 step: 426, loss is 0.3367416262626648\n",
      "epoch: 1 step: 427, loss is 0.4318946301937103\n",
      "epoch: 1 step: 428, loss is 0.3430955410003662\n",
      "epoch: 1 step: 429, loss is 0.38944610953330994\n",
      "epoch: 1 step: 430, loss is 0.38260704278945923\n",
      "epoch: 1 step: 431, loss is 0.26286953687667847\n",
      "epoch: 1 step: 432, loss is 0.37346893548965454\n",
      "epoch: 1 step: 433, loss is 0.3901052474975586\n",
      "epoch: 1 step: 434, loss is 0.2906803488731384\n",
      "epoch: 1 step: 435, loss is 0.34003350138664246\n",
      "epoch: 1 step: 436, loss is 0.26300281286239624\n",
      "epoch: 1 step: 437, loss is 0.5200713276863098\n",
      "epoch: 1 step: 438, loss is 0.8588995933532715\n",
      "epoch: 1 step: 439, loss is 0.4541972875595093\n",
      "epoch: 1 step: 440, loss is 0.4265318512916565\n",
      "epoch: 1 step: 441, loss is 0.33230060338974\n",
      "epoch: 1 step: 442, loss is 0.4534963369369507\n",
      "epoch: 1 step: 443, loss is 0.38046810030937195\n",
      "epoch: 1 step: 444, loss is 0.41887009143829346\n",
      "epoch: 1 step: 445, loss is 0.4396652579307556\n",
      "epoch: 1 step: 446, loss is 0.423850953578949\n",
      "epoch: 1 step: 447, loss is 0.4450528919696808\n",
      "epoch: 1 step: 448, loss is 0.4080662429332733\n",
      "epoch: 1 step: 449, loss is 0.45058536529541016\n",
      "epoch: 1 step: 450, loss is 0.32671934366226196\n",
      "epoch: 1 step: 451, loss is 0.3605417013168335\n",
      "epoch: 1 step: 452, loss is 0.3976684808731079\n",
      "epoch: 1 step: 453, loss is 0.2920241951942444\n",
      "epoch: 1 step: 454, loss is 0.35211560130119324\n",
      "epoch: 1 step: 455, loss is 0.3224448561668396\n",
      "epoch: 1 step: 456, loss is 0.5486248731613159\n",
      "epoch: 1 step: 457, loss is 0.3719772696495056\n",
      "epoch: 1 step: 458, loss is 0.40738645195961\n",
      "epoch: 1 step: 459, loss is 0.3194112777709961\n",
      "epoch: 1 step: 460, loss is 0.3164333403110504\n",
      "epoch: 1 step: 461, loss is 0.380962073802948\n",
      "epoch: 1 step: 462, loss is 0.31631147861480713\n",
      "epoch: 1 step: 463, loss is 0.30281099677085876\n",
      "epoch: 1 step: 464, loss is 0.3958825469017029\n",
      "epoch: 1 step: 465, loss is 0.5889667868614197\n",
      "epoch: 1 step: 466, loss is 0.5246174335479736\n",
      "epoch: 1 step: 467, loss is 0.27972090244293213\n",
      "epoch: 1 step: 468, loss is 0.5170516967773438\n",
      "epoch: 1 step: 469, loss is 0.23403595387935638\n",
      "epoch: 1 step: 470, loss is 0.5648584365844727\n",
      "epoch: 1 step: 471, loss is 0.48565250635147095\n",
      "epoch: 1 step: 472, loss is 0.4685990810394287\n",
      "epoch: 1 step: 473, loss is 0.38001322746276855\n",
      "epoch: 1 step: 474, loss is 0.43010950088500977\n",
      "epoch: 1 step: 475, loss is 0.3171050250530243\n",
      "epoch: 1 step: 476, loss is 0.5360503792762756\n",
      "epoch: 1 step: 477, loss is 0.4398873448371887\n",
      "epoch: 1 step: 478, loss is 0.4420396387577057\n",
      "epoch: 1 step: 479, loss is 0.5403413772583008\n",
      "epoch: 1 step: 480, loss is 0.38367870450019836\n",
      "epoch: 1 step: 481, loss is 0.5670795440673828\n",
      "epoch: 1 step: 482, loss is 0.41340166330337524\n",
      "epoch: 1 step: 483, loss is 0.4004402756690979\n",
      "epoch: 1 step: 484, loss is 0.5112613439559937\n",
      "epoch: 1 step: 485, loss is 0.39584046602249146\n",
      "epoch: 1 step: 486, loss is 0.35887521505355835\n",
      "epoch: 1 step: 487, loss is 0.38999229669570923\n",
      "epoch: 1 step: 488, loss is 0.4787403643131256\n",
      "epoch: 1 step: 489, loss is 0.3732586205005646\n",
      "epoch: 1 step: 490, loss is 0.36301785707473755\n",
      "epoch: 1 step: 491, loss is 0.38159245252609253\n",
      "epoch: 1 step: 492, loss is 0.4487529993057251\n",
      "epoch: 1 step: 493, loss is 0.3938339352607727\n",
      "epoch: 1 step: 494, loss is 0.4676583409309387\n",
      "epoch: 1 step: 495, loss is 0.5116421580314636\n",
      "epoch: 1 step: 496, loss is 0.4547070860862732\n",
      "epoch: 1 step: 497, loss is 0.3369981646537781\n",
      "epoch: 1 step: 498, loss is 0.3499807119369507\n",
      "epoch: 1 step: 499, loss is 0.2953360080718994\n",
      "epoch: 1 step: 500, loss is 0.28480061888694763\n",
      "epoch: 1 step: 501, loss is 0.2728804051876068\n",
      "epoch: 1 step: 502, loss is 0.3499852418899536\n",
      "epoch: 1 step: 503, loss is 0.7076255083084106\n",
      "epoch: 1 step: 504, loss is 0.43222635984420776\n",
      "epoch: 1 step: 505, loss is 0.5916486382484436\n",
      "epoch: 1 step: 506, loss is 0.41354167461395264\n",
      "epoch: 1 step: 507, loss is 0.40453147888183594\n",
      "epoch: 1 step: 508, loss is 0.3821234703063965\n",
      "epoch: 1 step: 509, loss is 0.3890705108642578\n",
      "epoch: 1 step: 510, loss is 0.4332076609134674\n",
      "epoch: 1 step: 511, loss is 0.4450107216835022\n",
      "epoch: 1 step: 512, loss is 0.4097859859466553\n",
      "epoch: 1 step: 513, loss is 0.3746400475502014\n",
      "epoch: 1 step: 514, loss is 0.34839439392089844\n",
      "epoch: 1 step: 515, loss is 0.3358208239078522\n",
      "epoch: 1 step: 516, loss is 0.40713736414909363\n",
      "epoch: 1 step: 517, loss is 0.19947363436222076\n",
      "epoch: 1 step: 518, loss is 0.4930891990661621\n",
      "epoch: 1 step: 519, loss is 0.4084453880786896\n",
      "epoch: 1 step: 520, loss is 0.6574833393096924\n",
      "epoch: 1 step: 521, loss is 0.36033302545547485\n",
      "epoch: 1 step: 522, loss is 0.30499786138534546\n",
      "epoch: 1 step: 523, loss is 0.37924283742904663\n",
      "epoch: 1 step: 524, loss is 0.4665239453315735\n",
      "epoch: 1 step: 525, loss is 0.3965546786785126\n",
      "epoch: 1 step: 526, loss is 0.4077167212963104\n",
      "epoch: 1 step: 527, loss is 0.6407985687255859\n",
      "epoch: 1 step: 528, loss is 0.42003390192985535\n",
      "epoch: 1 step: 529, loss is 0.36575251817703247\n",
      "epoch: 1 step: 530, loss is 0.2869744300842285\n",
      "epoch: 1 step: 531, loss is 0.593635082244873\n",
      "epoch: 1 step: 532, loss is 0.4369620084762573\n",
      "epoch: 1 step: 533, loss is 0.42336100339889526\n",
      "epoch: 1 step: 534, loss is 0.3503771722316742\n",
      "epoch: 1 step: 535, loss is 0.44113466143608093\n",
      "epoch: 1 step: 536, loss is 0.3056752681732178\n",
      "epoch: 1 step: 537, loss is 0.34490716457366943\n",
      "epoch: 1 step: 538, loss is 0.4331601858139038\n",
      "epoch: 1 step: 539, loss is 0.5382879972457886\n",
      "epoch: 1 step: 540, loss is 0.4007534384727478\n",
      "epoch: 1 step: 541, loss is 0.425062894821167\n",
      "epoch: 1 step: 542, loss is 0.3451926112174988\n",
      "epoch: 1 step: 543, loss is 0.4248332381248474\n",
      "epoch: 1 step: 544, loss is 0.3670867681503296\n",
      "epoch: 1 step: 545, loss is 0.41444131731987\n",
      "epoch: 1 step: 546, loss is 0.5501428842544556\n",
      "epoch: 1 step: 547, loss is 0.5221966505050659\n",
      "epoch: 1 step: 548, loss is 0.654977560043335\n",
      "epoch: 1 step: 549, loss is 0.48114195466041565\n",
      "epoch: 1 step: 550, loss is 0.4865517020225525\n",
      "epoch: 1 step: 551, loss is 0.47003018856048584\n",
      "epoch: 1 step: 552, loss is 0.49250054359436035\n",
      "epoch: 1 step: 553, loss is 0.4138850271701813\n",
      "epoch: 1 step: 554, loss is 0.4121023118495941\n",
      "epoch: 1 step: 555, loss is 0.35478800535202026\n",
      "epoch: 1 step: 556, loss is 0.4833771586418152\n",
      "epoch: 1 step: 557, loss is 0.3356236219406128\n",
      "epoch: 1 step: 558, loss is 0.6613214015960693\n",
      "epoch: 1 step: 559, loss is 0.4101417064666748\n",
      "epoch: 1 step: 560, loss is 0.3918551206588745\n",
      "epoch: 1 step: 561, loss is 0.5401033759117126\n",
      "epoch: 1 step: 562, loss is 0.7263728976249695\n",
      "epoch: 1 step: 563, loss is 0.3581995368003845\n",
      "epoch: 1 step: 564, loss is 0.1997799277305603\n",
      "epoch: 1 step: 565, loss is 0.5916203260421753\n",
      "epoch: 1 step: 566, loss is 0.34640613198280334\n",
      "epoch: 1 step: 567, loss is 0.752421498298645\n",
      "epoch: 1 step: 568, loss is 0.36293938755989075\n",
      "epoch: 1 step: 569, loss is 0.5680025219917297\n",
      "epoch: 1 step: 570, loss is 0.4579368829727173\n",
      "epoch: 1 step: 571, loss is 0.3756837844848633\n",
      "epoch: 1 step: 572, loss is 0.4075128734111786\n",
      "epoch: 1 step: 573, loss is 0.35611259937286377\n",
      "epoch: 1 step: 574, loss is 0.460726797580719\n",
      "epoch: 1 step: 575, loss is 0.5723863840103149\n",
      "epoch: 1 step: 576, loss is 0.3983669877052307\n",
      "epoch: 1 step: 577, loss is 0.3245503306388855\n",
      "epoch: 1 step: 578, loss is 0.359374463558197\n",
      "epoch: 1 step: 579, loss is 0.3669155240058899\n",
      "epoch: 1 step: 580, loss is 0.31178799271583557\n",
      "epoch: 1 step: 581, loss is 0.5205817222595215\n",
      "epoch: 1 step: 582, loss is 0.3397566080093384\n",
      "epoch: 1 step: 583, loss is 0.567996621131897\n",
      "epoch: 1 step: 584, loss is 0.33701926469802856\n",
      "epoch: 1 step: 585, loss is 0.28033339977264404\n",
      "epoch: 1 step: 586, loss is 0.3705178499221802\n",
      "epoch: 1 step: 587, loss is 0.3889586329460144\n",
      "epoch: 1 step: 588, loss is 0.32872000336647034\n",
      "epoch: 1 step: 589, loss is 0.4388773739337921\n",
      "epoch: 1 step: 590, loss is 0.4792254567146301\n",
      "epoch: 1 step: 591, loss is 0.2562134265899658\n",
      "epoch: 1 step: 592, loss is 0.4595877230167389\n",
      "epoch: 1 step: 593, loss is 0.3218314051628113\n",
      "epoch: 1 step: 594, loss is 0.3230319917201996\n",
      "epoch: 1 step: 595, loss is 0.44387465715408325\n",
      "epoch: 1 step: 596, loss is 0.3093089461326599\n",
      "epoch: 1 step: 597, loss is 0.3478618860244751\n",
      "epoch: 1 step: 598, loss is 0.3700157403945923\n",
      "epoch: 1 step: 599, loss is 0.34767699241638184\n",
      "epoch: 1 step: 600, loss is 0.5187023878097534\n",
      "epoch: 1 step: 601, loss is 0.4689469337463379\n",
      "epoch: 1 step: 602, loss is 0.29361623525619507\n",
      "epoch: 1 step: 603, loss is 0.36271047592163086\n",
      "epoch: 1 step: 604, loss is 0.2816627621650696\n",
      "epoch: 1 step: 605, loss is 0.36688095331192017\n",
      "epoch: 1 step: 606, loss is 0.3618769645690918\n",
      "epoch: 1 step: 607, loss is 0.3998098075389862\n",
      "epoch: 1 step: 608, loss is 0.3382966220378876\n",
      "epoch: 1 step: 609, loss is 0.5000434517860413\n",
      "epoch: 1 step: 610, loss is 0.20541083812713623\n",
      "epoch: 1 step: 611, loss is 0.5302137136459351\n",
      "epoch: 1 step: 612, loss is 0.41307899355888367\n",
      "epoch: 1 step: 613, loss is 0.3238832950592041\n",
      "epoch: 1 step: 614, loss is 0.4435787498950958\n",
      "epoch: 1 step: 615, loss is 0.30953916907310486\n",
      "epoch: 1 step: 616, loss is 0.43812888860702515\n",
      "epoch: 1 step: 617, loss is 0.3982398808002472\n",
      "epoch: 1 step: 618, loss is 0.6206796169281006\n",
      "epoch: 1 step: 619, loss is 0.2533615231513977\n",
      "epoch: 1 step: 620, loss is 0.5786900520324707\n",
      "epoch: 1 step: 621, loss is 0.40890002250671387\n",
      "epoch: 1 step: 622, loss is 0.38527822494506836\n",
      "epoch: 1 step: 623, loss is 0.3717198967933655\n",
      "epoch: 1 step: 624, loss is 0.3845939040184021\n",
      "epoch: 1 step: 625, loss is 0.5598427057266235\n",
      "epoch: 1 step: 626, loss is 0.3073650002479553\n",
      "epoch: 1 step: 627, loss is 0.4677172005176544\n",
      "epoch: 1 step: 628, loss is 0.38635823130607605\n",
      "epoch: 1 step: 629, loss is 0.4125900864601135\n",
      "epoch: 1 step: 630, loss is 0.33024275302886963\n",
      "epoch: 1 step: 631, loss is 0.48956426978111267\n",
      "epoch: 1 step: 632, loss is 0.46732306480407715\n",
      "epoch: 1 step: 633, loss is 0.3898332118988037\n",
      "epoch: 1 step: 634, loss is 0.22339871525764465\n",
      "epoch: 1 step: 635, loss is 0.4181016683578491\n",
      "epoch: 1 step: 636, loss is 0.27635711431503296\n",
      "epoch: 1 step: 637, loss is 0.3839227557182312\n",
      "epoch: 1 step: 638, loss is 0.315857470035553\n",
      "epoch: 1 step: 639, loss is 0.38809865713119507\n",
      "epoch: 1 step: 640, loss is 0.3269687294960022\n",
      "epoch: 1 step: 641, loss is 0.32475191354751587\n",
      "epoch: 1 step: 642, loss is 0.23977947235107422\n",
      "epoch: 1 step: 643, loss is 0.46852603554725647\n",
      "epoch: 1 step: 644, loss is 0.38682910799980164\n",
      "epoch: 1 step: 645, loss is 0.44532811641693115\n",
      "epoch: 1 step: 646, loss is 0.44654545187950134\n",
      "epoch: 1 step: 647, loss is 0.3553309440612793\n",
      "epoch: 1 step: 648, loss is 0.494667112827301\n",
      "epoch: 1 step: 649, loss is 0.295884907245636\n",
      "epoch: 1 step: 650, loss is 0.3954770267009735\n",
      "epoch: 1 step: 651, loss is 0.3931078314781189\n",
      "epoch: 1 step: 652, loss is 0.49718472361564636\n",
      "epoch: 1 step: 653, loss is 0.22393859922885895\n",
      "epoch: 1 step: 654, loss is 0.5549524426460266\n",
      "epoch: 1 step: 655, loss is 0.3177224099636078\n",
      "epoch: 1 step: 656, loss is 0.26944226026535034\n",
      "epoch: 1 step: 657, loss is 0.38887226581573486\n",
      "epoch: 1 step: 658, loss is 0.47084686160087585\n",
      "epoch: 1 step: 659, loss is 0.4420306980609894\n",
      "epoch: 1 step: 660, loss is 0.4571572542190552\n",
      "epoch: 1 step: 661, loss is 0.354516863822937\n",
      "epoch: 1 step: 662, loss is 0.4273078441619873\n",
      "epoch: 1 step: 663, loss is 0.23674631118774414\n",
      "epoch: 1 step: 664, loss is 0.4776531457901001\n",
      "epoch: 1 step: 665, loss is 0.49086466431617737\n",
      "epoch: 1 step: 666, loss is 0.3556400537490845\n",
      "epoch: 1 step: 667, loss is 0.3274182677268982\n",
      "epoch: 1 step: 668, loss is 0.6022003293037415\n",
      "epoch: 1 step: 669, loss is 0.5424264073371887\n",
      "epoch: 1 step: 670, loss is 0.4927561581134796\n",
      "epoch: 1 step: 671, loss is 0.44098180532455444\n",
      "epoch: 1 step: 672, loss is 0.33418506383895874\n",
      "epoch: 1 step: 673, loss is 0.41152265667915344\n",
      "epoch: 1 step: 674, loss is 0.26569730043411255\n",
      "epoch: 1 step: 675, loss is 0.3739159107208252\n",
      "epoch: 1 step: 676, loss is 0.4175180494785309\n",
      "epoch: 1 step: 677, loss is 0.31519222259521484\n",
      "epoch: 1 step: 678, loss is 0.5895054936408997\n",
      "epoch: 1 step: 679, loss is 0.22704532742500305\n",
      "epoch: 1 step: 680, loss is 0.39324304461479187\n",
      "epoch: 1 step: 681, loss is 0.30713480710983276\n",
      "epoch: 1 step: 682, loss is 0.35357534885406494\n",
      "epoch: 1 step: 683, loss is 0.35932546854019165\n",
      "epoch: 1 step: 684, loss is 0.25374841690063477\n",
      "epoch: 1 step: 685, loss is 0.24003157019615173\n",
      "epoch: 1 step: 686, loss is 0.2826426923274994\n",
      "epoch: 1 step: 687, loss is 0.6127439737319946\n",
      "epoch: 1 step: 688, loss is 0.3271505832672119\n",
      "epoch: 1 step: 689, loss is 0.3337917923927307\n",
      "epoch: 1 step: 690, loss is 0.3833273649215698\n",
      "epoch: 1 step: 691, loss is 0.3294280171394348\n",
      "epoch: 1 step: 692, loss is 0.37964004278182983\n",
      "epoch: 1 step: 693, loss is 0.27305588126182556\n",
      "epoch: 1 step: 694, loss is 0.4017563462257385\n",
      "epoch: 1 step: 695, loss is 0.2505270540714264\n",
      "epoch: 1 step: 696, loss is 0.3152664005756378\n",
      "epoch: 1 step: 697, loss is 0.3574230670928955\n",
      "epoch: 1 step: 698, loss is 0.43304237723350525\n",
      "epoch: 1 step: 699, loss is 0.4533223807811737\n",
      "epoch: 1 step: 700, loss is 0.21127668023109436\n",
      "epoch: 1 step: 701, loss is 0.46214669942855835\n",
      "epoch: 1 step: 702, loss is 0.5326727628707886\n",
      "epoch: 1 step: 703, loss is 0.3138805031776428\n",
      "epoch: 1 step: 704, loss is 0.3833184540271759\n",
      "epoch: 1 step: 705, loss is 0.39073124527931213\n",
      "epoch: 1 step: 706, loss is 0.36168959736824036\n",
      "epoch: 1 step: 707, loss is 0.4743329584598541\n",
      "epoch: 1 step: 708, loss is 0.3556939959526062\n",
      "epoch: 1 step: 709, loss is 0.3215329647064209\n",
      "epoch: 1 step: 710, loss is 0.32323697209358215\n",
      "epoch: 1 step: 711, loss is 0.45505842566490173\n",
      "epoch: 1 step: 712, loss is 0.29965299367904663\n",
      "epoch: 1 step: 713, loss is 0.2865166664123535\n",
      "epoch: 1 step: 714, loss is 0.3545004725456238\n",
      "epoch: 1 step: 715, loss is 0.3790287971496582\n",
      "epoch: 1 step: 716, loss is 0.48204827308654785\n",
      "epoch: 1 step: 717, loss is 0.4644797742366791\n",
      "epoch: 1 step: 718, loss is 0.2162783443927765\n",
      "epoch: 1 step: 719, loss is 0.3179630637168884\n",
      "epoch: 1 step: 720, loss is 0.3870736062526703\n",
      "epoch: 1 step: 721, loss is 0.4366408884525299\n",
      "epoch: 1 step: 722, loss is 0.41760504245758057\n",
      "epoch: 1 step: 723, loss is 0.31152835488319397\n",
      "epoch: 1 step: 724, loss is 0.4225426912307739\n",
      "epoch: 1 step: 725, loss is 0.2619527578353882\n",
      "epoch: 1 step: 726, loss is 0.47867533564567566\n",
      "epoch: 1 step: 727, loss is 0.510353147983551\n",
      "epoch: 1 step: 728, loss is 0.4583732485771179\n",
      "epoch: 1 step: 729, loss is 0.3399710953235626\n",
      "epoch: 1 step: 730, loss is 0.23518922924995422\n",
      "epoch: 1 step: 731, loss is 0.4442752003669739\n",
      "epoch: 1 step: 732, loss is 0.23304130136966705\n",
      "epoch: 1 step: 733, loss is 0.5677669048309326\n",
      "epoch: 1 step: 734, loss is 0.3456081449985504\n",
      "epoch: 1 step: 735, loss is 0.46913227438926697\n",
      "epoch: 1 step: 736, loss is 0.37776780128479004\n",
      "epoch: 1 step: 737, loss is 0.22284409403800964\n",
      "epoch: 1 step: 738, loss is 0.45839670300483704\n",
      "epoch: 1 step: 739, loss is 0.36356890201568604\n",
      "epoch: 1 step: 740, loss is 0.4542989134788513\n",
      "epoch: 1 step: 741, loss is 0.3335847854614258\n",
      "epoch: 1 step: 742, loss is 0.3422998785972595\n",
      "epoch: 1 step: 743, loss is 0.2427968531847\n",
      "epoch: 1 step: 744, loss is 0.34251558780670166\n",
      "epoch: 1 step: 745, loss is 0.26738935708999634\n",
      "epoch: 1 step: 746, loss is 0.30548709630966187\n",
      "epoch: 1 step: 747, loss is 0.24192771315574646\n",
      "epoch: 1 step: 748, loss is 0.34183841943740845\n",
      "epoch: 1 step: 749, loss is 0.6076580882072449\n",
      "epoch: 1 step: 750, loss is 0.46542513370513916\n",
      "epoch: 1 step: 751, loss is 0.386749267578125\n",
      "epoch: 1 step: 752, loss is 0.48260921239852905\n",
      "epoch: 1 step: 753, loss is 0.4590204954147339\n",
      "epoch: 1 step: 754, loss is 0.4995669424533844\n",
      "epoch: 1 step: 755, loss is 0.3345398008823395\n",
      "epoch: 1 step: 756, loss is 0.4199502468109131\n",
      "epoch: 1 step: 757, loss is 0.4445301294326782\n",
      "epoch: 1 step: 758, loss is 0.34361663460731506\n",
      "epoch: 1 step: 759, loss is 0.3827950358390808\n",
      "epoch: 1 step: 760, loss is 0.4783245325088501\n",
      "epoch: 1 step: 761, loss is 0.37092626094818115\n",
      "epoch: 1 step: 762, loss is 0.322503924369812\n",
      "epoch: 1 step: 763, loss is 0.24403463304042816\n",
      "epoch: 1 step: 764, loss is 0.2890717685222626\n",
      "epoch: 1 step: 765, loss is 0.35401225090026855\n",
      "epoch: 1 step: 766, loss is 0.28937357664108276\n",
      "epoch: 1 step: 767, loss is 0.7190581560134888\n",
      "epoch: 1 step: 768, loss is 0.6995399594306946\n",
      "epoch: 1 step: 769, loss is 0.3786044716835022\n",
      "epoch: 1 step: 770, loss is 0.4995307922363281\n",
      "epoch: 1 step: 771, loss is 0.3654606342315674\n",
      "epoch: 1 step: 772, loss is 0.40338969230651855\n",
      "epoch: 1 step: 773, loss is 0.32328590750694275\n",
      "epoch: 1 step: 774, loss is 0.4526937007904053\n",
      "epoch: 1 step: 775, loss is 0.2493266463279724\n",
      "epoch: 1 step: 776, loss is 0.40522971749305725\n",
      "epoch: 1 step: 777, loss is 0.38582783937454224\n",
      "epoch: 1 step: 778, loss is 0.36286991834640503\n",
      "epoch: 1 step: 779, loss is 0.3525017201900482\n",
      "epoch: 1 step: 780, loss is 0.5765506029129028\n",
      "epoch: 1 step: 781, loss is 0.5207312107086182\n",
      "epoch: 1 step: 782, loss is 0.48784929513931274\n",
      "epoch: 1 step: 783, loss is 0.40823686122894287\n",
      "epoch: 1 step: 784, loss is 0.42974215745925903\n",
      "epoch: 1 step: 785, loss is 0.4497492015361786\n",
      "epoch: 1 step: 786, loss is 0.4943333864212036\n",
      "epoch: 1 step: 787, loss is 0.3586023151874542\n",
      "epoch: 1 step: 788, loss is 0.15966123342514038\n",
      "epoch: 1 step: 789, loss is 0.3958335816860199\n",
      "epoch: 1 step: 790, loss is 0.3164244294166565\n",
      "epoch: 1 step: 791, loss is 0.24027200043201447\n",
      "epoch: 1 step: 792, loss is 0.25625649094581604\n",
      "epoch: 1 step: 793, loss is 0.5307008624076843\n",
      "epoch: 1 step: 794, loss is 0.4352930188179016\n",
      "epoch: 1 step: 795, loss is 0.3216707706451416\n",
      "epoch: 1 step: 796, loss is 0.21586662530899048\n",
      "epoch: 1 step: 797, loss is 0.3843300938606262\n",
      "epoch: 1 step: 798, loss is 0.3130575716495514\n",
      "epoch: 1 step: 799, loss is 0.29155993461608887\n",
      "epoch: 1 step: 800, loss is 0.37844234704971313\n",
      "epoch: 1 step: 801, loss is 0.29309946298599243\n",
      "epoch: 1 step: 802, loss is 0.3056408762931824\n",
      "epoch: 1 step: 803, loss is 0.30055081844329834\n",
      "epoch: 1 step: 804, loss is 0.33939695358276367\n",
      "epoch: 1 step: 805, loss is 0.4072860777378082\n",
      "epoch: 1 step: 806, loss is 0.5212588310241699\n",
      "epoch: 1 step: 807, loss is 0.3077158033847809\n",
      "epoch: 1 step: 808, loss is 0.30692774057388306\n",
      "epoch: 1 step: 809, loss is 0.7008078098297119\n",
      "epoch: 1 step: 810, loss is 0.31216180324554443\n",
      "epoch: 1 step: 811, loss is 0.285902738571167\n",
      "epoch: 1 step: 812, loss is 0.6253623962402344\n",
      "epoch: 1 step: 813, loss is 0.351028174161911\n",
      "epoch: 1 step: 814, loss is 0.35581764578819275\n",
      "epoch: 1 step: 815, loss is 0.3703996539115906\n",
      "epoch: 1 step: 816, loss is 0.5088626742362976\n",
      "epoch: 1 step: 817, loss is 0.19197088479995728\n",
      "epoch: 1 step: 818, loss is 0.30271926522254944\n",
      "epoch: 1 step: 819, loss is 0.282598614692688\n",
      "epoch: 1 step: 820, loss is 0.2763574719429016\n",
      "epoch: 1 step: 821, loss is 0.30912280082702637\n",
      "epoch: 1 step: 822, loss is 0.31516292691230774\n",
      "epoch: 1 step: 823, loss is 0.4190334975719452\n",
      "epoch: 1 step: 824, loss is 0.26998233795166016\n",
      "epoch: 1 step: 825, loss is 0.24352799355983734\n",
      "epoch: 1 step: 826, loss is 0.27748748660087585\n",
      "epoch: 1 step: 827, loss is 0.16665732860565186\n",
      "epoch: 1 step: 828, loss is 0.21061742305755615\n",
      "epoch: 1 step: 829, loss is 0.23504535853862762\n",
      "epoch: 1 step: 830, loss is 0.25180429220199585\n",
      "epoch: 1 step: 831, loss is 0.3245019316673279\n",
      "epoch: 1 step: 832, loss is 0.28125739097595215\n",
      "epoch: 1 step: 833, loss is 0.23784972727298737\n",
      "epoch: 1 step: 834, loss is 0.3839741349220276\n",
      "epoch: 1 step: 835, loss is 0.34510597586631775\n",
      "epoch: 1 step: 836, loss is 0.2311670184135437\n",
      "epoch: 1 step: 837, loss is 0.2819788455963135\n",
      "epoch: 1 step: 838, loss is 0.21306878328323364\n",
      "epoch: 1 step: 839, loss is 0.32594043016433716\n",
      "epoch: 1 step: 840, loss is 0.26884958148002625\n",
      "epoch: 1 step: 841, loss is 0.36284440755844116\n",
      "epoch: 1 step: 842, loss is 0.2723864018917084\n",
      "epoch: 1 step: 843, loss is 0.5058678984642029\n",
      "epoch: 1 step: 844, loss is 0.3841092586517334\n",
      "epoch: 1 step: 845, loss is 0.47371116280555725\n",
      "epoch: 1 step: 846, loss is 0.31817901134490967\n",
      "epoch: 1 step: 847, loss is 0.39803433418273926\n",
      "epoch: 1 step: 848, loss is 0.5032879114151001\n",
      "epoch: 1 step: 849, loss is 0.3074275255203247\n",
      "epoch: 1 step: 850, loss is 0.22910048067569733\n",
      "epoch: 1 step: 851, loss is 0.23062463104724884\n",
      "epoch: 1 step: 852, loss is 0.44618940353393555\n",
      "epoch: 1 step: 853, loss is 0.35848531126976013\n",
      "epoch: 1 step: 854, loss is 0.26793205738067627\n",
      "epoch: 1 step: 855, loss is 0.2958778738975525\n",
      "epoch: 1 step: 856, loss is 0.3934178352355957\n",
      "epoch: 1 step: 857, loss is 0.34045037627220154\n",
      "epoch: 1 step: 858, loss is 0.312545508146286\n",
      "epoch: 1 step: 859, loss is 0.2772776186466217\n",
      "epoch: 1 step: 860, loss is 0.3344273567199707\n",
      "epoch: 1 step: 861, loss is 0.28090769052505493\n",
      "epoch: 1 step: 862, loss is 0.2472892701625824\n",
      "epoch: 1 step: 863, loss is 0.2984403371810913\n",
      "epoch: 1 step: 864, loss is 0.542104184627533\n",
      "epoch: 1 step: 865, loss is 0.2921874523162842\n",
      "epoch: 1 step: 866, loss is 0.32961541414260864\n",
      "epoch: 1 step: 867, loss is 0.40143945813179016\n",
      "epoch: 1 step: 868, loss is 0.3606746196746826\n",
      "epoch: 1 step: 869, loss is 0.4589611291885376\n",
      "epoch: 1 step: 870, loss is 0.31742969155311584\n",
      "epoch: 1 step: 871, loss is 0.4949936270713806\n",
      "epoch: 1 step: 872, loss is 0.3802626132965088\n",
      "epoch: 1 step: 873, loss is 0.33108755946159363\n",
      "epoch: 1 step: 874, loss is 0.36623936891555786\n",
      "epoch: 1 step: 875, loss is 0.48839887976646423\n",
      "epoch: 1 step: 876, loss is 0.3878674805164337\n",
      "epoch: 1 step: 877, loss is 0.3517484664916992\n",
      "epoch: 1 step: 878, loss is 0.3192176818847656\n",
      "epoch: 1 step: 879, loss is 0.4537021815776825\n",
      "epoch: 1 step: 880, loss is 0.3804585635662079\n",
      "epoch: 1 step: 881, loss is 0.4491332769393921\n",
      "epoch: 1 step: 882, loss is 0.3379858732223511\n",
      "epoch: 1 step: 883, loss is 0.3550599217414856\n",
      "epoch: 1 step: 884, loss is 0.26417526602745056\n",
      "epoch: 1 step: 885, loss is 0.38340672850608826\n",
      "epoch: 1 step: 886, loss is 0.3559984564781189\n",
      "epoch: 1 step: 887, loss is 0.2887537181377411\n",
      "epoch: 1 step: 888, loss is 0.5393667221069336\n",
      "epoch: 1 step: 889, loss is 0.2996947169303894\n",
      "epoch: 1 step: 890, loss is 0.44066348671913147\n",
      "epoch: 1 step: 891, loss is 0.3919786214828491\n",
      "epoch: 1 step: 892, loss is 0.42146164178848267\n",
      "epoch: 1 step: 893, loss is 0.3163021504878998\n",
      "epoch: 1 step: 894, loss is 0.5262315273284912\n",
      "epoch: 1 step: 895, loss is 0.38599100708961487\n",
      "epoch: 1 step: 896, loss is 0.4794268012046814\n",
      "epoch: 1 step: 897, loss is 0.4221128821372986\n",
      "epoch: 1 step: 898, loss is 0.5226901769638062\n",
      "epoch: 1 step: 899, loss is 0.3780575096607208\n",
      "epoch: 1 step: 900, loss is 0.47753971815109253\n",
      "epoch: 1 step: 901, loss is 0.18572424352169037\n",
      "epoch: 1 step: 902, loss is 0.4040651023387909\n",
      "epoch: 1 step: 903, loss is 0.2657337188720703\n",
      "epoch: 1 step: 904, loss is 0.22743284702301025\n",
      "epoch: 1 step: 905, loss is 0.33037269115448\n",
      "epoch: 1 step: 906, loss is 0.27922576665878296\n",
      "epoch: 1 step: 907, loss is 0.3162088394165039\n",
      "epoch: 1 step: 908, loss is 0.3610192537307739\n",
      "epoch: 1 step: 909, loss is 0.4230451285839081\n",
      "epoch: 1 step: 910, loss is 0.1979007124900818\n",
      "epoch: 1 step: 911, loss is 0.49447476863861084\n",
      "epoch: 1 step: 912, loss is 0.4553371071815491\n",
      "epoch: 1 step: 913, loss is 0.351149320602417\n",
      "epoch: 1 step: 914, loss is 0.42976123094558716\n",
      "epoch: 1 step: 915, loss is 0.4830133020877838\n",
      "epoch: 1 step: 916, loss is 0.25690120458602905\n",
      "epoch: 1 step: 917, loss is 0.37312015891075134\n",
      "epoch: 1 step: 918, loss is 0.24802827835083008\n",
      "epoch: 1 step: 919, loss is 0.36430081725120544\n",
      "epoch: 1 step: 920, loss is 0.25975173711776733\n",
      "epoch: 1 step: 921, loss is 0.39405035972595215\n",
      "epoch: 1 step: 922, loss is 0.278393030166626\n",
      "epoch: 1 step: 923, loss is 0.2242737114429474\n",
      "epoch: 1 step: 924, loss is 0.3459198474884033\n",
      "epoch: 1 step: 925, loss is 0.19644251465797424\n",
      "epoch: 1 step: 926, loss is 0.5378624200820923\n",
      "epoch: 1 step: 927, loss is 0.2758992612361908\n",
      "epoch: 1 step: 928, loss is 0.37553870677948\n",
      "epoch: 1 step: 929, loss is 0.3261694312095642\n",
      "epoch: 1 step: 930, loss is 0.24903619289398193\n",
      "epoch: 1 step: 931, loss is 0.3721320331096649\n",
      "epoch: 1 step: 932, loss is 0.3344995081424713\n",
      "epoch: 1 step: 933, loss is 0.2405892312526703\n",
      "epoch: 1 step: 934, loss is 0.36465156078338623\n",
      "epoch: 1 step: 935, loss is 0.3192489743232727\n",
      "epoch: 1 step: 936, loss is 0.2927837371826172\n",
      "epoch: 1 step: 937, loss is 0.29269373416900635\n",
      "epoch: 2 step: 1, loss is 0.2805790305137634\n",
      "epoch: 2 step: 2, loss is 0.21704241633415222\n",
      "epoch: 2 step: 3, loss is 0.5097750425338745\n",
      "epoch: 2 step: 4, loss is 0.34432536363601685\n",
      "epoch: 2 step: 5, loss is 0.20382171869277954\n",
      "epoch: 2 step: 6, loss is 0.4813341796398163\n",
      "epoch: 2 step: 7, loss is 0.3204309344291687\n",
      "epoch: 2 step: 8, loss is 0.4290596842765808\n",
      "epoch: 2 step: 9, loss is 0.24407748878002167\n",
      "epoch: 2 step: 10, loss is 0.22410628199577332\n",
      "epoch: 2 step: 11, loss is 0.32668817043304443\n",
      "epoch: 2 step: 12, loss is 0.2109343260526657\n",
      "epoch: 2 step: 13, loss is 0.4525158107280731\n",
      "epoch: 2 step: 14, loss is 0.3980523943901062\n",
      "epoch: 2 step: 15, loss is 0.40539848804473877\n",
      "epoch: 2 step: 16, loss is 0.302886426448822\n",
      "epoch: 2 step: 17, loss is 0.24782472848892212\n",
      "epoch: 2 step: 18, loss is 0.3646391034126282\n",
      "epoch: 2 step: 19, loss is 0.31300896406173706\n",
      "epoch: 2 step: 20, loss is 0.49064862728118896\n",
      "epoch: 2 step: 21, loss is 0.1801198273897171\n",
      "epoch: 2 step: 22, loss is 0.15067362785339355\n",
      "epoch: 2 step: 23, loss is 0.3550046682357788\n",
      "epoch: 2 step: 24, loss is 0.22555704414844513\n",
      "epoch: 2 step: 25, loss is 0.2539668083190918\n",
      "epoch: 2 step: 26, loss is 0.37531206011772156\n",
      "epoch: 2 step: 27, loss is 0.3997650146484375\n",
      "epoch: 2 step: 28, loss is 0.3279407322406769\n",
      "epoch: 2 step: 29, loss is 0.2901783585548401\n",
      "epoch: 2 step: 30, loss is 0.21363773941993713\n",
      "epoch: 2 step: 31, loss is 0.3144058883190155\n",
      "epoch: 2 step: 32, loss is 0.358503520488739\n",
      "epoch: 2 step: 33, loss is 0.2526590824127197\n",
      "epoch: 2 step: 34, loss is 0.37897974252700806\n",
      "epoch: 2 step: 35, loss is 0.34106919169425964\n",
      "epoch: 2 step: 36, loss is 0.19731184840202332\n",
      "epoch: 2 step: 37, loss is 0.2706361711025238\n",
      "epoch: 2 step: 38, loss is 0.2794826328754425\n",
      "epoch: 2 step: 39, loss is 0.23032066226005554\n",
      "epoch: 2 step: 40, loss is 0.4255272150039673\n",
      "epoch: 2 step: 41, loss is 0.18437594175338745\n",
      "epoch: 2 step: 42, loss is 0.3360369801521301\n",
      "epoch: 2 step: 43, loss is 0.4916653037071228\n",
      "epoch: 2 step: 44, loss is 0.5871944427490234\n",
      "epoch: 2 step: 45, loss is 0.24996493756771088\n",
      "epoch: 2 step: 46, loss is 0.38235345482826233\n",
      "epoch: 2 step: 47, loss is 0.2478499859571457\n",
      "epoch: 2 step: 48, loss is 0.36882108449935913\n",
      "epoch: 2 step: 49, loss is 0.38728854060173035\n",
      "epoch: 2 step: 50, loss is 0.29712241888046265\n",
      "epoch: 2 step: 51, loss is 0.3271530866622925\n",
      "epoch: 2 step: 52, loss is 0.3038995862007141\n",
      "epoch: 2 step: 53, loss is 0.270368754863739\n",
      "epoch: 2 step: 54, loss is 0.12019931524991989\n",
      "epoch: 2 step: 55, loss is 0.5911170840263367\n",
      "epoch: 2 step: 56, loss is 0.3377428650856018\n",
      "epoch: 2 step: 57, loss is 0.25331375002861023\n",
      "epoch: 2 step: 58, loss is 0.22888875007629395\n",
      "epoch: 2 step: 59, loss is 0.25940126180648804\n",
      "epoch: 2 step: 60, loss is 0.33168384432792664\n",
      "epoch: 2 step: 61, loss is 0.345112681388855\n",
      "epoch: 2 step: 62, loss is 0.286515474319458\n",
      "epoch: 2 step: 63, loss is 0.3232489228248596\n",
      "epoch: 2 step: 64, loss is 0.28589463233947754\n",
      "epoch: 2 step: 65, loss is 0.3156048357486725\n",
      "epoch: 2 step: 66, loss is 0.34998154640197754\n",
      "epoch: 2 step: 67, loss is 0.380004346370697\n",
      "epoch: 2 step: 68, loss is 0.450451135635376\n",
      "epoch: 2 step: 69, loss is 0.2783058285713196\n",
      "epoch: 2 step: 70, loss is 0.40321221947669983\n",
      "epoch: 2 step: 71, loss is 0.4387126564979553\n",
      "epoch: 2 step: 72, loss is 0.3545975089073181\n",
      "epoch: 2 step: 73, loss is 0.23857159912586212\n",
      "epoch: 2 step: 74, loss is 0.2542644441127777\n",
      "epoch: 2 step: 75, loss is 0.2700319290161133\n",
      "epoch: 2 step: 76, loss is 0.3708738684654236\n",
      "epoch: 2 step: 77, loss is 0.23249578475952148\n",
      "epoch: 2 step: 78, loss is 0.27474480867385864\n",
      "epoch: 2 step: 79, loss is 0.4687004089355469\n",
      "epoch: 2 step: 80, loss is 0.3921556770801544\n",
      "epoch: 2 step: 81, loss is 0.15106002986431122\n",
      "epoch: 2 step: 82, loss is 0.2888656258583069\n",
      "epoch: 2 step: 83, loss is 0.3024047017097473\n",
      "epoch: 2 step: 84, loss is 0.4495842754840851\n",
      "epoch: 2 step: 85, loss is 0.28967002034187317\n",
      "epoch: 2 step: 86, loss is 0.2899434566497803\n",
      "epoch: 2 step: 87, loss is 0.3017202913761139\n",
      "epoch: 2 step: 88, loss is 0.3744962215423584\n",
      "epoch: 2 step: 89, loss is 0.3485955595970154\n",
      "epoch: 2 step: 90, loss is 0.24502043426036835\n",
      "epoch: 2 step: 91, loss is 0.2847112715244293\n",
      "epoch: 2 step: 92, loss is 0.3445786237716675\n",
      "epoch: 2 step: 93, loss is 0.450161874294281\n",
      "epoch: 2 step: 94, loss is 0.21877741813659668\n",
      "epoch: 2 step: 95, loss is 0.23709937930107117\n",
      "epoch: 2 step: 96, loss is 0.4619644284248352\n",
      "epoch: 2 step: 97, loss is 0.26493868231773376\n",
      "epoch: 2 step: 98, loss is 0.41008853912353516\n",
      "epoch: 2 step: 99, loss is 0.5473924279212952\n",
      "epoch: 2 step: 100, loss is 0.35620880126953125\n",
      "epoch: 2 step: 101, loss is 0.11648166179656982\n",
      "epoch: 2 step: 102, loss is 0.47516459226608276\n",
      "epoch: 2 step: 103, loss is 0.5136572122573853\n",
      "epoch: 2 step: 104, loss is 0.3747994899749756\n",
      "epoch: 2 step: 105, loss is 0.3933607041835785\n",
      "epoch: 2 step: 106, loss is 0.32088568806648254\n",
      "epoch: 2 step: 107, loss is 0.2131941318511963\n",
      "epoch: 2 step: 108, loss is 0.35062843561172485\n",
      "epoch: 2 step: 109, loss is 0.33823704719543457\n",
      "epoch: 2 step: 110, loss is 0.3339083790779114\n",
      "epoch: 2 step: 111, loss is 0.22093939781188965\n",
      "epoch: 2 step: 112, loss is 0.5081250667572021\n",
      "epoch: 2 step: 113, loss is 0.19720420241355896\n",
      "epoch: 2 step: 114, loss is 0.27046963572502136\n",
      "epoch: 2 step: 115, loss is 0.2847134470939636\n",
      "epoch: 2 step: 116, loss is 0.23008140921592712\n",
      "epoch: 2 step: 117, loss is 0.3026176393032074\n",
      "epoch: 2 step: 118, loss is 0.35074150562286377\n",
      "epoch: 2 step: 119, loss is 0.22020956873893738\n",
      "epoch: 2 step: 120, loss is 0.23036983609199524\n",
      "epoch: 2 step: 121, loss is 0.23962904512882233\n",
      "epoch: 2 step: 122, loss is 0.45262497663497925\n",
      "epoch: 2 step: 123, loss is 0.5324537754058838\n",
      "epoch: 2 step: 124, loss is 0.41135314106941223\n",
      "epoch: 2 step: 125, loss is 0.2585795819759369\n",
      "epoch: 2 step: 126, loss is 0.18607357144355774\n",
      "epoch: 2 step: 127, loss is 0.30680331587791443\n",
      "epoch: 2 step: 128, loss is 0.24620403349399567\n",
      "epoch: 2 step: 129, loss is 0.18789978325366974\n",
      "epoch: 2 step: 130, loss is 0.4482087194919586\n",
      "epoch: 2 step: 131, loss is 0.2870119512081146\n",
      "epoch: 2 step: 132, loss is 0.3425232768058777\n",
      "epoch: 2 step: 133, loss is 0.2603590488433838\n",
      "epoch: 2 step: 134, loss is 0.19271865487098694\n",
      "epoch: 2 step: 135, loss is 0.42820245027542114\n",
      "epoch: 2 step: 136, loss is 0.5626425743103027\n",
      "epoch: 2 step: 137, loss is 0.2771499752998352\n",
      "epoch: 2 step: 138, loss is 0.22913295030593872\n",
      "epoch: 2 step: 139, loss is 0.323959082365036\n",
      "epoch: 2 step: 140, loss is 0.28781694173812866\n",
      "epoch: 2 step: 141, loss is 0.313232421875\n",
      "epoch: 2 step: 142, loss is 0.16419768333435059\n",
      "epoch: 2 step: 143, loss is 0.3333018124103546\n",
      "epoch: 2 step: 144, loss is 0.34983059763908386\n",
      "epoch: 2 step: 145, loss is 0.26626408100128174\n",
      "epoch: 2 step: 146, loss is 0.2753399610519409\n",
      "epoch: 2 step: 147, loss is 0.42658549547195435\n",
      "epoch: 2 step: 148, loss is 0.3159503936767578\n",
      "epoch: 2 step: 149, loss is 0.5549602508544922\n",
      "epoch: 2 step: 150, loss is 0.2860235273838043\n",
      "epoch: 2 step: 151, loss is 0.294569194316864\n",
      "epoch: 2 step: 152, loss is 0.2922893762588501\n",
      "epoch: 2 step: 153, loss is 0.421589732170105\n",
      "epoch: 2 step: 154, loss is 0.34637147188186646\n",
      "epoch: 2 step: 155, loss is 0.42911356687545776\n",
      "epoch: 2 step: 156, loss is 0.24147382378578186\n",
      "epoch: 2 step: 157, loss is 0.1385950744152069\n",
      "epoch: 2 step: 158, loss is 0.47795844078063965\n",
      "epoch: 2 step: 159, loss is 0.34740978479385376\n",
      "epoch: 2 step: 160, loss is 0.4088287055492401\n",
      "epoch: 2 step: 161, loss is 0.3439611792564392\n",
      "epoch: 2 step: 162, loss is 0.25138920545578003\n",
      "epoch: 2 step: 163, loss is 0.2135968953371048\n",
      "epoch: 2 step: 164, loss is 0.28929227590560913\n",
      "epoch: 2 step: 165, loss is 0.27363330125808716\n",
      "epoch: 2 step: 166, loss is 0.2491672784090042\n",
      "epoch: 2 step: 167, loss is 0.34399157762527466\n",
      "epoch: 2 step: 168, loss is 0.3252263367176056\n",
      "epoch: 2 step: 169, loss is 0.43370920419692993\n",
      "epoch: 2 step: 170, loss is 0.28056710958480835\n",
      "epoch: 2 step: 171, loss is 0.17133711278438568\n",
      "epoch: 2 step: 172, loss is 0.3499757945537567\n",
      "epoch: 2 step: 173, loss is 0.3583780527114868\n",
      "epoch: 2 step: 174, loss is 0.28066712617874146\n",
      "epoch: 2 step: 175, loss is 0.46669209003448486\n",
      "epoch: 2 step: 176, loss is 0.26583898067474365\n",
      "epoch: 2 step: 177, loss is 0.4714832901954651\n",
      "epoch: 2 step: 178, loss is 0.3590252995491028\n",
      "epoch: 2 step: 179, loss is 0.37501972913742065\n",
      "epoch: 2 step: 180, loss is 0.16588130593299866\n",
      "epoch: 2 step: 181, loss is 0.3443931043148041\n",
      "epoch: 2 step: 182, loss is 0.26288604736328125\n",
      "epoch: 2 step: 183, loss is 0.15284845232963562\n",
      "epoch: 2 step: 184, loss is 0.19676263630390167\n",
      "epoch: 2 step: 185, loss is 0.2069205939769745\n",
      "epoch: 2 step: 186, loss is 0.22518528997898102\n",
      "epoch: 2 step: 187, loss is 0.4562663733959198\n",
      "epoch: 2 step: 188, loss is 0.2801316976547241\n",
      "epoch: 2 step: 189, loss is 0.3147062659263611\n",
      "epoch: 2 step: 190, loss is 0.2598973214626312\n",
      "epoch: 2 step: 191, loss is 0.399996817111969\n",
      "epoch: 2 step: 192, loss is 0.24647700786590576\n",
      "epoch: 2 step: 193, loss is 0.39199891686439514\n",
      "epoch: 2 step: 194, loss is 0.2694242000579834\n",
      "epoch: 2 step: 195, loss is 0.19534537196159363\n",
      "epoch: 2 step: 196, loss is 0.3458617329597473\n",
      "epoch: 2 step: 197, loss is 0.22902385890483856\n",
      "epoch: 2 step: 198, loss is 0.21110868453979492\n",
      "epoch: 2 step: 199, loss is 0.2637428641319275\n",
      "epoch: 2 step: 200, loss is 0.3214894235134125\n",
      "epoch: 2 step: 201, loss is 0.2555989921092987\n",
      "epoch: 2 step: 202, loss is 0.245618537068367\n",
      "epoch: 2 step: 203, loss is 0.22733592987060547\n",
      "epoch: 2 step: 204, loss is 0.30343231558799744\n",
      "epoch: 2 step: 205, loss is 0.46206367015838623\n",
      "epoch: 2 step: 206, loss is 0.40019190311431885\n",
      "epoch: 2 step: 207, loss is 0.31082165241241455\n",
      "epoch: 2 step: 208, loss is 0.37697291374206543\n",
      "epoch: 2 step: 209, loss is 0.46418219804763794\n",
      "epoch: 2 step: 210, loss is 0.3546827435493469\n",
      "epoch: 2 step: 211, loss is 0.38395506143569946\n",
      "epoch: 2 step: 212, loss is 0.3685097098350525\n",
      "epoch: 2 step: 213, loss is 0.27302202582359314\n",
      "epoch: 2 step: 214, loss is 0.24267065525054932\n",
      "epoch: 2 step: 215, loss is 0.4454742670059204\n",
      "epoch: 2 step: 216, loss is 0.22828316688537598\n",
      "epoch: 2 step: 217, loss is 0.24798434972763062\n",
      "epoch: 2 step: 218, loss is 0.3910808563232422\n",
      "epoch: 2 step: 219, loss is 0.23215806484222412\n",
      "epoch: 2 step: 220, loss is 0.288119375705719\n",
      "epoch: 2 step: 221, loss is 0.1864774227142334\n",
      "epoch: 2 step: 222, loss is 0.24583688378334045\n",
      "epoch: 2 step: 223, loss is 0.2923435568809509\n",
      "epoch: 2 step: 224, loss is 0.35220223665237427\n",
      "epoch: 2 step: 225, loss is 0.2868649363517761\n",
      "epoch: 2 step: 226, loss is 0.27504077553749084\n",
      "epoch: 2 step: 227, loss is 0.3681411147117615\n",
      "epoch: 2 step: 228, loss is 0.24028995633125305\n",
      "epoch: 2 step: 229, loss is 0.2880138158798218\n",
      "epoch: 2 step: 230, loss is 0.47001197934150696\n",
      "epoch: 2 step: 231, loss is 0.2920222878456116\n",
      "epoch: 2 step: 232, loss is 0.2807893455028534\n",
      "epoch: 2 step: 233, loss is 0.4598703682422638\n",
      "epoch: 2 step: 234, loss is 0.19888904690742493\n",
      "epoch: 2 step: 235, loss is 0.2554893493652344\n",
      "epoch: 2 step: 236, loss is 0.12594987452030182\n",
      "epoch: 2 step: 237, loss is 0.23725219070911407\n",
      "epoch: 2 step: 238, loss is 0.3234920799732208\n",
      "epoch: 2 step: 239, loss is 0.4311600923538208\n",
      "epoch: 2 step: 240, loss is 0.30332857370376587\n",
      "epoch: 2 step: 241, loss is 0.48305052518844604\n",
      "epoch: 2 step: 242, loss is 0.16822996735572815\n",
      "epoch: 2 step: 243, loss is 0.32506120204925537\n",
      "epoch: 2 step: 244, loss is 0.2160245031118393\n",
      "epoch: 2 step: 245, loss is 0.477460652589798\n",
      "epoch: 2 step: 246, loss is 0.32919031381607056\n",
      "epoch: 2 step: 247, loss is 0.22903475165367126\n",
      "epoch: 2 step: 248, loss is 0.32015448808670044\n",
      "epoch: 2 step: 249, loss is 0.27317309379577637\n",
      "epoch: 2 step: 250, loss is 0.15307524800300598\n",
      "epoch: 2 step: 251, loss is 0.4895731210708618\n",
      "epoch: 2 step: 252, loss is 0.3910713791847229\n",
      "epoch: 2 step: 253, loss is 0.25468939542770386\n",
      "epoch: 2 step: 254, loss is 0.2645845115184784\n",
      "epoch: 2 step: 255, loss is 0.1835608333349228\n",
      "epoch: 2 step: 256, loss is 0.26186615228652954\n",
      "epoch: 2 step: 257, loss is 0.4501672387123108\n",
      "epoch: 2 step: 258, loss is 0.2437155544757843\n",
      "epoch: 2 step: 259, loss is 0.2304559051990509\n",
      "epoch: 2 step: 260, loss is 0.18596524000167847\n",
      "epoch: 2 step: 261, loss is 0.36215561628341675\n",
      "epoch: 2 step: 262, loss is 0.4167996346950531\n",
      "epoch: 2 step: 263, loss is 0.26097598671913147\n",
      "epoch: 2 step: 264, loss is 0.40447983145713806\n",
      "epoch: 2 step: 265, loss is 0.31002500653266907\n",
      "epoch: 2 step: 266, loss is 0.4669230282306671\n",
      "epoch: 2 step: 267, loss is 0.23862415552139282\n",
      "epoch: 2 step: 268, loss is 0.3070918619632721\n",
      "epoch: 2 step: 269, loss is 0.21947543323040009\n",
      "epoch: 2 step: 270, loss is 0.31715136766433716\n",
      "epoch: 2 step: 271, loss is 0.31418079137802124\n",
      "epoch: 2 step: 272, loss is 0.28823453187942505\n",
      "epoch: 2 step: 273, loss is 0.3253819942474365\n",
      "epoch: 2 step: 274, loss is 0.186867818236351\n",
      "epoch: 2 step: 275, loss is 0.15552860498428345\n",
      "epoch: 2 step: 276, loss is 0.3324111998081207\n",
      "epoch: 2 step: 277, loss is 0.30598658323287964\n",
      "epoch: 2 step: 278, loss is 0.21578481793403625\n",
      "epoch: 2 step: 279, loss is 0.20834997296333313\n",
      "epoch: 2 step: 280, loss is 0.25573164224624634\n",
      "epoch: 2 step: 281, loss is 0.2147715538740158\n",
      "epoch: 2 step: 282, loss is 0.47434329986572266\n",
      "epoch: 2 step: 283, loss is 0.3052642345428467\n",
      "epoch: 2 step: 284, loss is 0.1560165137052536\n",
      "epoch: 2 step: 285, loss is 0.31549903750419617\n",
      "epoch: 2 step: 286, loss is 0.1705784648656845\n",
      "epoch: 2 step: 287, loss is 0.329251229763031\n",
      "epoch: 2 step: 288, loss is 0.2780188322067261\n",
      "epoch: 2 step: 289, loss is 0.3260192275047302\n",
      "epoch: 2 step: 290, loss is 0.4822138249874115\n",
      "epoch: 2 step: 291, loss is 0.4166513681411743\n",
      "epoch: 2 step: 292, loss is 0.21639356017112732\n",
      "epoch: 2 step: 293, loss is 0.16759729385375977\n",
      "epoch: 2 step: 294, loss is 0.18298396468162537\n",
      "epoch: 2 step: 295, loss is 0.38878703117370605\n",
      "epoch: 2 step: 296, loss is 0.34925222396850586\n",
      "epoch: 2 step: 297, loss is 0.16746459901332855\n",
      "epoch: 2 step: 298, loss is 0.2926793098449707\n",
      "epoch: 2 step: 299, loss is 0.645089864730835\n",
      "epoch: 2 step: 300, loss is 0.3868705630302429\n",
      "epoch: 2 step: 301, loss is 0.16482096910476685\n",
      "epoch: 2 step: 302, loss is 0.10664580762386322\n",
      "epoch: 2 step: 303, loss is 0.2937295138835907\n",
      "epoch: 2 step: 304, loss is 0.2675330638885498\n",
      "epoch: 2 step: 305, loss is 0.324334979057312\n",
      "epoch: 2 step: 306, loss is 0.4631037414073944\n",
      "epoch: 2 step: 307, loss is 0.3948288559913635\n",
      "epoch: 2 step: 308, loss is 0.23296107351779938\n",
      "epoch: 2 step: 309, loss is 0.29645299911499023\n",
      "epoch: 2 step: 310, loss is 0.2051190435886383\n",
      "epoch: 2 step: 311, loss is 0.5090938806533813\n",
      "epoch: 2 step: 312, loss is 0.3006804883480072\n",
      "epoch: 2 step: 313, loss is 0.3051590025424957\n",
      "epoch: 2 step: 314, loss is 0.24196240305900574\n",
      "epoch: 2 step: 315, loss is 0.41599422693252563\n",
      "epoch: 2 step: 316, loss is 0.44397610425949097\n",
      "epoch: 2 step: 317, loss is 0.1988382339477539\n",
      "epoch: 2 step: 318, loss is 0.5213780403137207\n",
      "epoch: 2 step: 319, loss is 0.2270340919494629\n",
      "epoch: 2 step: 320, loss is 0.39247602224349976\n",
      "epoch: 2 step: 321, loss is 0.3703622817993164\n",
      "epoch: 2 step: 322, loss is 0.28586381673812866\n",
      "epoch: 2 step: 323, loss is 0.2711470127105713\n",
      "epoch: 2 step: 324, loss is 0.37342867255210876\n",
      "epoch: 2 step: 325, loss is 0.36560457944869995\n",
      "epoch: 2 step: 326, loss is 0.19968895614147186\n",
      "epoch: 2 step: 327, loss is 0.3374967873096466\n",
      "epoch: 2 step: 328, loss is 0.3330630362033844\n",
      "epoch: 2 step: 329, loss is 0.3948967158794403\n",
      "epoch: 2 step: 330, loss is 0.21231244504451752\n",
      "epoch: 2 step: 331, loss is 0.3170318603515625\n",
      "epoch: 2 step: 332, loss is 0.30976811051368713\n",
      "epoch: 2 step: 333, loss is 0.2823416590690613\n",
      "epoch: 2 step: 334, loss is 0.27898621559143066\n",
      "epoch: 2 step: 335, loss is 0.29646193981170654\n",
      "epoch: 2 step: 336, loss is 0.24988944828510284\n",
      "epoch: 2 step: 337, loss is 0.25199568271636963\n",
      "epoch: 2 step: 338, loss is 0.2577524483203888\n",
      "epoch: 2 step: 339, loss is 0.5063364505767822\n",
      "epoch: 2 step: 340, loss is 0.20154592394828796\n",
      "epoch: 2 step: 341, loss is 0.31994491815567017\n",
      "epoch: 2 step: 342, loss is 0.341679185628891\n",
      "epoch: 2 step: 343, loss is 0.30944812297821045\n",
      "epoch: 2 step: 344, loss is 0.26932981610298157\n",
      "epoch: 2 step: 345, loss is 0.2824755311012268\n",
      "epoch: 2 step: 346, loss is 0.33343106508255005\n",
      "epoch: 2 step: 347, loss is 0.38428229093551636\n",
      "epoch: 2 step: 348, loss is 0.43524667620658875\n",
      "epoch: 2 step: 349, loss is 0.29179778695106506\n",
      "epoch: 2 step: 350, loss is 0.20438848435878754\n",
      "epoch: 2 step: 351, loss is 0.21234636008739471\n",
      "epoch: 2 step: 352, loss is 0.12323293089866638\n",
      "epoch: 2 step: 353, loss is 0.38298383355140686\n",
      "epoch: 2 step: 354, loss is 0.2946051359176636\n",
      "epoch: 2 step: 355, loss is 0.3087916672229767\n",
      "epoch: 2 step: 356, loss is 0.1463572084903717\n",
      "epoch: 2 step: 357, loss is 0.2809196710586548\n",
      "epoch: 2 step: 358, loss is 0.34734728932380676\n",
      "epoch: 2 step: 359, loss is 0.24383053183555603\n",
      "epoch: 2 step: 360, loss is 0.4363704323768616\n",
      "epoch: 2 step: 361, loss is 0.2748759984970093\n",
      "epoch: 2 step: 362, loss is 0.1987985372543335\n",
      "epoch: 2 step: 363, loss is 0.35698962211608887\n",
      "epoch: 2 step: 364, loss is 0.23161011934280396\n",
      "epoch: 2 step: 365, loss is 0.19391334056854248\n",
      "epoch: 2 step: 366, loss is 0.2594386637210846\n",
      "epoch: 2 step: 367, loss is 0.32997554540634155\n",
      "epoch: 2 step: 368, loss is 0.2032395601272583\n",
      "epoch: 2 step: 369, loss is 0.467472106218338\n",
      "epoch: 2 step: 370, loss is 0.2648826241493225\n",
      "epoch: 2 step: 371, loss is 0.3699439764022827\n",
      "epoch: 2 step: 372, loss is 0.29253894090652466\n",
      "epoch: 2 step: 373, loss is 0.5982891321182251\n",
      "epoch: 2 step: 374, loss is 0.24361810088157654\n",
      "epoch: 2 step: 375, loss is 0.32246965169906616\n",
      "epoch: 2 step: 376, loss is 0.31411993503570557\n",
      "epoch: 2 step: 377, loss is 0.270100474357605\n",
      "epoch: 2 step: 378, loss is 0.20875978469848633\n",
      "epoch: 2 step: 379, loss is 0.2852032780647278\n",
      "epoch: 2 step: 380, loss is 0.4210842251777649\n",
      "epoch: 2 step: 381, loss is 0.40394100546836853\n",
      "epoch: 2 step: 382, loss is 0.19778870046138763\n",
      "epoch: 2 step: 383, loss is 0.3629758358001709\n",
      "epoch: 2 step: 384, loss is 0.43780237436294556\n",
      "epoch: 2 step: 385, loss is 0.2594827711582184\n",
      "epoch: 2 step: 386, loss is 0.28051429986953735\n",
      "epoch: 2 step: 387, loss is 0.32122600078582764\n",
      "epoch: 2 step: 388, loss is 0.29294353723526\n",
      "epoch: 2 step: 389, loss is 0.33018094301223755\n",
      "epoch: 2 step: 390, loss is 0.433966726064682\n",
      "epoch: 2 step: 391, loss is 0.4830116033554077\n",
      "epoch: 2 step: 392, loss is 0.41412708163261414\n",
      "epoch: 2 step: 393, loss is 0.20723676681518555\n",
      "epoch: 2 step: 394, loss is 0.3426489233970642\n",
      "epoch: 2 step: 395, loss is 0.18148763477802277\n",
      "epoch: 2 step: 396, loss is 0.2782289385795593\n",
      "epoch: 2 step: 397, loss is 0.20901980996131897\n",
      "epoch: 2 step: 398, loss is 0.2865496873855591\n",
      "epoch: 2 step: 399, loss is 0.2999650239944458\n",
      "epoch: 2 step: 400, loss is 0.36245518922805786\n",
      "epoch: 2 step: 401, loss is 0.3979707360267639\n",
      "epoch: 2 step: 402, loss is 0.5017183423042297\n",
      "epoch: 2 step: 403, loss is 0.23542079329490662\n",
      "epoch: 2 step: 404, loss is 0.3356572687625885\n",
      "epoch: 2 step: 405, loss is 0.21779002249240875\n",
      "epoch: 2 step: 406, loss is 0.15756340324878693\n",
      "epoch: 2 step: 407, loss is 0.2554783821105957\n",
      "epoch: 2 step: 408, loss is 0.1694265902042389\n",
      "epoch: 2 step: 409, loss is 0.26787862181663513\n",
      "epoch: 2 step: 410, loss is 0.21493104100227356\n",
      "epoch: 2 step: 411, loss is 0.35087358951568604\n",
      "epoch: 2 step: 412, loss is 0.2395153045654297\n",
      "epoch: 2 step: 413, loss is 0.24165627360343933\n",
      "epoch: 2 step: 414, loss is 0.322925329208374\n",
      "epoch: 2 step: 415, loss is 0.24167673289775848\n",
      "epoch: 2 step: 416, loss is 0.3064735531806946\n",
      "epoch: 2 step: 417, loss is 0.24938197433948517\n",
      "epoch: 2 step: 418, loss is 0.38513123989105225\n",
      "epoch: 2 step: 419, loss is 0.22412614524364471\n",
      "epoch: 2 step: 420, loss is 0.15297037363052368\n",
      "epoch: 2 step: 421, loss is 0.4325789213180542\n",
      "epoch: 2 step: 422, loss is 0.3204507529735565\n",
      "epoch: 2 step: 423, loss is 0.4520280361175537\n",
      "epoch: 2 step: 424, loss is 0.3896540403366089\n",
      "epoch: 2 step: 425, loss is 0.14847500622272491\n",
      "epoch: 2 step: 426, loss is 0.33267897367477417\n",
      "epoch: 2 step: 427, loss is 0.16959890723228455\n",
      "epoch: 2 step: 428, loss is 0.4705772399902344\n",
      "epoch: 2 step: 429, loss is 0.3032000660896301\n",
      "epoch: 2 step: 430, loss is 0.20924882590770721\n",
      "epoch: 2 step: 431, loss is 0.3223210275173187\n",
      "epoch: 2 step: 432, loss is 0.2682306170463562\n",
      "epoch: 2 step: 433, loss is 0.4741356372833252\n",
      "epoch: 2 step: 434, loss is 0.24354027211666107\n",
      "epoch: 2 step: 435, loss is 0.21020498871803284\n",
      "epoch: 2 step: 436, loss is 0.368407666683197\n",
      "epoch: 2 step: 437, loss is 0.288987398147583\n",
      "epoch: 2 step: 438, loss is 0.17685088515281677\n",
      "epoch: 2 step: 439, loss is 0.4302058815956116\n",
      "epoch: 2 step: 440, loss is 0.17641721665859222\n",
      "epoch: 2 step: 441, loss is 0.24606043100357056\n",
      "epoch: 2 step: 442, loss is 0.26284632086753845\n",
      "epoch: 2 step: 443, loss is 0.30104053020477295\n",
      "epoch: 2 step: 444, loss is 0.33764052391052246\n",
      "epoch: 2 step: 445, loss is 0.3203592896461487\n",
      "epoch: 2 step: 446, loss is 0.25491541624069214\n",
      "epoch: 2 step: 447, loss is 0.25158265233039856\n",
      "epoch: 2 step: 448, loss is 0.27780434489250183\n",
      "epoch: 2 step: 449, loss is 0.25458961725234985\n",
      "epoch: 2 step: 450, loss is 0.2679189145565033\n",
      "epoch: 2 step: 451, loss is 0.3593255877494812\n",
      "epoch: 2 step: 452, loss is 0.41264575719833374\n",
      "epoch: 2 step: 453, loss is 0.27392303943634033\n",
      "epoch: 2 step: 454, loss is 0.31228703260421753\n",
      "epoch: 2 step: 455, loss is 0.2837115526199341\n",
      "epoch: 2 step: 456, loss is 0.18641917407512665\n",
      "epoch: 2 step: 457, loss is 0.27971088886260986\n",
      "epoch: 2 step: 458, loss is 0.3103982210159302\n",
      "epoch: 2 step: 459, loss is 0.11074532568454742\n",
      "epoch: 2 step: 460, loss is 0.37386345863342285\n",
      "epoch: 2 step: 461, loss is 0.2161763459444046\n",
      "epoch: 2 step: 462, loss is 0.21764622628688812\n",
      "epoch: 2 step: 463, loss is 0.21325884759426117\n",
      "epoch: 2 step: 464, loss is 0.24916937947273254\n",
      "epoch: 2 step: 465, loss is 0.2637396454811096\n",
      "epoch: 2 step: 466, loss is 0.25073522329330444\n",
      "epoch: 2 step: 467, loss is 0.319378137588501\n",
      "epoch: 2 step: 468, loss is 0.4232111871242523\n",
      "epoch: 2 step: 469, loss is 0.2942837178707123\n",
      "epoch: 2 step: 470, loss is 0.18074777722358704\n",
      "epoch: 2 step: 471, loss is 0.4725251793861389\n",
      "epoch: 2 step: 472, loss is 0.2603970170021057\n",
      "epoch: 2 step: 473, loss is 0.3296932578086853\n",
      "epoch: 2 step: 474, loss is 0.34170085191726685\n",
      "epoch: 2 step: 475, loss is 0.16483274102210999\n",
      "epoch: 2 step: 476, loss is 0.2775004804134369\n",
      "epoch: 2 step: 477, loss is 0.2686229348182678\n",
      "epoch: 2 step: 478, loss is 0.11532752960920334\n",
      "epoch: 2 step: 479, loss is 0.32813629508018494\n",
      "epoch: 2 step: 480, loss is 0.15610072016716003\n",
      "epoch: 2 step: 481, loss is 0.35079336166381836\n",
      "epoch: 2 step: 482, loss is 0.3942427933216095\n",
      "epoch: 2 step: 483, loss is 0.2942359447479248\n",
      "epoch: 2 step: 484, loss is 0.19776450097560883\n",
      "epoch: 2 step: 485, loss is 0.28831908106803894\n",
      "epoch: 2 step: 486, loss is 0.22566437721252441\n",
      "epoch: 2 step: 487, loss is 0.21012292802333832\n",
      "epoch: 2 step: 488, loss is 0.3042888343334198\n",
      "epoch: 2 step: 489, loss is 0.29788267612457275\n",
      "epoch: 2 step: 490, loss is 0.22682669758796692\n",
      "epoch: 2 step: 491, loss is 0.3684138059616089\n",
      "epoch: 2 step: 492, loss is 0.21805793046951294\n",
      "epoch: 2 step: 493, loss is 0.3894491195678711\n",
      "epoch: 2 step: 494, loss is 0.2761116325855255\n",
      "epoch: 2 step: 495, loss is 0.30012309551239014\n",
      "epoch: 2 step: 496, loss is 0.19658145308494568\n",
      "epoch: 2 step: 497, loss is 0.41471853852272034\n",
      "epoch: 2 step: 498, loss is 0.24833889305591583\n",
      "epoch: 2 step: 499, loss is 0.13433903455734253\n",
      "epoch: 2 step: 500, loss is 0.23242545127868652\n",
      "epoch: 2 step: 501, loss is 0.338581919670105\n",
      "epoch: 2 step: 502, loss is 0.24155893921852112\n",
      "epoch: 2 step: 503, loss is 0.20878466963768005\n",
      "epoch: 2 step: 504, loss is 0.2999498248100281\n",
      "epoch: 2 step: 505, loss is 0.3619430363178253\n",
      "epoch: 2 step: 506, loss is 0.21643148362636566\n",
      "epoch: 2 step: 507, loss is 0.5413179397583008\n",
      "epoch: 2 step: 508, loss is 0.3807675242424011\n",
      "epoch: 2 step: 509, loss is 0.2775183320045471\n",
      "epoch: 2 step: 510, loss is 0.34189361333847046\n",
      "epoch: 2 step: 511, loss is 0.2993359863758087\n",
      "epoch: 2 step: 512, loss is 0.3965480923652649\n",
      "epoch: 2 step: 513, loss is 0.281646728515625\n",
      "epoch: 2 step: 514, loss is 0.3806909918785095\n",
      "epoch: 2 step: 515, loss is 0.46108344197273254\n",
      "epoch: 2 step: 516, loss is 0.24327979981899261\n",
      "epoch: 2 step: 517, loss is 0.3524267077445984\n",
      "epoch: 2 step: 518, loss is 0.26069000363349915\n",
      "epoch: 2 step: 519, loss is 0.4618164002895355\n",
      "epoch: 2 step: 520, loss is 0.3302735388278961\n",
      "epoch: 2 step: 521, loss is 0.28385549783706665\n",
      "epoch: 2 step: 522, loss is 0.2822152376174927\n",
      "epoch: 2 step: 523, loss is 0.1922547072172165\n",
      "epoch: 2 step: 524, loss is 0.4272081255912781\n",
      "epoch: 2 step: 525, loss is 0.512036144733429\n",
      "epoch: 2 step: 526, loss is 0.3897705078125\n",
      "epoch: 2 step: 527, loss is 0.2527081370353699\n",
      "epoch: 2 step: 528, loss is 0.43018829822540283\n",
      "epoch: 2 step: 529, loss is 0.20525145530700684\n",
      "epoch: 2 step: 530, loss is 0.1857932060956955\n",
      "epoch: 2 step: 531, loss is 0.47316762804985046\n",
      "epoch: 2 step: 532, loss is 0.2902175784111023\n",
      "epoch: 2 step: 533, loss is 0.21150702238082886\n",
      "epoch: 2 step: 534, loss is 0.1667921096086502\n",
      "epoch: 2 step: 535, loss is 0.3124065399169922\n",
      "epoch: 2 step: 536, loss is 0.316999614238739\n",
      "epoch: 2 step: 537, loss is 0.23318248987197876\n",
      "epoch: 2 step: 538, loss is 0.3379111588001251\n",
      "epoch: 2 step: 539, loss is 0.28689253330230713\n",
      "epoch: 2 step: 540, loss is 0.25369060039520264\n",
      "epoch: 2 step: 541, loss is 0.46345055103302\n",
      "epoch: 2 step: 542, loss is 0.3079783320426941\n",
      "epoch: 2 step: 543, loss is 0.28613707423210144\n",
      "epoch: 2 step: 544, loss is 0.35112470388412476\n",
      "epoch: 2 step: 545, loss is 0.2829071283340454\n",
      "epoch: 2 step: 546, loss is 0.2419985830783844\n",
      "epoch: 2 step: 547, loss is 0.36405250430107117\n",
      "epoch: 2 step: 548, loss is 0.33282485604286194\n",
      "epoch: 2 step: 549, loss is 0.2863074839115143\n",
      "epoch: 2 step: 550, loss is 0.49184906482696533\n",
      "epoch: 2 step: 551, loss is 0.30253419280052185\n",
      "epoch: 2 step: 552, loss is 0.32850831747055054\n",
      "epoch: 2 step: 553, loss is 0.2372043877840042\n",
      "epoch: 2 step: 554, loss is 0.4167743921279907\n",
      "epoch: 2 step: 555, loss is 0.3255109488964081\n",
      "epoch: 2 step: 556, loss is 0.16714344918727875\n",
      "epoch: 2 step: 557, loss is 0.3533764183521271\n",
      "epoch: 2 step: 558, loss is 0.36639082431793213\n",
      "epoch: 2 step: 559, loss is 0.22343981266021729\n",
      "epoch: 2 step: 560, loss is 0.16747164726257324\n",
      "epoch: 2 step: 561, loss is 0.2552754282951355\n",
      "epoch: 2 step: 562, loss is 0.3520734906196594\n",
      "epoch: 2 step: 563, loss is 0.5224556922912598\n",
      "epoch: 2 step: 564, loss is 0.6276150941848755\n",
      "epoch: 2 step: 565, loss is 0.14357158541679382\n",
      "epoch: 2 step: 566, loss is 0.26229792833328247\n",
      "epoch: 2 step: 567, loss is 0.16482597589492798\n",
      "epoch: 2 step: 568, loss is 0.3288804292678833\n",
      "epoch: 2 step: 569, loss is 0.4004085659980774\n",
      "epoch: 2 step: 570, loss is 0.5149770379066467\n",
      "epoch: 2 step: 571, loss is 0.3165402412414551\n",
      "epoch: 2 step: 572, loss is 0.5150335431098938\n",
      "epoch: 2 step: 573, loss is 0.12245668470859528\n",
      "epoch: 2 step: 574, loss is 0.3700460195541382\n",
      "epoch: 2 step: 575, loss is 0.31470760703086853\n",
      "epoch: 2 step: 576, loss is 0.18625709414482117\n",
      "epoch: 2 step: 577, loss is 0.30172455310821533\n",
      "epoch: 2 step: 578, loss is 0.2705118656158447\n",
      "epoch: 2 step: 579, loss is 0.35460078716278076\n",
      "epoch: 2 step: 580, loss is 0.30857008695602417\n",
      "epoch: 2 step: 581, loss is 0.09996312856674194\n",
      "epoch: 2 step: 582, loss is 0.2320522964000702\n",
      "epoch: 2 step: 583, loss is 0.21981245279312134\n",
      "epoch: 2 step: 584, loss is 0.31903812289237976\n",
      "epoch: 2 step: 585, loss is 0.22907589375972748\n",
      "epoch: 2 step: 586, loss is 0.4112176299095154\n",
      "epoch: 2 step: 587, loss is 0.14581458270549774\n",
      "epoch: 2 step: 588, loss is 0.3557734191417694\n",
      "epoch: 2 step: 589, loss is 0.2621561586856842\n",
      "epoch: 2 step: 590, loss is 0.2602839171886444\n",
      "epoch: 2 step: 591, loss is 0.300444632768631\n",
      "epoch: 2 step: 592, loss is 0.2838653326034546\n",
      "epoch: 2 step: 593, loss is 0.3312992453575134\n",
      "epoch: 2 step: 594, loss is 0.23646575212478638\n",
      "epoch: 2 step: 595, loss is 0.24807757139205933\n",
      "epoch: 2 step: 596, loss is 0.2771012783050537\n",
      "epoch: 2 step: 597, loss is 0.20358876883983612\n",
      "epoch: 2 step: 598, loss is 0.19026342034339905\n",
      "epoch: 2 step: 599, loss is 0.2820988893508911\n",
      "epoch: 2 step: 600, loss is 0.15761402249336243\n",
      "epoch: 2 step: 601, loss is 0.32057932019233704\n",
      "epoch: 2 step: 602, loss is 0.22225117683410645\n",
      "epoch: 2 step: 603, loss is 0.1823655366897583\n",
      "epoch: 2 step: 604, loss is 0.2692551612854004\n",
      "epoch: 2 step: 605, loss is 0.44412750005722046\n",
      "epoch: 2 step: 606, loss is 0.3042200803756714\n",
      "epoch: 2 step: 607, loss is 0.41397979855537415\n",
      "epoch: 2 step: 608, loss is 0.2760542035102844\n",
      "epoch: 2 step: 609, loss is 0.1427842527627945\n",
      "epoch: 2 step: 610, loss is 0.3913871645927429\n",
      "epoch: 2 step: 611, loss is 0.4456509053707123\n",
      "epoch: 2 step: 612, loss is 0.2072981894016266\n",
      "epoch: 2 step: 613, loss is 0.19459262490272522\n",
      "epoch: 2 step: 614, loss is 0.6068096160888672\n",
      "epoch: 2 step: 615, loss is 0.4645949602127075\n",
      "epoch: 2 step: 616, loss is 0.34474432468414307\n",
      "epoch: 2 step: 617, loss is 0.29091814160346985\n",
      "epoch: 2 step: 618, loss is 0.34755662083625793\n",
      "epoch: 2 step: 619, loss is 0.15298835933208466\n",
      "epoch: 2 step: 620, loss is 0.31678998470306396\n",
      "epoch: 2 step: 621, loss is 0.26242977380752563\n",
      "epoch: 2 step: 622, loss is 0.3044074773788452\n",
      "epoch: 2 step: 623, loss is 0.3710607588291168\n",
      "epoch: 2 step: 624, loss is 0.24305033683776855\n",
      "epoch: 2 step: 625, loss is 0.4705098867416382\n",
      "epoch: 2 step: 626, loss is 0.26140791177749634\n",
      "epoch: 2 step: 627, loss is 0.332059383392334\n",
      "epoch: 2 step: 628, loss is 0.3528134822845459\n",
      "epoch: 2 step: 629, loss is 0.26049721240997314\n",
      "epoch: 2 step: 630, loss is 0.2244490087032318\n",
      "epoch: 2 step: 631, loss is 0.2192298173904419\n",
      "epoch: 2 step: 632, loss is 0.2552504539489746\n",
      "epoch: 2 step: 633, loss is 0.3040943145751953\n",
      "epoch: 2 step: 634, loss is 0.33903956413269043\n",
      "epoch: 2 step: 635, loss is 0.23724430799484253\n",
      "epoch: 2 step: 636, loss is 0.3269386887550354\n",
      "epoch: 2 step: 637, loss is 0.2444399893283844\n",
      "epoch: 2 step: 638, loss is 0.30703508853912354\n",
      "epoch: 2 step: 639, loss is 0.3054530918598175\n",
      "epoch: 2 step: 640, loss is 0.33035987615585327\n",
      "epoch: 2 step: 641, loss is 0.34940850734710693\n",
      "epoch: 2 step: 642, loss is 0.326030969619751\n",
      "epoch: 2 step: 643, loss is 0.25651487708091736\n",
      "epoch: 2 step: 644, loss is 0.20179840922355652\n",
      "epoch: 2 step: 645, loss is 0.1877678483724594\n",
      "epoch: 2 step: 646, loss is 0.28980690240859985\n",
      "epoch: 2 step: 647, loss is 0.5644780397415161\n",
      "epoch: 2 step: 648, loss is 0.20686660706996918\n",
      "epoch: 2 step: 649, loss is 0.3881834149360657\n",
      "epoch: 2 step: 650, loss is 0.3378916084766388\n",
      "epoch: 2 step: 651, loss is 0.26775452494621277\n",
      "epoch: 2 step: 652, loss is 0.34442269802093506\n",
      "epoch: 2 step: 653, loss is 0.3211955428123474\n",
      "epoch: 2 step: 654, loss is 0.3325793743133545\n",
      "epoch: 2 step: 655, loss is 0.16800515353679657\n",
      "epoch: 2 step: 656, loss is 0.17046940326690674\n",
      "epoch: 2 step: 657, loss is 0.23273003101348877\n",
      "epoch: 2 step: 658, loss is 0.29041212797164917\n",
      "epoch: 2 step: 659, loss is 0.333678275346756\n",
      "epoch: 2 step: 660, loss is 0.4209411144256592\n",
      "epoch: 2 step: 661, loss is 0.12938030064105988\n",
      "epoch: 2 step: 662, loss is 0.2121971845626831\n",
      "epoch: 2 step: 663, loss is 0.18586289882659912\n",
      "epoch: 2 step: 664, loss is 0.39151832461357117\n",
      "epoch: 2 step: 665, loss is 0.24263489246368408\n",
      "epoch: 2 step: 666, loss is 0.17734020948410034\n",
      "epoch: 2 step: 667, loss is 0.31520962715148926\n",
      "epoch: 2 step: 668, loss is 0.2109363079071045\n",
      "epoch: 2 step: 669, loss is 0.22347861528396606\n",
      "epoch: 2 step: 670, loss is 0.2507583796977997\n",
      "epoch: 2 step: 671, loss is 0.4046264886856079\n",
      "epoch: 2 step: 672, loss is 0.40384918451309204\n",
      "epoch: 2 step: 673, loss is 0.23028844594955444\n",
      "epoch: 2 step: 674, loss is 0.4379633069038391\n",
      "epoch: 2 step: 675, loss is 0.22814898192882538\n",
      "epoch: 2 step: 676, loss is 0.16342362761497498\n",
      "epoch: 2 step: 677, loss is 0.27856314182281494\n",
      "epoch: 2 step: 678, loss is 0.35897839069366455\n",
      "epoch: 2 step: 679, loss is 0.45560145378112793\n",
      "epoch: 2 step: 680, loss is 0.2697165012359619\n",
      "epoch: 2 step: 681, loss is 0.39254578948020935\n",
      "epoch: 2 step: 682, loss is 0.3718349039554596\n",
      "epoch: 2 step: 683, loss is 0.4112842381000519\n",
      "epoch: 2 step: 684, loss is 0.14649420976638794\n",
      "epoch: 2 step: 685, loss is 0.4906041622161865\n",
      "epoch: 2 step: 686, loss is 0.3285791873931885\n",
      "epoch: 2 step: 687, loss is 0.24248962104320526\n",
      "epoch: 2 step: 688, loss is 0.1885441094636917\n",
      "epoch: 2 step: 689, loss is 0.2611563205718994\n",
      "epoch: 2 step: 690, loss is 0.19079777598381042\n",
      "epoch: 2 step: 691, loss is 0.13011592626571655\n",
      "epoch: 2 step: 692, loss is 0.2899961769580841\n",
      "epoch: 2 step: 693, loss is 0.2041390836238861\n",
      "epoch: 2 step: 694, loss is 0.37367987632751465\n",
      "epoch: 2 step: 695, loss is 0.1583859622478485\n",
      "epoch: 2 step: 696, loss is 0.3458326458930969\n",
      "epoch: 2 step: 697, loss is 0.3063419461250305\n",
      "epoch: 2 step: 698, loss is 0.2019263207912445\n",
      "epoch: 2 step: 699, loss is 0.40411680936813354\n",
      "epoch: 2 step: 700, loss is 0.2530912756919861\n",
      "epoch: 2 step: 701, loss is 0.4786776602268219\n",
      "epoch: 2 step: 702, loss is 0.23399469256401062\n",
      "epoch: 2 step: 703, loss is 0.3202207088470459\n",
      "epoch: 2 step: 704, loss is 0.14318270981311798\n",
      "epoch: 2 step: 705, loss is 0.31720250844955444\n",
      "epoch: 2 step: 706, loss is 0.4971005916595459\n",
      "epoch: 2 step: 707, loss is 0.31078657507896423\n",
      "epoch: 2 step: 708, loss is 0.30906879901885986\n",
      "epoch: 2 step: 709, loss is 0.2457628846168518\n",
      "epoch: 2 step: 710, loss is 0.31395936012268066\n",
      "epoch: 2 step: 711, loss is 0.2967034578323364\n",
      "epoch: 2 step: 712, loss is 0.257606565952301\n",
      "epoch: 2 step: 713, loss is 0.2446216642856598\n",
      "epoch: 2 step: 714, loss is 0.22745667397975922\n",
      "epoch: 2 step: 715, loss is 0.1374160647392273\n",
      "epoch: 2 step: 716, loss is 0.3053712844848633\n",
      "epoch: 2 step: 717, loss is 0.39096343517303467\n",
      "epoch: 2 step: 718, loss is 0.10256345570087433\n",
      "epoch: 2 step: 719, loss is 0.23837962746620178\n",
      "epoch: 2 step: 720, loss is 0.15082135796546936\n",
      "epoch: 2 step: 721, loss is 0.4252534806728363\n",
      "epoch: 2 step: 722, loss is 0.1676524579524994\n",
      "epoch: 2 step: 723, loss is 0.28035181760787964\n",
      "epoch: 2 step: 724, loss is 0.5267308950424194\n",
      "epoch: 2 step: 725, loss is 0.3138726055622101\n",
      "epoch: 2 step: 726, loss is 0.13157713413238525\n",
      "epoch: 2 step: 727, loss is 0.21583768725395203\n",
      "epoch: 2 step: 728, loss is 0.2759486436843872\n",
      "epoch: 2 step: 729, loss is 0.1626192033290863\n",
      "epoch: 2 step: 730, loss is 0.3427277207374573\n",
      "epoch: 2 step: 731, loss is 0.37001723051071167\n",
      "epoch: 2 step: 732, loss is 0.22819408774375916\n",
      "epoch: 2 step: 733, loss is 0.22697994112968445\n",
      "epoch: 2 step: 734, loss is 0.15796716511249542\n",
      "epoch: 2 step: 735, loss is 0.16604353487491608\n",
      "epoch: 2 step: 736, loss is 0.20078961551189423\n",
      "epoch: 2 step: 737, loss is 0.22689701616764069\n",
      "epoch: 2 step: 738, loss is 0.09187059104442596\n",
      "epoch: 2 step: 739, loss is 0.26373744010925293\n",
      "epoch: 2 step: 740, loss is 0.3235664367675781\n",
      "epoch: 2 step: 741, loss is 0.324088990688324\n",
      "epoch: 2 step: 742, loss is 0.3386678099632263\n",
      "epoch: 2 step: 743, loss is 0.4174680709838867\n",
      "epoch: 2 step: 744, loss is 0.2004452645778656\n",
      "epoch: 2 step: 745, loss is 0.22272592782974243\n",
      "epoch: 2 step: 746, loss is 0.22519098222255707\n",
      "epoch: 2 step: 747, loss is 0.34844672679901123\n",
      "epoch: 2 step: 748, loss is 0.5800524353981018\n",
      "epoch: 2 step: 749, loss is 0.26828286051750183\n",
      "epoch: 2 step: 750, loss is 0.28059613704681396\n",
      "epoch: 2 step: 751, loss is 0.30431151390075684\n",
      "epoch: 2 step: 752, loss is 0.33119910955429077\n",
      "epoch: 2 step: 753, loss is 0.3741883635520935\n",
      "epoch: 2 step: 754, loss is 0.33422398567199707\n",
      "epoch: 2 step: 755, loss is 0.3007771968841553\n",
      "epoch: 2 step: 756, loss is 0.32400810718536377\n",
      "epoch: 2 step: 757, loss is 0.1459600031375885\n",
      "epoch: 2 step: 758, loss is 0.1907002478837967\n",
      "epoch: 2 step: 759, loss is 0.3192096948623657\n",
      "epoch: 2 step: 760, loss is 0.4192063808441162\n",
      "epoch: 2 step: 761, loss is 0.2176576554775238\n",
      "epoch: 2 step: 762, loss is 0.3626628518104553\n",
      "epoch: 2 step: 763, loss is 0.46668586134910583\n",
      "epoch: 2 step: 764, loss is 0.509697437286377\n",
      "epoch: 2 step: 765, loss is 0.19853457808494568\n",
      "epoch: 2 step: 766, loss is 0.11514042317867279\n",
      "epoch: 2 step: 767, loss is 0.19660614430904388\n",
      "epoch: 2 step: 768, loss is 0.48833024501800537\n",
      "epoch: 2 step: 769, loss is 0.16827623546123505\n",
      "epoch: 2 step: 770, loss is 0.25559288263320923\n",
      "epoch: 2 step: 771, loss is 0.2364683896303177\n",
      "epoch: 2 step: 772, loss is 0.17577850818634033\n",
      "epoch: 2 step: 773, loss is 0.17705754935741425\n",
      "epoch: 2 step: 774, loss is 0.37673038244247437\n",
      "epoch: 2 step: 775, loss is 0.290137380361557\n",
      "epoch: 2 step: 776, loss is 0.29175812005996704\n",
      "epoch: 2 step: 777, loss is 0.19193047285079956\n",
      "epoch: 2 step: 778, loss is 0.36722251772880554\n",
      "epoch: 2 step: 779, loss is 0.3243221640586853\n",
      "epoch: 2 step: 780, loss is 0.24355828762054443\n",
      "epoch: 2 step: 781, loss is 0.2451588660478592\n",
      "epoch: 2 step: 782, loss is 0.19108490645885468\n",
      "epoch: 2 step: 783, loss is 0.3221258521080017\n",
      "epoch: 2 step: 784, loss is 0.21872210502624512\n",
      "epoch: 2 step: 785, loss is 0.19038426876068115\n",
      "epoch: 2 step: 786, loss is 0.12190409004688263\n",
      "epoch: 2 step: 787, loss is 0.314267098903656\n",
      "epoch: 2 step: 788, loss is 0.2795051336288452\n",
      "epoch: 2 step: 789, loss is 0.2716369032859802\n",
      "epoch: 2 step: 790, loss is 0.3971039652824402\n",
      "epoch: 2 step: 791, loss is 0.3141106367111206\n",
      "epoch: 2 step: 792, loss is 0.2353990077972412\n",
      "epoch: 2 step: 793, loss is 0.2929023504257202\n",
      "epoch: 2 step: 794, loss is 0.2203715443611145\n",
      "epoch: 2 step: 795, loss is 0.11873065680265427\n",
      "epoch: 2 step: 796, loss is 0.2500581443309784\n",
      "epoch: 2 step: 797, loss is 0.4268508553504944\n",
      "epoch: 2 step: 798, loss is 0.2426171749830246\n",
      "epoch: 2 step: 799, loss is 0.24797996878623962\n",
      "epoch: 2 step: 800, loss is 0.2915129065513611\n",
      "epoch: 2 step: 801, loss is 0.2598634362220764\n",
      "epoch: 2 step: 802, loss is 0.5420675873756409\n",
      "epoch: 2 step: 803, loss is 0.20564976334571838\n",
      "epoch: 2 step: 804, loss is 0.15545019507408142\n",
      "epoch: 2 step: 805, loss is 0.22493645548820496\n",
      "epoch: 2 step: 806, loss is 0.27642083168029785\n",
      "epoch: 2 step: 807, loss is 0.3172863721847534\n",
      "epoch: 2 step: 808, loss is 0.3051634430885315\n",
      "epoch: 2 step: 809, loss is 0.40468233823776245\n",
      "epoch: 2 step: 810, loss is 0.23539705574512482\n",
      "epoch: 2 step: 811, loss is 0.5255919098854065\n",
      "epoch: 2 step: 812, loss is 0.4620113968849182\n",
      "epoch: 2 step: 813, loss is 0.2827802896499634\n",
      "epoch: 2 step: 814, loss is 0.32057881355285645\n",
      "epoch: 2 step: 815, loss is 0.2647465765476227\n",
      "epoch: 2 step: 816, loss is 0.3168400526046753\n",
      "epoch: 2 step: 817, loss is 0.4482930302619934\n",
      "epoch: 2 step: 818, loss is 0.20732226967811584\n",
      "epoch: 2 step: 819, loss is 0.42377936840057373\n",
      "epoch: 2 step: 820, loss is 0.2581947147846222\n",
      "epoch: 2 step: 821, loss is 0.23609045147895813\n",
      "epoch: 2 step: 822, loss is 0.24669185280799866\n",
      "epoch: 2 step: 823, loss is 0.2621167302131653\n",
      "epoch: 2 step: 824, loss is 0.2062484622001648\n",
      "epoch: 2 step: 825, loss is 0.4061242938041687\n",
      "epoch: 2 step: 826, loss is 0.5977322459220886\n",
      "epoch: 2 step: 827, loss is 0.24496272206306458\n",
      "epoch: 2 step: 828, loss is 0.22233889997005463\n",
      "epoch: 2 step: 829, loss is 0.19752347469329834\n",
      "epoch: 2 step: 830, loss is 0.32444173097610474\n",
      "epoch: 2 step: 831, loss is 0.23556217551231384\n",
      "epoch: 2 step: 832, loss is 0.21793048083782196\n",
      "epoch: 2 step: 833, loss is 0.37431079149246216\n",
      "epoch: 2 step: 834, loss is 0.23073329031467438\n",
      "epoch: 2 step: 835, loss is 0.3721195459365845\n",
      "epoch: 2 step: 836, loss is 0.35010039806365967\n",
      "epoch: 2 step: 837, loss is 0.2559250593185425\n",
      "epoch: 2 step: 838, loss is 0.34362179040908813\n",
      "epoch: 2 step: 839, loss is 0.2728995382785797\n",
      "epoch: 2 step: 840, loss is 0.22927138209342957\n",
      "epoch: 2 step: 841, loss is 0.22246742248535156\n",
      "epoch: 2 step: 842, loss is 0.3308755159378052\n",
      "epoch: 2 step: 843, loss is 0.30461591482162476\n",
      "epoch: 2 step: 844, loss is 0.28825944662094116\n",
      "epoch: 2 step: 845, loss is 0.47998756170272827\n",
      "epoch: 2 step: 846, loss is 0.16207627952098846\n",
      "epoch: 2 step: 847, loss is 0.2802524268627167\n",
      "epoch: 2 step: 848, loss is 0.3759111762046814\n",
      "epoch: 2 step: 849, loss is 0.18949027359485626\n",
      "epoch: 2 step: 850, loss is 0.34410449862480164\n",
      "epoch: 2 step: 851, loss is 0.3907991647720337\n",
      "epoch: 2 step: 852, loss is 0.253888875246048\n",
      "epoch: 2 step: 853, loss is 0.30825650691986084\n",
      "epoch: 2 step: 854, loss is 0.34116947650909424\n",
      "epoch: 2 step: 855, loss is 0.4126645624637604\n",
      "epoch: 2 step: 856, loss is 0.421934574842453\n",
      "epoch: 2 step: 857, loss is 0.2763458490371704\n",
      "epoch: 2 step: 858, loss is 0.2855646014213562\n",
      "epoch: 2 step: 859, loss is 0.29079076647758484\n",
      "epoch: 2 step: 860, loss is 0.37962377071380615\n",
      "epoch: 2 step: 861, loss is 0.2349121868610382\n",
      "epoch: 2 step: 862, loss is 0.2337172031402588\n",
      "epoch: 2 step: 863, loss is 0.3091273307800293\n",
      "epoch: 2 step: 864, loss is 0.3767770528793335\n",
      "epoch: 2 step: 865, loss is 0.18091288208961487\n",
      "epoch: 2 step: 866, loss is 0.2856346368789673\n",
      "epoch: 2 step: 867, loss is 0.21776077151298523\n",
      "epoch: 2 step: 868, loss is 0.2202642858028412\n",
      "epoch: 2 step: 869, loss is 0.30657657980918884\n",
      "epoch: 2 step: 870, loss is 0.3706600069999695\n",
      "epoch: 2 step: 871, loss is 0.31536346673965454\n",
      "epoch: 2 step: 872, loss is 0.2650313973426819\n",
      "epoch: 2 step: 873, loss is 0.40402737259864807\n",
      "epoch: 2 step: 874, loss is 0.19709119200706482\n",
      "epoch: 2 step: 875, loss is 0.19133546948432922\n",
      "epoch: 2 step: 876, loss is 0.2759441137313843\n",
      "epoch: 2 step: 877, loss is 0.3598702549934387\n",
      "epoch: 2 step: 878, loss is 0.3222452998161316\n",
      "epoch: 2 step: 879, loss is 0.33732616901397705\n",
      "epoch: 2 step: 880, loss is 0.20963799953460693\n",
      "epoch: 2 step: 881, loss is 0.24733586609363556\n",
      "epoch: 2 step: 882, loss is 0.30482450127601624\n",
      "epoch: 2 step: 883, loss is 0.37515535950660706\n",
      "epoch: 2 step: 884, loss is 0.27441203594207764\n",
      "epoch: 2 step: 885, loss is 0.22980853915214539\n",
      "epoch: 2 step: 886, loss is 0.16512683033943176\n",
      "epoch: 2 step: 887, loss is 0.3743828237056732\n",
      "epoch: 2 step: 888, loss is 0.2035844624042511\n",
      "epoch: 2 step: 889, loss is 0.13635775446891785\n",
      "epoch: 2 step: 890, loss is 0.148561492562294\n",
      "epoch: 2 step: 891, loss is 0.21025767922401428\n",
      "epoch: 2 step: 892, loss is 0.1708931177854538\n",
      "epoch: 2 step: 893, loss is 0.31556445360183716\n",
      "epoch: 2 step: 894, loss is 0.231877401471138\n",
      "epoch: 2 step: 895, loss is 0.2745828628540039\n",
      "epoch: 2 step: 896, loss is 0.2779991030693054\n",
      "epoch: 2 step: 897, loss is 0.1067996472120285\n",
      "epoch: 2 step: 898, loss is 0.36942052841186523\n",
      "epoch: 2 step: 899, loss is 0.21064183115959167\n",
      "epoch: 2 step: 900, loss is 0.12502208352088928\n",
      "epoch: 2 step: 901, loss is 0.2597409188747406\n",
      "epoch: 2 step: 902, loss is 0.2722882032394409\n",
      "epoch: 2 step: 903, loss is 0.18801727890968323\n",
      "epoch: 2 step: 904, loss is 0.3940911293029785\n",
      "epoch: 2 step: 905, loss is 0.2223854959011078\n",
      "epoch: 2 step: 906, loss is 0.3211127519607544\n",
      "epoch: 2 step: 907, loss is 0.2297041416168213\n",
      "epoch: 2 step: 908, loss is 0.15904177725315094\n",
      "epoch: 2 step: 909, loss is 0.21005888283252716\n",
      "epoch: 2 step: 910, loss is 0.22496706247329712\n",
      "epoch: 2 step: 911, loss is 0.22312010824680328\n",
      "epoch: 2 step: 912, loss is 0.23003891110420227\n",
      "epoch: 2 step: 913, loss is 0.3575069308280945\n",
      "epoch: 2 step: 914, loss is 0.28268808126449585\n",
      "epoch: 2 step: 915, loss is 0.22100381553173065\n",
      "epoch: 2 step: 916, loss is 0.522555410861969\n",
      "epoch: 2 step: 917, loss is 0.1668386310338974\n",
      "epoch: 2 step: 918, loss is 0.22483617067337036\n",
      "epoch: 2 step: 919, loss is 0.20296058058738708\n",
      "epoch: 2 step: 920, loss is 0.4516066610813141\n",
      "epoch: 2 step: 921, loss is 0.39008891582489014\n",
      "epoch: 2 step: 922, loss is 0.29752612113952637\n",
      "epoch: 2 step: 923, loss is 0.22578541934490204\n",
      "epoch: 2 step: 924, loss is 0.4684278964996338\n",
      "epoch: 2 step: 925, loss is 0.1971617043018341\n",
      "epoch: 2 step: 926, loss is 0.16586115956306458\n",
      "epoch: 2 step: 927, loss is 0.14832118153572083\n",
      "epoch: 2 step: 928, loss is 0.42726030945777893\n",
      "epoch: 2 step: 929, loss is 0.284866064786911\n",
      "epoch: 2 step: 930, loss is 0.17413964867591858\n",
      "epoch: 2 step: 931, loss is 0.28304556012153625\n",
      "epoch: 2 step: 932, loss is 0.3878801167011261\n",
      "epoch: 2 step: 933, loss is 0.24722199141979218\n",
      "epoch: 2 step: 934, loss is 0.26028838753700256\n",
      "epoch: 2 step: 935, loss is 0.23123793303966522\n",
      "epoch: 2 step: 936, loss is 0.2046676129102707\n",
      "epoch: 2 step: 937, loss is 0.22559869289398193\n",
      "epoch: 3 step: 1, loss is 0.21411265432834625\n",
      "epoch: 3 step: 2, loss is 0.17973878979682922\n",
      "epoch: 3 step: 3, loss is 0.21129319071769714\n",
      "epoch: 3 step: 4, loss is 0.20179343223571777\n",
      "epoch: 3 step: 5, loss is 0.28364408016204834\n",
      "epoch: 3 step: 6, loss is 0.18217715620994568\n",
      "epoch: 3 step: 7, loss is 0.1181793063879013\n",
      "epoch: 3 step: 8, loss is 0.3199559450149536\n",
      "epoch: 3 step: 9, loss is 0.2033466398715973\n",
      "epoch: 3 step: 10, loss is 0.248583123087883\n",
      "epoch: 3 step: 11, loss is 0.23670661449432373\n",
      "epoch: 3 step: 12, loss is 0.2931484878063202\n",
      "epoch: 3 step: 13, loss is 0.30399227142333984\n",
      "epoch: 3 step: 14, loss is 0.2005358636379242\n",
      "epoch: 3 step: 15, loss is 0.12328730523586273\n",
      "epoch: 3 step: 16, loss is 0.2559129595756531\n",
      "epoch: 3 step: 17, loss is 0.29238051176071167\n",
      "epoch: 3 step: 18, loss is 0.3670949637889862\n",
      "epoch: 3 step: 19, loss is 0.3325684666633606\n",
      "epoch: 3 step: 20, loss is 0.4870038628578186\n",
      "epoch: 3 step: 21, loss is 0.17197483777999878\n",
      "epoch: 3 step: 22, loss is 0.25957411527633667\n",
      "epoch: 3 step: 23, loss is 0.3662688136100769\n",
      "epoch: 3 step: 24, loss is 0.27215397357940674\n",
      "epoch: 3 step: 25, loss is 0.18774589896202087\n",
      "epoch: 3 step: 26, loss is 0.3147163391113281\n",
      "epoch: 3 step: 27, loss is 0.20689189434051514\n",
      "epoch: 3 step: 28, loss is 0.2521289587020874\n",
      "epoch: 3 step: 29, loss is 0.17180484533309937\n",
      "epoch: 3 step: 30, loss is 0.38027626276016235\n",
      "epoch: 3 step: 31, loss is 0.2718556523323059\n",
      "epoch: 3 step: 32, loss is 0.26187098026275635\n",
      "epoch: 3 step: 33, loss is 0.16170907020568848\n",
      "epoch: 3 step: 34, loss is 0.3026570975780487\n",
      "epoch: 3 step: 35, loss is 0.2561284601688385\n",
      "epoch: 3 step: 36, loss is 0.13571876287460327\n",
      "epoch: 3 step: 37, loss is 0.25236228108406067\n",
      "epoch: 3 step: 38, loss is 0.2948701083660126\n",
      "epoch: 3 step: 39, loss is 0.2779056429862976\n",
      "epoch: 3 step: 40, loss is 0.29100170731544495\n",
      "epoch: 3 step: 41, loss is 0.1500486135482788\n",
      "epoch: 3 step: 42, loss is 0.31830281019210815\n",
      "epoch: 3 step: 43, loss is 0.3922431468963623\n",
      "epoch: 3 step: 44, loss is 0.2684674561023712\n",
      "epoch: 3 step: 45, loss is 0.2631378471851349\n",
      "epoch: 3 step: 46, loss is 0.18487264215946198\n",
      "epoch: 3 step: 47, loss is 0.29236674308776855\n",
      "epoch: 3 step: 48, loss is 0.2693484127521515\n",
      "epoch: 3 step: 49, loss is 0.2560025155544281\n",
      "epoch: 3 step: 50, loss is 0.24581660330295563\n",
      "epoch: 3 step: 51, loss is 0.308771550655365\n",
      "epoch: 3 step: 52, loss is 0.17761008441448212\n",
      "epoch: 3 step: 53, loss is 0.30058199167251587\n",
      "epoch: 3 step: 54, loss is 0.19345596432685852\n",
      "epoch: 3 step: 55, loss is 0.19785240292549133\n",
      "epoch: 3 step: 56, loss is 0.12224233150482178\n",
      "epoch: 3 step: 57, loss is 0.21329620480537415\n",
      "epoch: 3 step: 58, loss is 0.3091123104095459\n",
      "epoch: 3 step: 59, loss is 0.1368788778781891\n",
      "epoch: 3 step: 60, loss is 0.3338628113269806\n",
      "epoch: 3 step: 61, loss is 0.4371141791343689\n",
      "epoch: 3 step: 62, loss is 0.2307383120059967\n",
      "epoch: 3 step: 63, loss is 0.28475162386894226\n",
      "epoch: 3 step: 64, loss is 0.18350565433502197\n",
      "epoch: 3 step: 65, loss is 0.37425947189331055\n",
      "epoch: 3 step: 66, loss is 0.19277435541152954\n",
      "epoch: 3 step: 67, loss is 0.42177754640579224\n",
      "epoch: 3 step: 68, loss is 0.2241172194480896\n",
      "epoch: 3 step: 69, loss is 0.09524606913328171\n",
      "epoch: 3 step: 70, loss is 0.3811660706996918\n",
      "epoch: 3 step: 71, loss is 0.274138480424881\n",
      "epoch: 3 step: 72, loss is 0.212739497423172\n",
      "epoch: 3 step: 73, loss is 0.5000361204147339\n",
      "epoch: 3 step: 74, loss is 0.2703930735588074\n",
      "epoch: 3 step: 75, loss is 0.2758657932281494\n",
      "epoch: 3 step: 76, loss is 0.21927471458911896\n",
      "epoch: 3 step: 77, loss is 0.29930973052978516\n",
      "epoch: 3 step: 78, loss is 0.1564343273639679\n",
      "epoch: 3 step: 79, loss is 0.21566903591156006\n",
      "epoch: 3 step: 80, loss is 0.2111535668373108\n",
      "epoch: 3 step: 81, loss is 0.38369590044021606\n",
      "epoch: 3 step: 82, loss is 0.33689406514167786\n",
      "epoch: 3 step: 83, loss is 0.18136101961135864\n",
      "epoch: 3 step: 84, loss is 0.19015124440193176\n",
      "epoch: 3 step: 85, loss is 0.15976637601852417\n",
      "epoch: 3 step: 86, loss is 0.3560808002948761\n",
      "epoch: 3 step: 87, loss is 0.33179885149002075\n",
      "epoch: 3 step: 88, loss is 0.1520637571811676\n",
      "epoch: 3 step: 89, loss is 0.16924378275871277\n",
      "epoch: 3 step: 90, loss is 0.3775864839553833\n",
      "epoch: 3 step: 91, loss is 0.16417726874351501\n",
      "epoch: 3 step: 92, loss is 0.27737629413604736\n",
      "epoch: 3 step: 93, loss is 0.2556896209716797\n",
      "epoch: 3 step: 94, loss is 0.24961647391319275\n",
      "epoch: 3 step: 95, loss is 0.3475888967514038\n",
      "epoch: 3 step: 96, loss is 0.4016314744949341\n",
      "epoch: 3 step: 97, loss is 0.1213420182466507\n",
      "epoch: 3 step: 98, loss is 0.2828180491924286\n",
      "epoch: 3 step: 99, loss is 0.33011749386787415\n",
      "epoch: 3 step: 100, loss is 0.2616930603981018\n",
      "epoch: 3 step: 101, loss is 0.18380706012248993\n",
      "epoch: 3 step: 102, loss is 0.1609247624874115\n",
      "epoch: 3 step: 103, loss is 0.2104269564151764\n",
      "epoch: 3 step: 104, loss is 0.22206175327301025\n",
      "epoch: 3 step: 105, loss is 0.09996319562196732\n",
      "epoch: 3 step: 106, loss is 0.2371681034564972\n",
      "epoch: 3 step: 107, loss is 0.22978726029396057\n",
      "epoch: 3 step: 108, loss is 0.4053938090801239\n",
      "epoch: 3 step: 109, loss is 0.22035522758960724\n",
      "epoch: 3 step: 110, loss is 0.20570974051952362\n",
      "epoch: 3 step: 111, loss is 0.15880253911018372\n",
      "epoch: 3 step: 112, loss is 0.35489019751548767\n",
      "epoch: 3 step: 113, loss is 0.1499088704586029\n",
      "epoch: 3 step: 114, loss is 0.23194172978401184\n",
      "epoch: 3 step: 115, loss is 0.15632358193397522\n",
      "epoch: 3 step: 116, loss is 0.33132627606391907\n",
      "epoch: 3 step: 117, loss is 0.23157215118408203\n",
      "epoch: 3 step: 118, loss is 0.26596128940582275\n",
      "epoch: 3 step: 119, loss is 0.15328696370124817\n",
      "epoch: 3 step: 120, loss is 0.2568396329879761\n",
      "epoch: 3 step: 121, loss is 0.22445455193519592\n",
      "epoch: 3 step: 122, loss is 0.2222905457019806\n",
      "epoch: 3 step: 123, loss is 0.316895067691803\n",
      "epoch: 3 step: 124, loss is 0.13207922875881195\n",
      "epoch: 3 step: 125, loss is 0.251839280128479\n",
      "epoch: 3 step: 126, loss is 0.3797453045845032\n",
      "epoch: 3 step: 127, loss is 0.07976125180721283\n",
      "epoch: 3 step: 128, loss is 0.25670379400253296\n",
      "epoch: 3 step: 129, loss is 0.12565141916275024\n",
      "epoch: 3 step: 130, loss is 0.29583674669265747\n",
      "epoch: 3 step: 131, loss is 0.2680331766605377\n",
      "epoch: 3 step: 132, loss is 0.32830798625946045\n",
      "epoch: 3 step: 133, loss is 0.2742726802825928\n",
      "epoch: 3 step: 134, loss is 0.18354807794094086\n",
      "epoch: 3 step: 135, loss is 0.1523822844028473\n",
      "epoch: 3 step: 136, loss is 0.09250648319721222\n",
      "epoch: 3 step: 137, loss is 0.23836082220077515\n",
      "epoch: 3 step: 138, loss is 0.29474902153015137\n",
      "epoch: 3 step: 139, loss is 0.2820398807525635\n",
      "epoch: 3 step: 140, loss is 0.3167533278465271\n",
      "epoch: 3 step: 141, loss is 0.2806481719017029\n",
      "epoch: 3 step: 142, loss is 0.29706037044525146\n",
      "epoch: 3 step: 143, loss is 0.19378571212291718\n",
      "epoch: 3 step: 144, loss is 0.10039577633142471\n",
      "epoch: 3 step: 145, loss is 0.1777837872505188\n",
      "epoch: 3 step: 146, loss is 0.3735664486885071\n",
      "epoch: 3 step: 147, loss is 0.20421676337718964\n",
      "epoch: 3 step: 148, loss is 0.23129013180732727\n",
      "epoch: 3 step: 149, loss is 0.17188863456249237\n",
      "epoch: 3 step: 150, loss is 0.23406827449798584\n",
      "epoch: 3 step: 151, loss is 0.2420508861541748\n",
      "epoch: 3 step: 152, loss is 0.18974438309669495\n",
      "epoch: 3 step: 153, loss is 0.35175657272338867\n",
      "epoch: 3 step: 154, loss is 0.25018981099128723\n",
      "epoch: 3 step: 155, loss is 0.23253990709781647\n",
      "epoch: 3 step: 156, loss is 0.18298691511154175\n",
      "epoch: 3 step: 157, loss is 0.3465466797351837\n",
      "epoch: 3 step: 158, loss is 0.17063772678375244\n",
      "epoch: 3 step: 159, loss is 0.09609603881835938\n",
      "epoch: 3 step: 160, loss is 0.2210194170475006\n",
      "epoch: 3 step: 161, loss is 0.2549075782299042\n",
      "epoch: 3 step: 162, loss is 0.2114109992980957\n",
      "epoch: 3 step: 163, loss is 0.5025551319122314\n",
      "epoch: 3 step: 164, loss is 0.20828785002231598\n",
      "epoch: 3 step: 165, loss is 0.21796900033950806\n",
      "epoch: 3 step: 166, loss is 0.2595047354698181\n",
      "epoch: 3 step: 167, loss is 0.2912713885307312\n",
      "epoch: 3 step: 168, loss is 0.12690295279026031\n",
      "epoch: 3 step: 169, loss is 0.384052574634552\n",
      "epoch: 3 step: 170, loss is 0.12947812676429749\n",
      "epoch: 3 step: 171, loss is 0.23575733602046967\n",
      "epoch: 3 step: 172, loss is 0.40911930799484253\n",
      "epoch: 3 step: 173, loss is 0.1771339625120163\n",
      "epoch: 3 step: 174, loss is 0.18820351362228394\n",
      "epoch: 3 step: 175, loss is 0.2164967954158783\n",
      "epoch: 3 step: 176, loss is 0.376900851726532\n",
      "epoch: 3 step: 177, loss is 0.18283095955848694\n",
      "epoch: 3 step: 178, loss is 0.350167453289032\n",
      "epoch: 3 step: 179, loss is 0.2130037099123001\n",
      "epoch: 3 step: 180, loss is 0.29682227969169617\n",
      "epoch: 3 step: 181, loss is 0.2599509358406067\n",
      "epoch: 3 step: 182, loss is 0.13726693391799927\n",
      "epoch: 3 step: 183, loss is 0.3202419877052307\n",
      "epoch: 3 step: 184, loss is 0.1700567901134491\n",
      "epoch: 3 step: 185, loss is 0.2747524380683899\n",
      "epoch: 3 step: 186, loss is 0.3232073485851288\n",
      "epoch: 3 step: 187, loss is 0.13655370473861694\n",
      "epoch: 3 step: 188, loss is 0.30494412779808044\n",
      "epoch: 3 step: 189, loss is 0.17219293117523193\n",
      "epoch: 3 step: 190, loss is 0.24017062783241272\n",
      "epoch: 3 step: 191, loss is 0.2558009624481201\n",
      "epoch: 3 step: 192, loss is 0.338232159614563\n",
      "epoch: 3 step: 193, loss is 0.1633923053741455\n",
      "epoch: 3 step: 194, loss is 0.25811970233917236\n",
      "epoch: 3 step: 195, loss is 0.2233266830444336\n",
      "epoch: 3 step: 196, loss is 0.28227922320365906\n",
      "epoch: 3 step: 197, loss is 0.18493720889091492\n",
      "epoch: 3 step: 198, loss is 0.34576186537742615\n",
      "epoch: 3 step: 199, loss is 0.1724061667919159\n",
      "epoch: 3 step: 200, loss is 0.28236764669418335\n",
      "epoch: 3 step: 201, loss is 0.16841068863868713\n",
      "epoch: 3 step: 202, loss is 0.34203559160232544\n",
      "epoch: 3 step: 203, loss is 0.2732495665550232\n",
      "epoch: 3 step: 204, loss is 0.3081214427947998\n",
      "epoch: 3 step: 205, loss is 0.18432047963142395\n",
      "epoch: 3 step: 206, loss is 0.19688241183757782\n",
      "epoch: 3 step: 207, loss is 0.27308711409568787\n",
      "epoch: 3 step: 208, loss is 0.3307012915611267\n",
      "epoch: 3 step: 209, loss is 0.2711030840873718\n",
      "epoch: 3 step: 210, loss is 0.27629154920578003\n",
      "epoch: 3 step: 211, loss is 0.36775755882263184\n",
      "epoch: 3 step: 212, loss is 0.17802794277668\n",
      "epoch: 3 step: 213, loss is 0.29888784885406494\n",
      "epoch: 3 step: 214, loss is 0.23487728834152222\n",
      "epoch: 3 step: 215, loss is 0.11882982403039932\n",
      "epoch: 3 step: 216, loss is 0.1472271978855133\n",
      "epoch: 3 step: 217, loss is 0.10899543017148972\n",
      "epoch: 3 step: 218, loss is 0.2898537516593933\n",
      "epoch: 3 step: 219, loss is 0.1550874263048172\n",
      "epoch: 3 step: 220, loss is 0.2860749363899231\n",
      "epoch: 3 step: 221, loss is 0.22321632504463196\n",
      "epoch: 3 step: 222, loss is 0.1640186607837677\n",
      "epoch: 3 step: 223, loss is 0.28062859177589417\n",
      "epoch: 3 step: 224, loss is 0.25502845644950867\n",
      "epoch: 3 step: 225, loss is 0.18040332198143005\n",
      "epoch: 3 step: 226, loss is 0.23162990808486938\n",
      "epoch: 3 step: 227, loss is 0.20997147262096405\n",
      "epoch: 3 step: 228, loss is 0.38659995794296265\n",
      "epoch: 3 step: 229, loss is 0.11880114674568176\n",
      "epoch: 3 step: 230, loss is 0.2819758951663971\n",
      "epoch: 3 step: 231, loss is 0.19202545285224915\n",
      "epoch: 3 step: 232, loss is 0.41237911581993103\n",
      "epoch: 3 step: 233, loss is 0.2518981397151947\n",
      "epoch: 3 step: 234, loss is 0.17647039890289307\n",
      "epoch: 3 step: 235, loss is 0.20773357152938843\n",
      "epoch: 3 step: 236, loss is 0.15373705327510834\n",
      "epoch: 3 step: 237, loss is 0.19552400708198547\n",
      "epoch: 3 step: 238, loss is 0.1999356746673584\n",
      "epoch: 3 step: 239, loss is 0.4633485972881317\n",
      "epoch: 3 step: 240, loss is 0.3121190667152405\n",
      "epoch: 3 step: 241, loss is 0.3200194835662842\n",
      "epoch: 3 step: 242, loss is 0.3672780394554138\n",
      "epoch: 3 step: 243, loss is 0.1858144849538803\n",
      "epoch: 3 step: 244, loss is 0.09491607546806335\n",
      "epoch: 3 step: 245, loss is 0.16689983010292053\n",
      "epoch: 3 step: 246, loss is 0.2512664198875427\n",
      "epoch: 3 step: 247, loss is 0.3374318778514862\n",
      "epoch: 3 step: 248, loss is 0.17645221948623657\n",
      "epoch: 3 step: 249, loss is 0.3344872295856476\n",
      "epoch: 3 step: 250, loss is 0.15532144904136658\n",
      "epoch: 3 step: 251, loss is 0.16715523600578308\n",
      "epoch: 3 step: 252, loss is 0.34158846735954285\n",
      "epoch: 3 step: 253, loss is 0.2989383935928345\n",
      "epoch: 3 step: 254, loss is 0.0853634774684906\n",
      "epoch: 3 step: 255, loss is 0.23622751235961914\n",
      "epoch: 3 step: 256, loss is 0.34256410598754883\n",
      "epoch: 3 step: 257, loss is 0.3181183338165283\n",
      "epoch: 3 step: 258, loss is 0.539236843585968\n",
      "epoch: 3 step: 259, loss is 0.3808249831199646\n",
      "epoch: 3 step: 260, loss is 0.19492769241333008\n",
      "epoch: 3 step: 261, loss is 0.18708351254463196\n",
      "epoch: 3 step: 262, loss is 0.2465181201696396\n",
      "epoch: 3 step: 263, loss is 0.18753847479820251\n",
      "epoch: 3 step: 264, loss is 0.24034054577350616\n",
      "epoch: 3 step: 265, loss is 0.35326236486434937\n",
      "epoch: 3 step: 266, loss is 0.2936655580997467\n",
      "epoch: 3 step: 267, loss is 0.2560288906097412\n",
      "epoch: 3 step: 268, loss is 0.20241692662239075\n",
      "epoch: 3 step: 269, loss is 0.2360047549009323\n",
      "epoch: 3 step: 270, loss is 0.4216591715812683\n",
      "epoch: 3 step: 271, loss is 0.26125070452690125\n",
      "epoch: 3 step: 272, loss is 0.19464728236198425\n",
      "epoch: 3 step: 273, loss is 0.18188047409057617\n",
      "epoch: 3 step: 274, loss is 0.18540972471237183\n",
      "epoch: 3 step: 275, loss is 0.1922680288553238\n",
      "epoch: 3 step: 276, loss is 0.18107876181602478\n",
      "epoch: 3 step: 277, loss is 0.2587110102176666\n",
      "epoch: 3 step: 278, loss is 0.14973045885562897\n",
      "epoch: 3 step: 279, loss is 0.17089545726776123\n",
      "epoch: 3 step: 280, loss is 0.26088181138038635\n",
      "epoch: 3 step: 281, loss is 0.23202234506607056\n",
      "epoch: 3 step: 282, loss is 0.3106386065483093\n",
      "epoch: 3 step: 283, loss is 0.1425992250442505\n",
      "epoch: 3 step: 284, loss is 0.4046793580055237\n",
      "epoch: 3 step: 285, loss is 0.147504523396492\n",
      "epoch: 3 step: 286, loss is 0.3569774627685547\n",
      "epoch: 3 step: 287, loss is 0.35915032029151917\n",
      "epoch: 3 step: 288, loss is 0.3050292432308197\n",
      "epoch: 3 step: 289, loss is 0.2893470525741577\n",
      "epoch: 3 step: 290, loss is 0.18584367632865906\n",
      "epoch: 3 step: 291, loss is 0.4159461259841919\n",
      "epoch: 3 step: 292, loss is 0.16458122432231903\n",
      "epoch: 3 step: 293, loss is 0.10520011186599731\n",
      "epoch: 3 step: 294, loss is 0.2837992012500763\n",
      "epoch: 3 step: 295, loss is 0.11888899654150009\n",
      "epoch: 3 step: 296, loss is 0.3440678119659424\n",
      "epoch: 3 step: 297, loss is 0.21472033858299255\n",
      "epoch: 3 step: 298, loss is 0.2197217047214508\n",
      "epoch: 3 step: 299, loss is 0.2390134483575821\n",
      "epoch: 3 step: 300, loss is 0.13549475371837616\n",
      "epoch: 3 step: 301, loss is 0.25912946462631226\n",
      "epoch: 3 step: 302, loss is 0.27624058723449707\n",
      "epoch: 3 step: 303, loss is 0.26937517523765564\n",
      "epoch: 3 step: 304, loss is 0.3951750099658966\n",
      "epoch: 3 step: 305, loss is 0.2352684885263443\n",
      "epoch: 3 step: 306, loss is 0.3393198251724243\n",
      "epoch: 3 step: 307, loss is 0.3353913724422455\n",
      "epoch: 3 step: 308, loss is 0.2507767081260681\n",
      "epoch: 3 step: 309, loss is 0.3519582748413086\n",
      "epoch: 3 step: 310, loss is 0.176620751619339\n",
      "epoch: 3 step: 311, loss is 0.28503578901290894\n",
      "epoch: 3 step: 312, loss is 0.29193753004074097\n",
      "epoch: 3 step: 313, loss is 0.2599888741970062\n",
      "epoch: 3 step: 314, loss is 0.12124165147542953\n",
      "epoch: 3 step: 315, loss is 0.2847854793071747\n",
      "epoch: 3 step: 316, loss is 0.14149110019207\n",
      "epoch: 3 step: 317, loss is 0.12262864410877228\n",
      "epoch: 3 step: 318, loss is 0.16009874641895294\n",
      "epoch: 3 step: 319, loss is 0.2695605158805847\n",
      "epoch: 3 step: 320, loss is 0.2050171196460724\n",
      "epoch: 3 step: 321, loss is 0.29240506887435913\n",
      "epoch: 3 step: 322, loss is 0.3853827714920044\n",
      "epoch: 3 step: 323, loss is 0.0878014862537384\n",
      "epoch: 3 step: 324, loss is 0.20377641916275024\n",
      "epoch: 3 step: 325, loss is 0.16839613020420074\n",
      "epoch: 3 step: 326, loss is 0.1763816475868225\n",
      "epoch: 3 step: 327, loss is 0.2816585302352905\n",
      "epoch: 3 step: 328, loss is 0.15197820961475372\n",
      "epoch: 3 step: 329, loss is 0.23442202806472778\n",
      "epoch: 3 step: 330, loss is 0.32649242877960205\n",
      "epoch: 3 step: 331, loss is 0.3903051018714905\n",
      "epoch: 3 step: 332, loss is 0.3152799904346466\n",
      "epoch: 3 step: 333, loss is 0.33285313844680786\n",
      "epoch: 3 step: 334, loss is 0.13931474089622498\n",
      "epoch: 3 step: 335, loss is 0.2863296866416931\n",
      "epoch: 3 step: 336, loss is 0.2329278290271759\n",
      "epoch: 3 step: 337, loss is 0.2683500647544861\n",
      "epoch: 3 step: 338, loss is 0.24788939952850342\n",
      "epoch: 3 step: 339, loss is 0.253371000289917\n",
      "epoch: 3 step: 340, loss is 0.1610213816165924\n",
      "epoch: 3 step: 341, loss is 0.3172546923160553\n",
      "epoch: 3 step: 342, loss is 0.22813044488430023\n",
      "epoch: 3 step: 343, loss is 0.3029967248439789\n",
      "epoch: 3 step: 344, loss is 0.21239310503005981\n",
      "epoch: 3 step: 345, loss is 0.20817044377326965\n",
      "epoch: 3 step: 346, loss is 0.20712068676948547\n",
      "epoch: 3 step: 347, loss is 0.2502138614654541\n",
      "epoch: 3 step: 348, loss is 0.23814934492111206\n",
      "epoch: 3 step: 349, loss is 0.28358975052833557\n",
      "epoch: 3 step: 350, loss is 0.1255876123905182\n",
      "epoch: 3 step: 351, loss is 0.32256293296813965\n",
      "epoch: 3 step: 352, loss is 0.14582018554210663\n",
      "epoch: 3 step: 353, loss is 0.257437527179718\n",
      "epoch: 3 step: 354, loss is 0.17617063224315643\n",
      "epoch: 3 step: 355, loss is 0.17766234278678894\n",
      "epoch: 3 step: 356, loss is 0.3608272075653076\n",
      "epoch: 3 step: 357, loss is 0.17141294479370117\n",
      "epoch: 3 step: 358, loss is 0.21905529499053955\n",
      "epoch: 3 step: 359, loss is 0.23921945691108704\n",
      "epoch: 3 step: 360, loss is 0.49275389313697815\n",
      "epoch: 3 step: 361, loss is 0.11179165542125702\n",
      "epoch: 3 step: 362, loss is 0.3212985396385193\n",
      "epoch: 3 step: 363, loss is 0.2000158578157425\n",
      "epoch: 3 step: 364, loss is 0.2907348871231079\n",
      "epoch: 3 step: 365, loss is 0.141880065202713\n",
      "epoch: 3 step: 366, loss is 0.37684014439582825\n",
      "epoch: 3 step: 367, loss is 0.27388274669647217\n",
      "epoch: 3 step: 368, loss is 0.15794852375984192\n",
      "epoch: 3 step: 369, loss is 0.21662844717502594\n",
      "epoch: 3 step: 370, loss is 0.16751091182231903\n",
      "epoch: 3 step: 371, loss is 0.2066890001296997\n",
      "epoch: 3 step: 372, loss is 0.2220609486103058\n",
      "epoch: 3 step: 373, loss is 0.19210779666900635\n",
      "epoch: 3 step: 374, loss is 0.2936827540397644\n",
      "epoch: 3 step: 375, loss is 0.2374563366174698\n",
      "epoch: 3 step: 376, loss is 0.21095213294029236\n",
      "epoch: 3 step: 377, loss is 0.17263975739479065\n",
      "epoch: 3 step: 378, loss is 0.20423738658428192\n",
      "epoch: 3 step: 379, loss is 0.2924782633781433\n",
      "epoch: 3 step: 380, loss is 0.374735563993454\n",
      "epoch: 3 step: 381, loss is 0.26494601368904114\n",
      "epoch: 3 step: 382, loss is 0.1988302320241928\n",
      "epoch: 3 step: 383, loss is 0.21664854884147644\n",
      "epoch: 3 step: 384, loss is 0.27150747179985046\n",
      "epoch: 3 step: 385, loss is 0.10508623719215393\n",
      "epoch: 3 step: 386, loss is 0.2916012406349182\n",
      "epoch: 3 step: 387, loss is 0.3295997679233551\n",
      "epoch: 3 step: 388, loss is 0.18722489476203918\n",
      "epoch: 3 step: 389, loss is 0.16197876632213593\n",
      "epoch: 3 step: 390, loss is 0.22586321830749512\n",
      "epoch: 3 step: 391, loss is 0.24383515119552612\n",
      "epoch: 3 step: 392, loss is 0.21303266286849976\n",
      "epoch: 3 step: 393, loss is 0.18168312311172485\n",
      "epoch: 3 step: 394, loss is 0.3220313787460327\n",
      "epoch: 3 step: 395, loss is 0.21850872039794922\n",
      "epoch: 3 step: 396, loss is 0.20560088753700256\n",
      "epoch: 3 step: 397, loss is 0.3730233907699585\n",
      "epoch: 3 step: 398, loss is 0.1537400186061859\n",
      "epoch: 3 step: 399, loss is 0.15967227518558502\n",
      "epoch: 3 step: 400, loss is 0.2049253284931183\n",
      "epoch: 3 step: 401, loss is 0.243928924202919\n",
      "epoch: 3 step: 402, loss is 0.10800457000732422\n",
      "epoch: 3 step: 403, loss is 0.21680819988250732\n",
      "epoch: 3 step: 404, loss is 0.22985070943832397\n",
      "epoch: 3 step: 405, loss is 0.2506050765514374\n",
      "epoch: 3 step: 406, loss is 0.26854196190834045\n",
      "epoch: 3 step: 407, loss is 0.18633714318275452\n",
      "epoch: 3 step: 408, loss is 0.32280632853507996\n",
      "epoch: 3 step: 409, loss is 0.3056678771972656\n",
      "epoch: 3 step: 410, loss is 0.1884179711341858\n",
      "epoch: 3 step: 411, loss is 0.22976896166801453\n",
      "epoch: 3 step: 412, loss is 0.24038110673427582\n",
      "epoch: 3 step: 413, loss is 0.2440696805715561\n",
      "epoch: 3 step: 414, loss is 0.25198936462402344\n",
      "epoch: 3 step: 415, loss is 0.18073982000350952\n",
      "epoch: 3 step: 416, loss is 0.30322757363319397\n",
      "epoch: 3 step: 417, loss is 0.16233676671981812\n",
      "epoch: 3 step: 418, loss is 0.26032930612564087\n",
      "epoch: 3 step: 419, loss is 0.1711675524711609\n",
      "epoch: 3 step: 420, loss is 0.233170285820961\n",
      "epoch: 3 step: 421, loss is 0.21619120240211487\n",
      "epoch: 3 step: 422, loss is 0.1471458077430725\n",
      "epoch: 3 step: 423, loss is 0.23693645000457764\n",
      "epoch: 3 step: 424, loss is 0.2346469759941101\n",
      "epoch: 3 step: 425, loss is 0.2959538698196411\n",
      "epoch: 3 step: 426, loss is 0.09822104871273041\n",
      "epoch: 3 step: 427, loss is 0.2245766520500183\n",
      "epoch: 3 step: 428, loss is 0.28094056248664856\n",
      "epoch: 3 step: 429, loss is 0.5318318605422974\n",
      "epoch: 3 step: 430, loss is 0.15234050154685974\n",
      "epoch: 3 step: 431, loss is 0.2391291856765747\n",
      "epoch: 3 step: 432, loss is 0.17758601903915405\n",
      "epoch: 3 step: 433, loss is 0.06045772135257721\n",
      "epoch: 3 step: 434, loss is 0.27513638138771057\n",
      "epoch: 3 step: 435, loss is 0.30141186714172363\n",
      "epoch: 3 step: 436, loss is 0.38606318831443787\n",
      "epoch: 3 step: 437, loss is 0.2584683299064636\n",
      "epoch: 3 step: 438, loss is 0.19605672359466553\n",
      "epoch: 3 step: 439, loss is 0.25095513463020325\n",
      "epoch: 3 step: 440, loss is 0.16945360600948334\n",
      "epoch: 3 step: 441, loss is 0.2312028706073761\n",
      "epoch: 3 step: 442, loss is 0.14800427854061127\n",
      "epoch: 3 step: 443, loss is 0.17244257032871246\n",
      "epoch: 3 step: 444, loss is 0.12029191851615906\n",
      "epoch: 3 step: 445, loss is 0.23650409281253815\n",
      "epoch: 3 step: 446, loss is 0.15864738821983337\n",
      "epoch: 3 step: 447, loss is 0.36933812499046326\n",
      "epoch: 3 step: 448, loss is 0.1626531183719635\n",
      "epoch: 3 step: 449, loss is 0.26040008664131165\n",
      "epoch: 3 step: 450, loss is 0.15441404283046722\n",
      "epoch: 3 step: 451, loss is 0.33747509121894836\n",
      "epoch: 3 step: 452, loss is 0.291837602853775\n",
      "epoch: 3 step: 453, loss is 0.2113165408372879\n",
      "epoch: 3 step: 454, loss is 0.10755687952041626\n",
      "epoch: 3 step: 455, loss is 0.40242183208465576\n",
      "epoch: 3 step: 456, loss is 0.2230074107646942\n",
      "epoch: 3 step: 457, loss is 0.12209340929985046\n",
      "epoch: 3 step: 458, loss is 0.29690366983413696\n",
      "epoch: 3 step: 459, loss is 0.27311038970947266\n",
      "epoch: 3 step: 460, loss is 0.13741552829742432\n",
      "epoch: 3 step: 461, loss is 0.1801472008228302\n",
      "epoch: 3 step: 462, loss is 0.25203490257263184\n",
      "epoch: 3 step: 463, loss is 0.2657921314239502\n",
      "epoch: 3 step: 464, loss is 0.20823432505130768\n",
      "epoch: 3 step: 465, loss is 0.1519639790058136\n",
      "epoch: 3 step: 466, loss is 0.2765234410762787\n",
      "epoch: 3 step: 467, loss is 0.2897310256958008\n",
      "epoch: 3 step: 468, loss is 0.3316386044025421\n",
      "epoch: 3 step: 469, loss is 0.3305017352104187\n",
      "epoch: 3 step: 470, loss is 0.12477755546569824\n",
      "epoch: 3 step: 471, loss is 0.17014554142951965\n",
      "epoch: 3 step: 472, loss is 0.2088341861963272\n",
      "epoch: 3 step: 473, loss is 0.1642007827758789\n",
      "epoch: 3 step: 474, loss is 0.13270804286003113\n",
      "epoch: 3 step: 475, loss is 0.25430822372436523\n",
      "epoch: 3 step: 476, loss is 0.17840799689292908\n",
      "epoch: 3 step: 477, loss is 0.25422966480255127\n",
      "epoch: 3 step: 478, loss is 0.2977357506752014\n",
      "epoch: 3 step: 479, loss is 0.19228756427764893\n",
      "epoch: 3 step: 480, loss is 0.14357365667819977\n",
      "epoch: 3 step: 481, loss is 0.18240977823734283\n",
      "epoch: 3 step: 482, loss is 0.20993246138095856\n",
      "epoch: 3 step: 483, loss is 0.2352311909198761\n",
      "epoch: 3 step: 484, loss is 0.12796178460121155\n",
      "epoch: 3 step: 485, loss is 0.3281638026237488\n",
      "epoch: 3 step: 486, loss is 0.2365133911371231\n",
      "epoch: 3 step: 487, loss is 0.42666274309158325\n",
      "epoch: 3 step: 488, loss is 0.18764057755470276\n",
      "epoch: 3 step: 489, loss is 0.32090699672698975\n",
      "epoch: 3 step: 490, loss is 0.16168001294136047\n",
      "epoch: 3 step: 491, loss is 0.45773470401763916\n",
      "epoch: 3 step: 492, loss is 0.27498388290405273\n",
      "epoch: 3 step: 493, loss is 0.2380780130624771\n",
      "epoch: 3 step: 494, loss is 0.15819275379180908\n",
      "epoch: 3 step: 495, loss is 0.36740201711654663\n",
      "epoch: 3 step: 496, loss is 0.24983380734920502\n",
      "epoch: 3 step: 497, loss is 0.11855411529541016\n",
      "epoch: 3 step: 498, loss is 0.39523905515670776\n",
      "epoch: 3 step: 499, loss is 0.3427157402038574\n",
      "epoch: 3 step: 500, loss is 0.09254242479801178\n",
      "epoch: 3 step: 501, loss is 0.16397453844547272\n",
      "epoch: 3 step: 502, loss is 0.3604868948459625\n",
      "epoch: 3 step: 503, loss is 0.13249467313289642\n",
      "epoch: 3 step: 504, loss is 0.1792806088924408\n",
      "epoch: 3 step: 505, loss is 0.2521171271800995\n",
      "epoch: 3 step: 506, loss is 0.40612906217575073\n",
      "epoch: 3 step: 507, loss is 0.25805115699768066\n",
      "epoch: 3 step: 508, loss is 0.10885782539844513\n",
      "epoch: 3 step: 509, loss is 0.16498996317386627\n",
      "epoch: 3 step: 510, loss is 0.3164326548576355\n",
      "epoch: 3 step: 511, loss is 0.3150785565376282\n",
      "epoch: 3 step: 512, loss is 0.18878482282161713\n",
      "epoch: 3 step: 513, loss is 0.16242751479148865\n",
      "epoch: 3 step: 514, loss is 0.2743203341960907\n",
      "epoch: 3 step: 515, loss is 0.1811142861843109\n",
      "epoch: 3 step: 516, loss is 0.2615636885166168\n",
      "epoch: 3 step: 517, loss is 0.2553493082523346\n",
      "epoch: 3 step: 518, loss is 0.14390136301517487\n",
      "epoch: 3 step: 519, loss is 0.24063047766685486\n",
      "epoch: 3 step: 520, loss is 0.18362367153167725\n",
      "epoch: 3 step: 521, loss is 0.41730359196662903\n",
      "epoch: 3 step: 522, loss is 0.37933093309402466\n",
      "epoch: 3 step: 523, loss is 0.2654510736465454\n",
      "epoch: 3 step: 524, loss is 0.2754533588886261\n",
      "epoch: 3 step: 525, loss is 0.13271744549274445\n",
      "epoch: 3 step: 526, loss is 0.24713324010372162\n",
      "epoch: 3 step: 527, loss is 0.18661129474639893\n",
      "epoch: 3 step: 528, loss is 0.2992585599422455\n",
      "epoch: 3 step: 529, loss is 0.2987222671508789\n",
      "epoch: 3 step: 530, loss is 0.29451337456703186\n",
      "epoch: 3 step: 531, loss is 0.1701970398426056\n",
      "epoch: 3 step: 532, loss is 0.262569785118103\n",
      "epoch: 3 step: 533, loss is 0.16869649291038513\n",
      "epoch: 3 step: 534, loss is 0.2690942585468292\n",
      "epoch: 3 step: 535, loss is 0.23244673013687134\n",
      "epoch: 3 step: 536, loss is 0.20800991356372833\n",
      "epoch: 3 step: 537, loss is 0.12881135940551758\n",
      "epoch: 3 step: 538, loss is 0.22556456923484802\n",
      "epoch: 3 step: 539, loss is 0.2052270472049713\n",
      "epoch: 3 step: 540, loss is 0.43824341893196106\n",
      "epoch: 3 step: 541, loss is 0.23806767165660858\n",
      "epoch: 3 step: 542, loss is 0.16658811271190643\n",
      "epoch: 3 step: 543, loss is 0.1880182921886444\n",
      "epoch: 3 step: 544, loss is 0.3112177848815918\n",
      "epoch: 3 step: 545, loss is 0.2900131642818451\n",
      "epoch: 3 step: 546, loss is 0.15996244549751282\n",
      "epoch: 3 step: 547, loss is 0.31987255811691284\n",
      "epoch: 3 step: 548, loss is 0.23055978119373322\n",
      "epoch: 3 step: 549, loss is 0.2881262004375458\n",
      "epoch: 3 step: 550, loss is 0.21257075667381287\n",
      "epoch: 3 step: 551, loss is 0.16870228946208954\n",
      "epoch: 3 step: 552, loss is 0.21997475624084473\n",
      "epoch: 3 step: 553, loss is 0.2291233241558075\n",
      "epoch: 3 step: 554, loss is 0.26273244619369507\n",
      "epoch: 3 step: 555, loss is 0.2204892933368683\n",
      "epoch: 3 step: 556, loss is 0.1580570936203003\n",
      "epoch: 3 step: 557, loss is 0.5064805746078491\n",
      "epoch: 3 step: 558, loss is 0.4123275876045227\n",
      "epoch: 3 step: 559, loss is 0.23817816376686096\n",
      "epoch: 3 step: 560, loss is 0.3337417244911194\n",
      "epoch: 3 step: 561, loss is 0.3001807928085327\n",
      "epoch: 3 step: 562, loss is 0.16643759608268738\n",
      "epoch: 3 step: 563, loss is 0.07803606241941452\n",
      "epoch: 3 step: 564, loss is 0.19309203326702118\n",
      "epoch: 3 step: 565, loss is 0.15646803379058838\n",
      "epoch: 3 step: 566, loss is 0.16215312480926514\n",
      "epoch: 3 step: 567, loss is 0.3112946152687073\n",
      "epoch: 3 step: 568, loss is 0.17218168079853058\n",
      "epoch: 3 step: 569, loss is 0.1474270075559616\n",
      "epoch: 3 step: 570, loss is 0.24531133472919464\n",
      "epoch: 3 step: 571, loss is 0.18469315767288208\n",
      "epoch: 3 step: 572, loss is 0.3176853656768799\n",
      "epoch: 3 step: 573, loss is 0.2576669752597809\n",
      "epoch: 3 step: 574, loss is 0.42394259572029114\n",
      "epoch: 3 step: 575, loss is 0.2837904095649719\n",
      "epoch: 3 step: 576, loss is 0.165536567568779\n",
      "epoch: 3 step: 577, loss is 0.20435895025730133\n",
      "epoch: 3 step: 578, loss is 0.22604599595069885\n",
      "epoch: 3 step: 579, loss is 0.19351701438426971\n",
      "epoch: 3 step: 580, loss is 0.2550145983695984\n",
      "epoch: 3 step: 581, loss is 0.1584164947271347\n",
      "epoch: 3 step: 582, loss is 0.12851423025131226\n",
      "epoch: 3 step: 583, loss is 0.2405398041009903\n",
      "epoch: 3 step: 584, loss is 0.2786213755607605\n",
      "epoch: 3 step: 585, loss is 0.15684090554714203\n",
      "epoch: 3 step: 586, loss is 0.17557533085346222\n",
      "epoch: 3 step: 587, loss is 0.2450244128704071\n",
      "epoch: 3 step: 588, loss is 0.24446725845336914\n",
      "epoch: 3 step: 589, loss is 0.35834193229675293\n",
      "epoch: 3 step: 590, loss is 0.2805323004722595\n",
      "epoch: 3 step: 591, loss is 0.21170300245285034\n",
      "epoch: 3 step: 592, loss is 0.26084190607070923\n",
      "epoch: 3 step: 593, loss is 0.27884572744369507\n",
      "epoch: 3 step: 594, loss is 0.275534063577652\n",
      "epoch: 3 step: 595, loss is 0.34459012746810913\n",
      "epoch: 3 step: 596, loss is 0.3721415400505066\n",
      "epoch: 3 step: 597, loss is 0.21425354480743408\n",
      "epoch: 3 step: 598, loss is 0.16665402054786682\n",
      "epoch: 3 step: 599, loss is 0.17928579449653625\n",
      "epoch: 3 step: 600, loss is 0.2416067123413086\n",
      "epoch: 3 step: 601, loss is 0.2618076503276825\n",
      "epoch: 3 step: 602, loss is 0.1979876458644867\n",
      "epoch: 3 step: 603, loss is 0.2676573693752289\n",
      "epoch: 3 step: 604, loss is 0.11339354515075684\n",
      "epoch: 3 step: 605, loss is 0.284881055355072\n",
      "epoch: 3 step: 606, loss is 0.2603045105934143\n",
      "epoch: 3 step: 607, loss is 0.18259559571743011\n",
      "epoch: 3 step: 608, loss is 0.3823145627975464\n",
      "epoch: 3 step: 609, loss is 0.23494915664196014\n",
      "epoch: 3 step: 610, loss is 0.1785925030708313\n",
      "epoch: 3 step: 611, loss is 0.22520668804645538\n",
      "epoch: 3 step: 612, loss is 0.09898923337459564\n",
      "epoch: 3 step: 613, loss is 0.22034242749214172\n",
      "epoch: 3 step: 614, loss is 0.22141709923744202\n",
      "epoch: 3 step: 615, loss is 0.1209513247013092\n",
      "epoch: 3 step: 616, loss is 0.10660582780838013\n",
      "epoch: 3 step: 617, loss is 0.08365534245967865\n",
      "epoch: 3 step: 618, loss is 0.3138464391231537\n",
      "epoch: 3 step: 619, loss is 0.28237295150756836\n",
      "epoch: 3 step: 620, loss is 0.1400119662284851\n",
      "epoch: 3 step: 621, loss is 0.2892775535583496\n",
      "epoch: 3 step: 622, loss is 0.18685275316238403\n",
      "epoch: 3 step: 623, loss is 0.20752473175525665\n",
      "epoch: 3 step: 624, loss is 0.24465464055538177\n",
      "epoch: 3 step: 625, loss is 0.1485687792301178\n",
      "epoch: 3 step: 626, loss is 0.1688738465309143\n",
      "epoch: 3 step: 627, loss is 0.22720614075660706\n",
      "epoch: 3 step: 628, loss is 0.14940184354782104\n",
      "epoch: 3 step: 629, loss is 0.15000414848327637\n",
      "epoch: 3 step: 630, loss is 0.19923776388168335\n",
      "epoch: 3 step: 631, loss is 0.2858455777168274\n",
      "epoch: 3 step: 632, loss is 0.23582226037979126\n",
      "epoch: 3 step: 633, loss is 0.2955150604248047\n",
      "epoch: 3 step: 634, loss is 0.10138244926929474\n",
      "epoch: 3 step: 635, loss is 0.14919334650039673\n",
      "epoch: 3 step: 636, loss is 0.36205440759658813\n",
      "epoch: 3 step: 637, loss is 0.13480105996131897\n",
      "epoch: 3 step: 638, loss is 0.10921043902635574\n",
      "epoch: 3 step: 639, loss is 0.13223797082901\n",
      "epoch: 3 step: 640, loss is 0.14626607298851013\n",
      "epoch: 3 step: 641, loss is 0.3446893095970154\n",
      "epoch: 3 step: 642, loss is 0.0893380269408226\n",
      "epoch: 3 step: 643, loss is 0.08863621950149536\n",
      "epoch: 3 step: 644, loss is 0.13991791009902954\n",
      "epoch: 3 step: 645, loss is 0.27433669567108154\n",
      "epoch: 3 step: 646, loss is 0.21722348034381866\n",
      "epoch: 3 step: 647, loss is 0.25993669033050537\n",
      "epoch: 3 step: 648, loss is 0.24569514393806458\n",
      "epoch: 3 step: 649, loss is 0.37366199493408203\n",
      "epoch: 3 step: 650, loss is 0.21540513634681702\n",
      "epoch: 3 step: 651, loss is 0.22166112065315247\n",
      "epoch: 3 step: 652, loss is 0.2819537818431854\n",
      "epoch: 3 step: 653, loss is 0.17447508871555328\n",
      "epoch: 3 step: 654, loss is 0.10366254299879074\n",
      "epoch: 3 step: 655, loss is 0.2477598488330841\n",
      "epoch: 3 step: 656, loss is 0.18846751749515533\n",
      "epoch: 3 step: 657, loss is 0.2628801167011261\n",
      "epoch: 3 step: 658, loss is 0.22257298231124878\n",
      "epoch: 3 step: 659, loss is 0.47570013999938965\n",
      "epoch: 3 step: 660, loss is 0.23432214558124542\n",
      "epoch: 3 step: 661, loss is 0.3184702396392822\n",
      "epoch: 3 step: 662, loss is 0.18355244398117065\n",
      "epoch: 3 step: 663, loss is 0.18436691164970398\n",
      "epoch: 3 step: 664, loss is 0.27002936601638794\n",
      "epoch: 3 step: 665, loss is 0.29041406512260437\n",
      "epoch: 3 step: 666, loss is 0.31730854511260986\n",
      "epoch: 3 step: 667, loss is 0.29100218415260315\n",
      "epoch: 3 step: 668, loss is 0.2920335531234741\n",
      "epoch: 3 step: 669, loss is 0.26272180676460266\n",
      "epoch: 3 step: 670, loss is 0.2878279685974121\n",
      "epoch: 3 step: 671, loss is 0.24096570909023285\n",
      "epoch: 3 step: 672, loss is 0.2711508870124817\n",
      "epoch: 3 step: 673, loss is 0.18624411523342133\n",
      "epoch: 3 step: 674, loss is 0.3280906081199646\n",
      "epoch: 3 step: 675, loss is 0.2691664397716522\n",
      "epoch: 3 step: 676, loss is 0.16630300879478455\n",
      "epoch: 3 step: 677, loss is 0.45011982321739197\n",
      "epoch: 3 step: 678, loss is 0.24911753833293915\n",
      "epoch: 3 step: 679, loss is 0.46697601675987244\n",
      "epoch: 3 step: 680, loss is 0.23011738061904907\n",
      "epoch: 3 step: 681, loss is 0.5014863014221191\n",
      "epoch: 3 step: 682, loss is 0.11931419372558594\n",
      "epoch: 3 step: 683, loss is 0.1819327473640442\n",
      "epoch: 3 step: 684, loss is 0.1697317361831665\n",
      "epoch: 3 step: 685, loss is 0.25257426500320435\n",
      "epoch: 3 step: 686, loss is 0.23857204616069794\n",
      "epoch: 3 step: 687, loss is 0.27994516491889954\n",
      "epoch: 3 step: 688, loss is 0.31227779388427734\n",
      "epoch: 3 step: 689, loss is 0.17114946246147156\n",
      "epoch: 3 step: 690, loss is 0.11376238614320755\n",
      "epoch: 3 step: 691, loss is 0.2910943031311035\n",
      "epoch: 3 step: 692, loss is 0.26431921124458313\n",
      "epoch: 3 step: 693, loss is 0.23296122252941132\n",
      "epoch: 3 step: 694, loss is 0.2862255573272705\n",
      "epoch: 3 step: 695, loss is 0.3976091146469116\n",
      "epoch: 3 step: 696, loss is 0.17420023679733276\n",
      "epoch: 3 step: 697, loss is 0.08195260167121887\n",
      "epoch: 3 step: 698, loss is 0.1759372502565384\n",
      "epoch: 3 step: 699, loss is 0.3289763331413269\n",
      "epoch: 3 step: 700, loss is 0.23339787125587463\n",
      "epoch: 3 step: 701, loss is 0.285121351480484\n",
      "epoch: 3 step: 702, loss is 0.314836323261261\n",
      "epoch: 3 step: 703, loss is 0.3293553590774536\n",
      "epoch: 3 step: 704, loss is 0.09692051261663437\n",
      "epoch: 3 step: 705, loss is 0.10897945612668991\n",
      "epoch: 3 step: 706, loss is 0.16878004372119904\n",
      "epoch: 3 step: 707, loss is 0.2649421691894531\n",
      "epoch: 3 step: 708, loss is 0.29313600063323975\n",
      "epoch: 3 step: 709, loss is 0.20884141325950623\n",
      "epoch: 3 step: 710, loss is 0.17985424399375916\n",
      "epoch: 3 step: 711, loss is 0.1750514656305313\n",
      "epoch: 3 step: 712, loss is 0.07227733731269836\n",
      "epoch: 3 step: 713, loss is 0.15384086966514587\n",
      "epoch: 3 step: 714, loss is 0.2737831473350525\n",
      "epoch: 3 step: 715, loss is 0.6471443176269531\n",
      "epoch: 3 step: 716, loss is 0.5387181639671326\n",
      "epoch: 3 step: 717, loss is 0.2746449112892151\n",
      "epoch: 3 step: 718, loss is 0.26525717973709106\n",
      "epoch: 3 step: 719, loss is 0.3184847831726074\n",
      "epoch: 3 step: 720, loss is 0.454437792301178\n",
      "epoch: 3 step: 721, loss is 0.16127921640872955\n",
      "epoch: 3 step: 722, loss is 0.2670595347881317\n",
      "epoch: 3 step: 723, loss is 0.18182986974716187\n",
      "epoch: 3 step: 724, loss is 0.19888372719287872\n",
      "epoch: 3 step: 725, loss is 0.2814187705516815\n",
      "epoch: 3 step: 726, loss is 0.3230898678302765\n",
      "epoch: 3 step: 727, loss is 0.1873345971107483\n",
      "epoch: 3 step: 728, loss is 0.30265769362449646\n",
      "epoch: 3 step: 729, loss is 0.21486300230026245\n",
      "epoch: 3 step: 730, loss is 0.10441111028194427\n",
      "epoch: 3 step: 731, loss is 0.2398601472377777\n",
      "epoch: 3 step: 732, loss is 0.23148305714130402\n",
      "epoch: 3 step: 733, loss is 0.36295878887176514\n",
      "epoch: 3 step: 734, loss is 0.23853959143161774\n",
      "epoch: 3 step: 735, loss is 0.13361304998397827\n",
      "epoch: 3 step: 736, loss is 0.3300604820251465\n",
      "epoch: 3 step: 737, loss is 0.20957230031490326\n",
      "epoch: 3 step: 738, loss is 0.27037832140922546\n",
      "epoch: 3 step: 739, loss is 0.27078086137771606\n",
      "epoch: 3 step: 740, loss is 0.19232526421546936\n",
      "epoch: 3 step: 741, loss is 0.2616780400276184\n",
      "epoch: 3 step: 742, loss is 0.09590566158294678\n",
      "epoch: 3 step: 743, loss is 0.12390722334384918\n",
      "epoch: 3 step: 744, loss is 0.18000692129135132\n",
      "epoch: 3 step: 745, loss is 0.13916060328483582\n",
      "epoch: 3 step: 746, loss is 0.21194875240325928\n",
      "epoch: 3 step: 747, loss is 0.32110396027565\n",
      "epoch: 3 step: 748, loss is 0.2378203421831131\n",
      "epoch: 3 step: 749, loss is 0.06272386014461517\n",
      "epoch: 3 step: 750, loss is 0.34827739000320435\n",
      "epoch: 3 step: 751, loss is 0.260159432888031\n",
      "epoch: 3 step: 752, loss is 0.11201928555965424\n",
      "epoch: 3 step: 753, loss is 0.10586245357990265\n",
      "epoch: 3 step: 754, loss is 0.11881336569786072\n",
      "epoch: 3 step: 755, loss is 0.12031261622905731\n",
      "epoch: 3 step: 756, loss is 0.34072253108024597\n",
      "epoch: 3 step: 757, loss is 0.0835440382361412\n",
      "epoch: 3 step: 758, loss is 0.20826712250709534\n",
      "epoch: 3 step: 759, loss is 0.24201272428035736\n",
      "epoch: 3 step: 760, loss is 0.1836378574371338\n",
      "epoch: 3 step: 761, loss is 0.09849970787763596\n",
      "epoch: 3 step: 762, loss is 0.17234189808368683\n",
      "epoch: 3 step: 763, loss is 0.2379506528377533\n",
      "epoch: 3 step: 764, loss is 0.29610294103622437\n",
      "epoch: 3 step: 765, loss is 0.15244317054748535\n",
      "epoch: 3 step: 766, loss is 0.29057958722114563\n",
      "epoch: 3 step: 767, loss is 0.2247861921787262\n",
      "epoch: 3 step: 768, loss is 0.13332243263721466\n",
      "epoch: 3 step: 769, loss is 0.3960864245891571\n",
      "epoch: 3 step: 770, loss is 0.2751878499984741\n",
      "epoch: 3 step: 771, loss is 0.1850023865699768\n",
      "epoch: 3 step: 772, loss is 0.20130038261413574\n",
      "epoch: 3 step: 773, loss is 0.21407246589660645\n",
      "epoch: 3 step: 774, loss is 0.19007469713687897\n",
      "epoch: 3 step: 775, loss is 0.24574437737464905\n",
      "epoch: 3 step: 776, loss is 0.2560823857784271\n",
      "epoch: 3 step: 777, loss is 0.19118544459342957\n",
      "epoch: 3 step: 778, loss is 0.12950366735458374\n",
      "epoch: 3 step: 779, loss is 0.25078240036964417\n",
      "epoch: 3 step: 780, loss is 0.14782287180423737\n",
      "epoch: 3 step: 781, loss is 0.429709792137146\n",
      "epoch: 3 step: 782, loss is 0.1689978837966919\n",
      "epoch: 3 step: 783, loss is 0.23100095987319946\n",
      "epoch: 3 step: 784, loss is 0.2649383842945099\n",
      "epoch: 3 step: 785, loss is 0.2868189513683319\n",
      "epoch: 3 step: 786, loss is 0.30885839462280273\n",
      "epoch: 3 step: 787, loss is 0.47806334495544434\n",
      "epoch: 3 step: 788, loss is 0.3282674252986908\n",
      "epoch: 3 step: 789, loss is 0.14810815453529358\n",
      "epoch: 3 step: 790, loss is 0.2610282003879547\n",
      "epoch: 3 step: 791, loss is 0.1534353494644165\n",
      "epoch: 3 step: 792, loss is 0.286885142326355\n",
      "epoch: 3 step: 793, loss is 0.4156051278114319\n",
      "epoch: 3 step: 794, loss is 0.21326294541358948\n",
      "epoch: 3 step: 795, loss is 0.38863179087638855\n",
      "epoch: 3 step: 796, loss is 0.15668106079101562\n",
      "epoch: 3 step: 797, loss is 0.22894474864006042\n",
      "epoch: 3 step: 798, loss is 0.23736244440078735\n",
      "epoch: 3 step: 799, loss is 0.17288519442081451\n",
      "epoch: 3 step: 800, loss is 0.1724158674478531\n",
      "epoch: 3 step: 801, loss is 0.2264399230480194\n",
      "epoch: 3 step: 802, loss is 0.13912785053253174\n",
      "epoch: 3 step: 803, loss is 0.2652081847190857\n",
      "epoch: 3 step: 804, loss is 0.3145332634449005\n",
      "epoch: 3 step: 805, loss is 0.09473752975463867\n",
      "epoch: 3 step: 806, loss is 0.15398022532463074\n",
      "epoch: 3 step: 807, loss is 0.37299883365631104\n",
      "epoch: 3 step: 808, loss is 0.17397423088550568\n",
      "epoch: 3 step: 809, loss is 0.18970899283885956\n",
      "epoch: 3 step: 810, loss is 0.10246602445840836\n",
      "epoch: 3 step: 811, loss is 0.15837593376636505\n",
      "epoch: 3 step: 812, loss is 0.30163300037384033\n",
      "epoch: 3 step: 813, loss is 0.06463482230901718\n",
      "epoch: 3 step: 814, loss is 0.2769721448421478\n",
      "epoch: 3 step: 815, loss is 0.3256087303161621\n",
      "epoch: 3 step: 816, loss is 0.21921536326408386\n",
      "epoch: 3 step: 817, loss is 0.16616949439048767\n",
      "epoch: 3 step: 818, loss is 0.33223414421081543\n",
      "epoch: 3 step: 819, loss is 0.3606211245059967\n",
      "epoch: 3 step: 820, loss is 0.1470400094985962\n",
      "epoch: 3 step: 821, loss is 0.16999374330043793\n",
      "epoch: 3 step: 822, loss is 0.33814746141433716\n",
      "epoch: 3 step: 823, loss is 0.2742679715156555\n",
      "epoch: 3 step: 824, loss is 0.26911109685897827\n",
      "epoch: 3 step: 825, loss is 0.21460920572280884\n",
      "epoch: 3 step: 826, loss is 0.2632811665534973\n",
      "epoch: 3 step: 827, loss is 0.29665568470954895\n",
      "epoch: 3 step: 828, loss is 0.4097439646720886\n",
      "epoch: 3 step: 829, loss is 0.3054026961326599\n",
      "epoch: 3 step: 830, loss is 0.2202477902173996\n",
      "epoch: 3 step: 831, loss is 0.21995539963245392\n",
      "epoch: 3 step: 832, loss is 0.24591581523418427\n",
      "epoch: 3 step: 833, loss is 0.20093092322349548\n",
      "epoch: 3 step: 834, loss is 0.19430647790431976\n",
      "epoch: 3 step: 835, loss is 0.458537757396698\n",
      "epoch: 3 step: 836, loss is 0.39680102467536926\n",
      "epoch: 3 step: 837, loss is 0.09891197085380554\n",
      "epoch: 3 step: 838, loss is 0.19823837280273438\n",
      "epoch: 3 step: 839, loss is 0.31707048416137695\n",
      "epoch: 3 step: 840, loss is 0.24662476778030396\n",
      "epoch: 3 step: 841, loss is 0.3608909547328949\n",
      "epoch: 3 step: 842, loss is 0.22233369946479797\n",
      "epoch: 3 step: 843, loss is 0.2901778221130371\n",
      "epoch: 3 step: 844, loss is 0.11008699238300323\n",
      "epoch: 3 step: 845, loss is 0.28319764137268066\n",
      "epoch: 3 step: 846, loss is 0.23867933452129364\n",
      "epoch: 3 step: 847, loss is 0.18202629685401917\n",
      "epoch: 3 step: 848, loss is 0.21011507511138916\n",
      "epoch: 3 step: 849, loss is 0.21075353026390076\n",
      "epoch: 3 step: 850, loss is 0.2765524983406067\n",
      "epoch: 3 step: 851, loss is 0.2983170747756958\n",
      "epoch: 3 step: 852, loss is 0.23732000589370728\n",
      "epoch: 3 step: 853, loss is 0.28392988443374634\n",
      "epoch: 3 step: 854, loss is 0.28660848736763\n",
      "epoch: 3 step: 855, loss is 0.1710103452205658\n",
      "epoch: 3 step: 856, loss is 0.2348274141550064\n",
      "epoch: 3 step: 857, loss is 0.23391646146774292\n",
      "epoch: 3 step: 858, loss is 0.20377054810523987\n",
      "epoch: 3 step: 859, loss is 0.09310109913349152\n",
      "epoch: 3 step: 860, loss is 0.3665441572666168\n",
      "epoch: 3 step: 861, loss is 0.23291316628456116\n",
      "epoch: 3 step: 862, loss is 0.2733681797981262\n",
      "epoch: 3 step: 863, loss is 0.3819639980792999\n",
      "epoch: 3 step: 864, loss is 0.2838177978992462\n",
      "epoch: 3 step: 865, loss is 0.08374727517366409\n",
      "epoch: 3 step: 866, loss is 0.32247698307037354\n",
      "epoch: 3 step: 867, loss is 0.23981115221977234\n",
      "epoch: 3 step: 868, loss is 0.21323370933532715\n",
      "epoch: 3 step: 869, loss is 0.22554826736450195\n",
      "epoch: 3 step: 870, loss is 0.22966903448104858\n",
      "epoch: 3 step: 871, loss is 0.24477124214172363\n",
      "epoch: 3 step: 872, loss is 0.2910633087158203\n",
      "epoch: 3 step: 873, loss is 0.18848498165607452\n",
      "epoch: 3 step: 874, loss is 0.1376897543668747\n",
      "epoch: 3 step: 875, loss is 0.21340277791023254\n",
      "epoch: 3 step: 876, loss is 0.13596756756305695\n",
      "epoch: 3 step: 877, loss is 0.21006843447685242\n",
      "epoch: 3 step: 878, loss is 0.13881222903728485\n",
      "epoch: 3 step: 879, loss is 0.40151578187942505\n",
      "epoch: 3 step: 880, loss is 0.3670905530452728\n",
      "epoch: 3 step: 881, loss is 0.1656855195760727\n",
      "epoch: 3 step: 882, loss is 0.3057735562324524\n",
      "epoch: 3 step: 883, loss is 0.23170486092567444\n",
      "epoch: 3 step: 884, loss is 0.33698779344558716\n",
      "epoch: 3 step: 885, loss is 0.1774088442325592\n",
      "epoch: 3 step: 886, loss is 0.1303299218416214\n",
      "epoch: 3 step: 887, loss is 0.23680323362350464\n",
      "epoch: 3 step: 888, loss is 0.2896515130996704\n",
      "epoch: 3 step: 889, loss is 0.22440144419670105\n",
      "epoch: 3 step: 890, loss is 0.2112220823764801\n",
      "epoch: 3 step: 891, loss is 0.253529816865921\n",
      "epoch: 3 step: 892, loss is 0.29098668694496155\n",
      "epoch: 3 step: 893, loss is 0.15577448904514313\n",
      "epoch: 3 step: 894, loss is 0.26656508445739746\n",
      "epoch: 3 step: 895, loss is 0.332832008600235\n",
      "epoch: 3 step: 896, loss is 0.1715536117553711\n",
      "epoch: 3 step: 897, loss is 0.41852807998657227\n",
      "epoch: 3 step: 898, loss is 0.1206633672118187\n",
      "epoch: 3 step: 899, loss is 0.07842449098825455\n",
      "epoch: 3 step: 900, loss is 0.17380091547966003\n",
      "epoch: 3 step: 901, loss is 0.1831829696893692\n",
      "epoch: 3 step: 902, loss is 0.3333415985107422\n",
      "epoch: 3 step: 903, loss is 0.21809889376163483\n",
      "epoch: 3 step: 904, loss is 0.36507126688957214\n",
      "epoch: 3 step: 905, loss is 0.25627803802490234\n",
      "epoch: 3 step: 906, loss is 0.10673195123672485\n",
      "epoch: 3 step: 907, loss is 0.0909048318862915\n",
      "epoch: 3 step: 908, loss is 0.2723277509212494\n",
      "epoch: 3 step: 909, loss is 0.2911096215248108\n",
      "epoch: 3 step: 910, loss is 0.11309351027011871\n",
      "epoch: 3 step: 911, loss is 0.4713169038295746\n",
      "epoch: 3 step: 912, loss is 0.2652733325958252\n",
      "epoch: 3 step: 913, loss is 0.44506192207336426\n",
      "epoch: 3 step: 914, loss is 0.12370594590902328\n",
      "epoch: 3 step: 915, loss is 0.19638052582740784\n",
      "epoch: 3 step: 916, loss is 0.29342031478881836\n",
      "epoch: 3 step: 917, loss is 0.40274882316589355\n",
      "epoch: 3 step: 918, loss is 0.2052495926618576\n",
      "epoch: 3 step: 919, loss is 0.25467127561569214\n",
      "epoch: 3 step: 920, loss is 0.2175140082836151\n",
      "epoch: 3 step: 921, loss is 0.2402018904685974\n",
      "epoch: 3 step: 922, loss is 0.2726498246192932\n",
      "epoch: 3 step: 923, loss is 0.29709592461586\n",
      "epoch: 3 step: 924, loss is 0.35103416442871094\n",
      "epoch: 3 step: 925, loss is 0.2308383285999298\n",
      "epoch: 3 step: 926, loss is 0.23740923404693604\n",
      "epoch: 3 step: 927, loss is 0.15357032418251038\n",
      "epoch: 3 step: 928, loss is 0.2687135338783264\n",
      "epoch: 3 step: 929, loss is 0.3524512052536011\n",
      "epoch: 3 step: 930, loss is 0.19958999752998352\n",
      "epoch: 3 step: 931, loss is 0.3335462510585785\n",
      "epoch: 3 step: 932, loss is 0.14028719067573547\n",
      "epoch: 3 step: 933, loss is 0.1320742666721344\n",
      "epoch: 3 step: 934, loss is 0.2608780860900879\n",
      "epoch: 3 step: 935, loss is 0.3160012364387512\n",
      "epoch: 3 step: 936, loss is 0.3925488293170929\n",
      "epoch: 3 step: 937, loss is 0.28940606117248535\n",
      "============== Evaluation for ForwardFashion ==============\n",
      "{'acc': 0.9042467948717948}\n",
      "\n",
      "--- Training with Regularization ---\n",
      "============== Starting Training for ForwardFashionRegularization ==============\n",
      "epoch: 1 step: 1, loss is 2.299605369567871\n",
      "epoch: 1 step: 2, loss is 2.276176929473877\n",
      "epoch: 1 step: 3, loss is 2.240939140319824\n",
      "epoch: 1 step: 4, loss is 2.129425525665283\n",
      "epoch: 1 step: 5, loss is 2.0967297554016113\n",
      "epoch: 1 step: 6, loss is 2.04263973236084\n",
      "epoch: 1 step: 7, loss is 1.9708322286605835\n",
      "epoch: 1 step: 8, loss is 1.9557822942733765\n",
      "epoch: 1 step: 9, loss is 1.970682144165039\n",
      "epoch: 1 step: 10, loss is 1.9055545330047607\n",
      "epoch: 1 step: 11, loss is 1.9129533767700195\n",
      "epoch: 1 step: 12, loss is 1.8051807880401611\n",
      "epoch: 1 step: 13, loss is 1.7804231643676758\n",
      "epoch: 1 step: 14, loss is 1.8194767236709595\n",
      "epoch: 1 step: 15, loss is 1.8231172561645508\n",
      "epoch: 1 step: 16, loss is 1.7105532884597778\n",
      "epoch: 1 step: 17, loss is 1.6123886108398438\n",
      "epoch: 1 step: 18, loss is 1.6223224401474\n",
      "epoch: 1 step: 19, loss is 1.5972222089767456\n",
      "epoch: 1 step: 20, loss is 1.6040822267532349\n",
      "epoch: 1 step: 21, loss is 1.5459479093551636\n",
      "epoch: 1 step: 22, loss is 1.51226007938385\n",
      "epoch: 1 step: 23, loss is 1.5181632041931152\n",
      "epoch: 1 step: 24, loss is 1.5556666851043701\n",
      "epoch: 1 step: 25, loss is 1.3752187490463257\n",
      "epoch: 1 step: 26, loss is 1.4559202194213867\n",
      "epoch: 1 step: 27, loss is 1.4593822956085205\n",
      "epoch: 1 step: 28, loss is 1.54673171043396\n",
      "epoch: 1 step: 29, loss is 1.3199574947357178\n",
      "epoch: 1 step: 30, loss is 1.3439009189605713\n",
      "epoch: 1 step: 31, loss is 1.4176987409591675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] DEVICE(52926,ffffabb60780,python):2025-07-05-23:14:27.945.629 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[DropoutGenMask] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(52926,ffffabb60780,python):2025-07-05-23:14:27.961.828 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[DropoutGenMask] don't support int64, reduce precision from int64 to int32.\n",
      "[WARNING] DEVICE(52926,ffffabb60780,python):2025-07-05-23:14:27.965.554 [mindspore/ccsrc/plugin/device/ascend/hal/device/kernel_select_ascend.cc:330] FilterRaisedOrReducePrecisionMatchedKernelInfo] Operator:[DropoutGenMask] don't support int64, reduce precision from int64 to int32.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 32, loss is 1.409163236618042\n",
      "epoch: 1 step: 33, loss is 1.3224217891693115\n",
      "epoch: 1 step: 34, loss is 1.3310039043426514\n",
      "epoch: 1 step: 35, loss is 1.5378003120422363\n",
      "epoch: 1 step: 36, loss is 1.3085674047470093\n",
      "epoch: 1 step: 37, loss is 1.366121768951416\n",
      "epoch: 1 step: 38, loss is 1.298721432685852\n",
      "epoch: 1 step: 39, loss is 1.3537378311157227\n",
      "epoch: 1 step: 40, loss is 1.3253750801086426\n",
      "epoch: 1 step: 41, loss is 1.2786190509796143\n",
      "epoch: 1 step: 42, loss is 1.3129839897155762\n",
      "epoch: 1 step: 43, loss is 1.148064136505127\n",
      "epoch: 1 step: 44, loss is 1.2019751071929932\n",
      "epoch: 1 step: 45, loss is 1.1124002933502197\n",
      "epoch: 1 step: 46, loss is 1.2596540451049805\n",
      "epoch: 1 step: 47, loss is 1.1668334007263184\n",
      "epoch: 1 step: 48, loss is 1.206587791442871\n",
      "epoch: 1 step: 49, loss is 1.1631790399551392\n",
      "epoch: 1 step: 50, loss is 1.2542027235031128\n",
      "epoch: 1 step: 51, loss is 1.3010598421096802\n",
      "epoch: 1 step: 52, loss is 1.213181972503662\n",
      "epoch: 1 step: 53, loss is 0.9077701568603516\n",
      "epoch: 1 step: 54, loss is 1.0983965396881104\n",
      "epoch: 1 step: 55, loss is 1.1971491575241089\n",
      "epoch: 1 step: 56, loss is 1.1770389080047607\n",
      "epoch: 1 step: 57, loss is 1.2396976947784424\n",
      "epoch: 1 step: 58, loss is 1.2710713148117065\n",
      "epoch: 1 step: 59, loss is 1.1430943012237549\n",
      "epoch: 1 step: 60, loss is 1.0255606174468994\n",
      "epoch: 1 step: 61, loss is 1.0373919010162354\n",
      "epoch: 1 step: 62, loss is 1.013145923614502\n",
      "epoch: 1 step: 63, loss is 1.1899876594543457\n",
      "epoch: 1 step: 64, loss is 0.999901533126831\n",
      "epoch: 1 step: 65, loss is 0.9114648103713989\n",
      "epoch: 1 step: 66, loss is 0.9229158163070679\n",
      "epoch: 1 step: 67, loss is 1.1391215324401855\n",
      "epoch: 1 step: 68, loss is 1.048803448677063\n",
      "epoch: 1 step: 69, loss is 1.0402770042419434\n",
      "epoch: 1 step: 70, loss is 0.7540819644927979\n",
      "epoch: 1 step: 71, loss is 0.8675331473350525\n",
      "epoch: 1 step: 72, loss is 0.9093550443649292\n",
      "epoch: 1 step: 73, loss is 0.9058371782302856\n",
      "epoch: 1 step: 74, loss is 0.9122340679168701\n",
      "epoch: 1 step: 75, loss is 0.9395717978477478\n",
      "epoch: 1 step: 76, loss is 0.8478879928588867\n",
      "epoch: 1 step: 77, loss is 0.8959190249443054\n",
      "epoch: 1 step: 78, loss is 0.7806299924850464\n",
      "epoch: 1 step: 79, loss is 0.9147849678993225\n",
      "epoch: 1 step: 80, loss is 0.8035390377044678\n",
      "epoch: 1 step: 81, loss is 0.8950968384742737\n",
      "epoch: 1 step: 82, loss is 0.7740018367767334\n",
      "epoch: 1 step: 83, loss is 0.9306910037994385\n",
      "epoch: 1 step: 84, loss is 0.8479706048965454\n",
      "epoch: 1 step: 85, loss is 0.810743510723114\n",
      "epoch: 1 step: 86, loss is 1.0679326057434082\n",
      "epoch: 1 step: 87, loss is 0.8361822962760925\n",
      "epoch: 1 step: 88, loss is 0.7591084837913513\n",
      "epoch: 1 step: 89, loss is 0.8326926827430725\n",
      "epoch: 1 step: 90, loss is 0.7157070636749268\n",
      "epoch: 1 step: 91, loss is 0.8809137344360352\n",
      "epoch: 1 step: 92, loss is 0.8747937083244324\n",
      "epoch: 1 step: 93, loss is 0.8558192253112793\n",
      "epoch: 1 step: 94, loss is 1.0572223663330078\n",
      "epoch: 1 step: 95, loss is 0.770937442779541\n",
      "epoch: 1 step: 96, loss is 0.7727593183517456\n",
      "epoch: 1 step: 97, loss is 0.814987301826477\n",
      "epoch: 1 step: 98, loss is 0.8423813581466675\n",
      "epoch: 1 step: 99, loss is 0.911364734172821\n",
      "epoch: 1 step: 100, loss is 0.8644145727157593\n",
      "epoch: 1 step: 101, loss is 0.7448310852050781\n",
      "epoch: 1 step: 102, loss is 0.8797523379325867\n",
      "epoch: 1 step: 103, loss is 1.0005505084991455\n",
      "epoch: 1 step: 104, loss is 0.8361299633979797\n",
      "epoch: 1 step: 105, loss is 0.8501521348953247\n",
      "epoch: 1 step: 106, loss is 0.6583314538002014\n",
      "epoch: 1 step: 107, loss is 0.8490622043609619\n",
      "epoch: 1 step: 108, loss is 0.7367773652076721\n",
      "epoch: 1 step: 109, loss is 0.8354640603065491\n",
      "epoch: 1 step: 110, loss is 0.6726666688919067\n",
      "epoch: 1 step: 111, loss is 0.737668514251709\n",
      "epoch: 1 step: 112, loss is 0.676804780960083\n",
      "epoch: 1 step: 113, loss is 0.753865122795105\n",
      "epoch: 1 step: 114, loss is 0.773837685585022\n",
      "epoch: 1 step: 115, loss is 0.5579400062561035\n",
      "epoch: 1 step: 116, loss is 0.854778528213501\n",
      "epoch: 1 step: 117, loss is 0.7090914249420166\n",
      "epoch: 1 step: 118, loss is 0.681036651134491\n",
      "epoch: 1 step: 119, loss is 0.8176418542861938\n",
      "epoch: 1 step: 120, loss is 0.77206951379776\n",
      "epoch: 1 step: 121, loss is 0.764549732208252\n",
      "epoch: 1 step: 122, loss is 0.8433322906494141\n",
      "epoch: 1 step: 123, loss is 0.6201909780502319\n",
      "epoch: 1 step: 124, loss is 0.6913779973983765\n",
      "epoch: 1 step: 125, loss is 0.7247974872589111\n",
      "epoch: 1 step: 126, loss is 0.8212932348251343\n",
      "epoch: 1 step: 127, loss is 0.7744474411010742\n",
      "epoch: 1 step: 128, loss is 0.7185433506965637\n",
      "epoch: 1 step: 129, loss is 0.702648937702179\n",
      "epoch: 1 step: 130, loss is 0.633285403251648\n",
      "epoch: 1 step: 131, loss is 0.7328981161117554\n",
      "epoch: 1 step: 132, loss is 0.6688494086265564\n",
      "epoch: 1 step: 133, loss is 0.6995593905448914\n",
      "epoch: 1 step: 134, loss is 0.613508403301239\n",
      "epoch: 1 step: 135, loss is 0.6919953227043152\n",
      "epoch: 1 step: 136, loss is 0.629613995552063\n",
      "epoch: 1 step: 137, loss is 0.7260847091674805\n",
      "epoch: 1 step: 138, loss is 0.7654324769973755\n",
      "epoch: 1 step: 139, loss is 0.8501620292663574\n",
      "epoch: 1 step: 140, loss is 0.6384462118148804\n",
      "epoch: 1 step: 141, loss is 0.523424506187439\n",
      "epoch: 1 step: 142, loss is 0.7371631860733032\n",
      "epoch: 1 step: 143, loss is 0.6998282074928284\n",
      "epoch: 1 step: 144, loss is 0.6624584197998047\n",
      "epoch: 1 step: 145, loss is 0.6978399753570557\n",
      "epoch: 1 step: 146, loss is 0.6601975560188293\n",
      "epoch: 1 step: 147, loss is 0.520748496055603\n",
      "epoch: 1 step: 148, loss is 0.5657119750976562\n",
      "epoch: 1 step: 149, loss is 0.631488561630249\n",
      "epoch: 1 step: 150, loss is 0.6091867685317993\n",
      "epoch: 1 step: 151, loss is 0.7229347825050354\n",
      "epoch: 1 step: 152, loss is 0.6148942708969116\n",
      "epoch: 1 step: 153, loss is 0.780654788017273\n",
      "epoch: 1 step: 154, loss is 0.7553689479827881\n",
      "epoch: 1 step: 155, loss is 0.6742198467254639\n",
      "epoch: 1 step: 156, loss is 0.5725751519203186\n",
      "epoch: 1 step: 157, loss is 0.6846697926521301\n",
      "epoch: 1 step: 158, loss is 0.6843152046203613\n",
      "epoch: 1 step: 159, loss is 0.7194348573684692\n",
      "epoch: 1 step: 160, loss is 0.7538347840309143\n",
      "epoch: 1 step: 161, loss is 0.6555116176605225\n",
      "epoch: 1 step: 162, loss is 0.7325511574745178\n",
      "epoch: 1 step: 163, loss is 0.6186778545379639\n",
      "epoch: 1 step: 164, loss is 0.8010134696960449\n",
      "epoch: 1 step: 165, loss is 0.709989070892334\n",
      "epoch: 1 step: 166, loss is 0.6784285306930542\n",
      "epoch: 1 step: 167, loss is 0.7805156707763672\n",
      "epoch: 1 step: 168, loss is 0.7470452785491943\n",
      "epoch: 1 step: 169, loss is 0.7279947996139526\n",
      "epoch: 1 step: 170, loss is 0.6944454908370972\n",
      "epoch: 1 step: 171, loss is 0.7128472924232483\n",
      "epoch: 1 step: 172, loss is 0.8071123957633972\n",
      "epoch: 1 step: 173, loss is 0.640583336353302\n",
      "epoch: 1 step: 174, loss is 0.600245475769043\n",
      "epoch: 1 step: 175, loss is 0.6194483041763306\n",
      "epoch: 1 step: 176, loss is 0.7259894013404846\n",
      "epoch: 1 step: 177, loss is 0.8052003383636475\n",
      "epoch: 1 step: 178, loss is 0.6614497900009155\n",
      "epoch: 1 step: 179, loss is 0.6375457048416138\n",
      "epoch: 1 step: 180, loss is 0.741141140460968\n",
      "epoch: 1 step: 181, loss is 0.6260648369789124\n",
      "epoch: 1 step: 182, loss is 0.6402652263641357\n",
      "epoch: 1 step: 183, loss is 0.6608959436416626\n",
      "epoch: 1 step: 184, loss is 0.5739291906356812\n",
      "epoch: 1 step: 185, loss is 0.7371419668197632\n",
      "epoch: 1 step: 186, loss is 0.7922234535217285\n",
      "epoch: 1 step: 187, loss is 0.777694821357727\n",
      "epoch: 1 step: 188, loss is 0.5741103887557983\n",
      "epoch: 1 step: 189, loss is 0.618110179901123\n",
      "epoch: 1 step: 190, loss is 0.5825382471084595\n",
      "epoch: 1 step: 191, loss is 0.5963472723960876\n",
      "epoch: 1 step: 192, loss is 0.6282542943954468\n",
      "epoch: 1 step: 193, loss is 0.6059212684631348\n",
      "epoch: 1 step: 194, loss is 0.6300044655799866\n",
      "epoch: 1 step: 195, loss is 0.6942814588546753\n",
      "epoch: 1 step: 196, loss is 0.5560362339019775\n",
      "epoch: 1 step: 197, loss is 0.6581445932388306\n",
      "epoch: 1 step: 198, loss is 0.585094690322876\n",
      "epoch: 1 step: 199, loss is 0.44764095544815063\n",
      "epoch: 1 step: 200, loss is 0.4887876510620117\n",
      "epoch: 1 step: 201, loss is 0.8233410120010376\n",
      "epoch: 1 step: 202, loss is 0.5055876970291138\n",
      "epoch: 1 step: 203, loss is 0.6157041788101196\n",
      "epoch: 1 step: 204, loss is 0.5544781684875488\n",
      "epoch: 1 step: 205, loss is 0.5549733638763428\n",
      "epoch: 1 step: 206, loss is 0.7550911903381348\n",
      "epoch: 1 step: 207, loss is 0.6476939916610718\n",
      "epoch: 1 step: 208, loss is 0.5917443037033081\n",
      "epoch: 1 step: 209, loss is 0.7345033288002014\n",
      "epoch: 1 step: 210, loss is 0.4890049397945404\n",
      "epoch: 1 step: 211, loss is 0.6181971430778503\n",
      "epoch: 1 step: 212, loss is 0.44888508319854736\n",
      "epoch: 1 step: 213, loss is 0.7975960969924927\n",
      "epoch: 1 step: 214, loss is 0.7350047826766968\n",
      "epoch: 1 step: 215, loss is 0.8001463413238525\n",
      "epoch: 1 step: 216, loss is 0.39452046155929565\n",
      "epoch: 1 step: 217, loss is 0.7538296580314636\n",
      "epoch: 1 step: 218, loss is 0.6694668531417847\n",
      "epoch: 1 step: 219, loss is 0.5755093693733215\n",
      "epoch: 1 step: 220, loss is 0.6578447818756104\n",
      "epoch: 1 step: 221, loss is 0.5855995416641235\n",
      "epoch: 1 step: 222, loss is 0.735598623752594\n",
      "epoch: 1 step: 223, loss is 0.5358713865280151\n",
      "epoch: 1 step: 224, loss is 0.5527667999267578\n",
      "epoch: 1 step: 225, loss is 0.5756691694259644\n",
      "epoch: 1 step: 226, loss is 0.6046954393386841\n",
      "epoch: 1 step: 227, loss is 0.7070557475090027\n",
      "epoch: 1 step: 228, loss is 0.7661027312278748\n",
      "epoch: 1 step: 229, loss is 0.4949754774570465\n",
      "epoch: 1 step: 230, loss is 0.6271858215332031\n",
      "epoch: 1 step: 231, loss is 0.6781465411186218\n",
      "epoch: 1 step: 232, loss is 0.7532278895378113\n",
      "epoch: 1 step: 233, loss is 0.5473580956459045\n",
      "epoch: 1 step: 234, loss is 0.6404311060905457\n",
      "epoch: 1 step: 235, loss is 0.5612326860427856\n",
      "epoch: 1 step: 236, loss is 0.4581320285797119\n",
      "epoch: 1 step: 237, loss is 0.6286886930465698\n",
      "epoch: 1 step: 238, loss is 0.584933876991272\n",
      "epoch: 1 step: 239, loss is 0.5053954720497131\n",
      "epoch: 1 step: 240, loss is 0.6666018962860107\n",
      "epoch: 1 step: 241, loss is 0.6084074378013611\n",
      "epoch: 1 step: 242, loss is 0.5814340710639954\n",
      "epoch: 1 step: 243, loss is 0.615429162979126\n",
      "epoch: 1 step: 244, loss is 0.6041809916496277\n",
      "epoch: 1 step: 245, loss is 0.7162622213363647\n",
      "epoch: 1 step: 246, loss is 0.5633738040924072\n",
      "epoch: 1 step: 247, loss is 0.44953837990760803\n",
      "epoch: 1 step: 248, loss is 0.5282685160636902\n",
      "epoch: 1 step: 249, loss is 0.7153040766716003\n",
      "epoch: 1 step: 250, loss is 0.657058835029602\n",
      "epoch: 1 step: 251, loss is 0.5923349857330322\n",
      "epoch: 1 step: 252, loss is 0.6196718215942383\n",
      "epoch: 1 step: 253, loss is 0.7889337539672852\n",
      "epoch: 1 step: 254, loss is 0.5750292539596558\n",
      "epoch: 1 step: 255, loss is 0.6806648969650269\n",
      "epoch: 1 step: 256, loss is 0.571159839630127\n",
      "epoch: 1 step: 257, loss is 0.6159056425094604\n",
      "epoch: 1 step: 258, loss is 0.47861140966415405\n",
      "epoch: 1 step: 259, loss is 0.5095924735069275\n",
      "epoch: 1 step: 260, loss is 0.6047269105911255\n",
      "epoch: 1 step: 261, loss is 0.5225433111190796\n",
      "epoch: 1 step: 262, loss is 0.7343106865882874\n",
      "epoch: 1 step: 263, loss is 0.6545022130012512\n",
      "epoch: 1 step: 264, loss is 0.5279198884963989\n",
      "epoch: 1 step: 265, loss is 0.6181510090827942\n",
      "epoch: 1 step: 266, loss is 0.6244902610778809\n",
      "epoch: 1 step: 267, loss is 0.5737743377685547\n",
      "epoch: 1 step: 268, loss is 0.5873161554336548\n",
      "epoch: 1 step: 269, loss is 0.5829737186431885\n",
      "epoch: 1 step: 270, loss is 0.5397683382034302\n",
      "epoch: 1 step: 271, loss is 0.5963277816772461\n",
      "epoch: 1 step: 272, loss is 0.48527055978775024\n",
      "epoch: 1 step: 273, loss is 0.6256747245788574\n",
      "epoch: 1 step: 274, loss is 0.6642388105392456\n",
      "epoch: 1 step: 275, loss is 0.6129472255706787\n",
      "epoch: 1 step: 276, loss is 0.3705274164676666\n",
      "epoch: 1 step: 277, loss is 0.7665557861328125\n",
      "epoch: 1 step: 278, loss is 0.5752895474433899\n",
      "epoch: 1 step: 279, loss is 0.5572689175605774\n",
      "epoch: 1 step: 280, loss is 0.7551255226135254\n",
      "epoch: 1 step: 281, loss is 0.6576278209686279\n",
      "epoch: 1 step: 282, loss is 0.5747412443161011\n",
      "epoch: 1 step: 283, loss is 0.508165717124939\n",
      "epoch: 1 step: 284, loss is 0.4515904188156128\n",
      "epoch: 1 step: 285, loss is 0.5892694592475891\n",
      "epoch: 1 step: 286, loss is 0.7739303112030029\n",
      "epoch: 1 step: 287, loss is 0.5505846738815308\n",
      "epoch: 1 step: 288, loss is 0.799005389213562\n",
      "epoch: 1 step: 289, loss is 0.6101072430610657\n",
      "epoch: 1 step: 290, loss is 0.5506887435913086\n",
      "epoch: 1 step: 291, loss is 0.5248139500617981\n",
      "epoch: 1 step: 292, loss is 0.9859543442726135\n",
      "epoch: 1 step: 293, loss is 0.716978907585144\n",
      "epoch: 1 step: 294, loss is 0.7746443748474121\n",
      "epoch: 1 step: 295, loss is 0.5931520462036133\n",
      "epoch: 1 step: 296, loss is 0.5332440137863159\n",
      "epoch: 1 step: 297, loss is 0.6689301133155823\n",
      "epoch: 1 step: 298, loss is 0.6173503398895264\n",
      "epoch: 1 step: 299, loss is 0.4377223253250122\n",
      "epoch: 1 step: 300, loss is 0.4270554482936859\n",
      "epoch: 1 step: 301, loss is 0.462512344121933\n",
      "epoch: 1 step: 302, loss is 0.6548343300819397\n",
      "epoch: 1 step: 303, loss is 0.5138221979141235\n",
      "epoch: 1 step: 304, loss is 0.549759030342102\n",
      "epoch: 1 step: 305, loss is 0.5816761255264282\n",
      "epoch: 1 step: 306, loss is 0.5242053270339966\n",
      "epoch: 1 step: 307, loss is 0.4215274751186371\n",
      "epoch: 1 step: 308, loss is 0.4688681662082672\n",
      "epoch: 1 step: 309, loss is 0.7351583242416382\n",
      "epoch: 1 step: 310, loss is 0.6554138660430908\n",
      "epoch: 1 step: 311, loss is 0.5658001899719238\n",
      "epoch: 1 step: 312, loss is 0.519696056842804\n",
      "epoch: 1 step: 313, loss is 0.6055362820625305\n",
      "epoch: 1 step: 314, loss is 0.42989581823349\n",
      "epoch: 1 step: 315, loss is 0.654803991317749\n",
      "epoch: 1 step: 316, loss is 0.46066054701805115\n",
      "epoch: 1 step: 317, loss is 0.5740100145339966\n",
      "epoch: 1 step: 318, loss is 0.5382455587387085\n",
      "epoch: 1 step: 319, loss is 0.643397331237793\n",
      "epoch: 1 step: 320, loss is 0.4855688810348511\n",
      "epoch: 1 step: 321, loss is 0.5341869592666626\n",
      "epoch: 1 step: 322, loss is 0.5712640285491943\n",
      "epoch: 1 step: 323, loss is 0.604133665561676\n",
      "epoch: 1 step: 324, loss is 0.5172303915023804\n",
      "epoch: 1 step: 325, loss is 0.63199383020401\n",
      "epoch: 1 step: 326, loss is 0.5310505628585815\n",
      "epoch: 1 step: 327, loss is 0.5074154138565063\n",
      "epoch: 1 step: 328, loss is 0.6519922018051147\n",
      "epoch: 1 step: 329, loss is 0.41083717346191406\n",
      "epoch: 1 step: 330, loss is 0.660487174987793\n",
      "epoch: 1 step: 331, loss is 0.7222020626068115\n",
      "epoch: 1 step: 332, loss is 0.7413759827613831\n",
      "epoch: 1 step: 333, loss is 0.4483349323272705\n",
      "epoch: 1 step: 334, loss is 0.518224835395813\n",
      "epoch: 1 step: 335, loss is 0.6232324838638306\n",
      "epoch: 1 step: 336, loss is 0.6897924542427063\n",
      "epoch: 1 step: 337, loss is 0.41608184576034546\n",
      "epoch: 1 step: 338, loss is 0.5702741146087646\n",
      "epoch: 1 step: 339, loss is 0.5167434215545654\n",
      "epoch: 1 step: 340, loss is 0.6474776268005371\n",
      "epoch: 1 step: 341, loss is 0.4749792218208313\n",
      "epoch: 1 step: 342, loss is 0.49156686663627625\n",
      "epoch: 1 step: 343, loss is 0.527801513671875\n",
      "epoch: 1 step: 344, loss is 0.603024423122406\n",
      "epoch: 1 step: 345, loss is 0.6427187323570251\n",
      "epoch: 1 step: 346, loss is 0.5673972368240356\n",
      "epoch: 1 step: 347, loss is 0.6626731753349304\n",
      "epoch: 1 step: 348, loss is 0.6441301107406616\n",
      "epoch: 1 step: 349, loss is 0.5720891952514648\n",
      "epoch: 1 step: 350, loss is 0.5142346024513245\n",
      "epoch: 1 step: 351, loss is 0.4315371811389923\n",
      "epoch: 1 step: 352, loss is 0.5602428913116455\n",
      "epoch: 1 step: 353, loss is 0.5454573035240173\n",
      "epoch: 1 step: 354, loss is 0.48258644342422485\n",
      "epoch: 1 step: 355, loss is 0.7417218089103699\n",
      "epoch: 1 step: 356, loss is 0.5637844800949097\n",
      "epoch: 1 step: 357, loss is 0.5540554523468018\n",
      "epoch: 1 step: 358, loss is 0.6801782846450806\n",
      "epoch: 1 step: 359, loss is 0.6252925395965576\n",
      "epoch: 1 step: 360, loss is 0.5430564880371094\n",
      "epoch: 1 step: 361, loss is 0.3449096083641052\n",
      "epoch: 1 step: 362, loss is 0.525364875793457\n",
      "epoch: 1 step: 363, loss is 0.48819398880004883\n",
      "epoch: 1 step: 364, loss is 0.5307859182357788\n",
      "epoch: 1 step: 365, loss is 0.7229762673377991\n",
      "epoch: 1 step: 366, loss is 0.6714503765106201\n",
      "epoch: 1 step: 367, loss is 0.5446481108665466\n",
      "epoch: 1 step: 368, loss is 0.6280261278152466\n",
      "epoch: 1 step: 369, loss is 0.6090430021286011\n",
      "epoch: 1 step: 370, loss is 0.3152618706226349\n",
      "epoch: 1 step: 371, loss is 0.42160505056381226\n",
      "epoch: 1 step: 372, loss is 0.7015092372894287\n",
      "epoch: 1 step: 373, loss is 0.4709062874317169\n",
      "epoch: 1 step: 374, loss is 0.4183422923088074\n",
      "epoch: 1 step: 375, loss is 0.5509123802185059\n",
      "epoch: 1 step: 376, loss is 0.6166559457778931\n",
      "epoch: 1 step: 377, loss is 0.6591061353683472\n",
      "epoch: 1 step: 378, loss is 0.5178251266479492\n",
      "epoch: 1 step: 379, loss is 0.5862263441085815\n",
      "epoch: 1 step: 380, loss is 0.5239364504814148\n",
      "epoch: 1 step: 381, loss is 0.6080547571182251\n",
      "epoch: 1 step: 382, loss is 0.43442803621292114\n",
      "epoch: 1 step: 383, loss is 0.5841514468193054\n",
      "epoch: 1 step: 384, loss is 0.553942859172821\n",
      "epoch: 1 step: 385, loss is 0.6363505125045776\n",
      "epoch: 1 step: 386, loss is 0.5280269384384155\n",
      "epoch: 1 step: 387, loss is 0.46323201060295105\n",
      "epoch: 1 step: 388, loss is 0.43838030099868774\n",
      "epoch: 1 step: 389, loss is 0.49541521072387695\n",
      "epoch: 1 step: 390, loss is 0.5616784691810608\n",
      "epoch: 1 step: 391, loss is 0.4369460940361023\n",
      "epoch: 1 step: 392, loss is 0.3986770212650299\n",
      "epoch: 1 step: 393, loss is 0.5031261444091797\n",
      "epoch: 1 step: 394, loss is 0.6410248279571533\n",
      "epoch: 1 step: 395, loss is 0.7062504291534424\n",
      "epoch: 1 step: 396, loss is 0.5501035451889038\n",
      "epoch: 1 step: 397, loss is 0.43936046957969666\n",
      "epoch: 1 step: 398, loss is 0.557405948638916\n",
      "epoch: 1 step: 399, loss is 0.5591153502464294\n",
      "epoch: 1 step: 400, loss is 0.6427765488624573\n",
      "epoch: 1 step: 401, loss is 0.6013428568840027\n",
      "epoch: 1 step: 402, loss is 0.4779958724975586\n",
      "epoch: 1 step: 403, loss is 0.590785026550293\n",
      "epoch: 1 step: 404, loss is 0.6032200455665588\n",
      "epoch: 1 step: 405, loss is 0.4453470706939697\n",
      "epoch: 1 step: 406, loss is 0.5612468719482422\n",
      "epoch: 1 step: 407, loss is 0.45830321311950684\n",
      "epoch: 1 step: 408, loss is 0.31347566843032837\n",
      "epoch: 1 step: 409, loss is 0.626193642616272\n",
      "epoch: 1 step: 410, loss is 0.4687012732028961\n",
      "epoch: 1 step: 411, loss is 0.4690972864627838\n",
      "epoch: 1 step: 412, loss is 0.40104854106903076\n",
      "epoch: 1 step: 413, loss is 0.6270053386688232\n",
      "epoch: 1 step: 414, loss is 0.54023277759552\n",
      "epoch: 1 step: 415, loss is 0.6475019454956055\n",
      "epoch: 1 step: 416, loss is 0.45607224106788635\n",
      "epoch: 1 step: 417, loss is 0.37889859080314636\n",
      "epoch: 1 step: 418, loss is 0.8219434022903442\n",
      "epoch: 1 step: 419, loss is 0.38362595438957214\n",
      "epoch: 1 step: 420, loss is 0.5036108493804932\n",
      "epoch: 1 step: 421, loss is 0.5368696451187134\n",
      "epoch: 1 step: 422, loss is 0.5230579376220703\n",
      "epoch: 1 step: 423, loss is 0.5233827233314514\n",
      "epoch: 1 step: 424, loss is 0.7686570882797241\n",
      "epoch: 1 step: 425, loss is 0.4611779451370239\n",
      "epoch: 1 step: 426, loss is 0.5233489274978638\n",
      "epoch: 1 step: 427, loss is 0.5623456239700317\n",
      "epoch: 1 step: 428, loss is 0.3862718939781189\n",
      "epoch: 1 step: 429, loss is 0.4395734667778015\n",
      "epoch: 1 step: 430, loss is 0.41496771574020386\n",
      "epoch: 1 step: 431, loss is 0.4358537197113037\n",
      "epoch: 1 step: 432, loss is 0.4626045823097229\n",
      "epoch: 1 step: 433, loss is 0.45346686244010925\n",
      "epoch: 1 step: 434, loss is 0.34454214572906494\n",
      "epoch: 1 step: 435, loss is 0.4507071375846863\n",
      "epoch: 1 step: 436, loss is 0.5159767866134644\n",
      "epoch: 1 step: 437, loss is 0.5558261871337891\n",
      "epoch: 1 step: 438, loss is 0.37681278586387634\n",
      "epoch: 1 step: 439, loss is 0.4323767125606537\n",
      "epoch: 1 step: 440, loss is 0.4868727922439575\n",
      "epoch: 1 step: 441, loss is 0.3324044346809387\n",
      "epoch: 1 step: 442, loss is 0.43769368529319763\n",
      "epoch: 1 step: 443, loss is 0.4862722158432007\n",
      "epoch: 1 step: 444, loss is 0.5024597644805908\n",
      "epoch: 1 step: 445, loss is 0.5444963574409485\n",
      "epoch: 1 step: 446, loss is 0.6765508651733398\n",
      "epoch: 1 step: 447, loss is 0.4882484972476959\n",
      "epoch: 1 step: 448, loss is 0.40804556012153625\n",
      "epoch: 1 step: 449, loss is 0.6349899768829346\n",
      "epoch: 1 step: 450, loss is 0.567186713218689\n",
      "epoch: 1 step: 451, loss is 0.6570873260498047\n",
      "epoch: 1 step: 452, loss is 0.6713143587112427\n",
      "epoch: 1 step: 453, loss is 0.6518105268478394\n",
      "epoch: 1 step: 454, loss is 0.580573320388794\n",
      "epoch: 1 step: 455, loss is 0.5643666982650757\n",
      "epoch: 1 step: 456, loss is 0.41660076379776\n",
      "epoch: 1 step: 457, loss is 0.5317142009735107\n",
      "epoch: 1 step: 458, loss is 0.45220673084259033\n",
      "epoch: 1 step: 459, loss is 0.7570687532424927\n",
      "epoch: 1 step: 460, loss is 0.4017583727836609\n",
      "epoch: 1 step: 461, loss is 0.4877379238605499\n",
      "epoch: 1 step: 462, loss is 0.38892534375190735\n",
      "epoch: 1 step: 463, loss is 0.2835240364074707\n",
      "epoch: 1 step: 464, loss is 0.7220553159713745\n",
      "epoch: 1 step: 465, loss is 0.401192843914032\n",
      "epoch: 1 step: 466, loss is 0.6125575304031372\n",
      "epoch: 1 step: 467, loss is 0.4356318712234497\n",
      "epoch: 1 step: 468, loss is 0.4304593801498413\n",
      "epoch: 1 step: 469, loss is 0.5372695922851562\n",
      "epoch: 1 step: 470, loss is 0.3892294764518738\n",
      "epoch: 1 step: 471, loss is 0.3420065939426422\n",
      "epoch: 1 step: 472, loss is 0.46125859022140503\n",
      "epoch: 1 step: 473, loss is 0.55454021692276\n",
      "epoch: 1 step: 474, loss is 0.5847058296203613\n",
      "epoch: 1 step: 475, loss is 0.3851807117462158\n",
      "epoch: 1 step: 476, loss is 0.5716061592102051\n",
      "epoch: 1 step: 477, loss is 0.4344136714935303\n",
      "epoch: 1 step: 478, loss is 0.437560498714447\n",
      "epoch: 1 step: 479, loss is 0.433956503868103\n",
      "epoch: 1 step: 480, loss is 0.5770513415336609\n",
      "epoch: 1 step: 481, loss is 0.5138206481933594\n",
      "epoch: 1 step: 482, loss is 0.37456244230270386\n",
      "epoch: 1 step: 483, loss is 0.6065245866775513\n",
      "epoch: 1 step: 484, loss is 0.5411138534545898\n",
      "epoch: 1 step: 485, loss is 0.5613420605659485\n",
      "epoch: 1 step: 486, loss is 0.615729808807373\n",
      "epoch: 1 step: 487, loss is 0.45995932817459106\n",
      "epoch: 1 step: 488, loss is 0.3630446195602417\n",
      "epoch: 1 step: 489, loss is 0.6358515024185181\n",
      "epoch: 1 step: 490, loss is 0.42724794149398804\n",
      "epoch: 1 step: 491, loss is 0.4766573905944824\n",
      "epoch: 1 step: 492, loss is 0.34322506189346313\n",
      "epoch: 1 step: 493, loss is 0.6432524919509888\n",
      "epoch: 1 step: 494, loss is 0.48569414019584656\n",
      "epoch: 1 step: 495, loss is 0.6408557891845703\n",
      "epoch: 1 step: 496, loss is 0.5894771814346313\n",
      "epoch: 1 step: 497, loss is 0.4883711636066437\n",
      "epoch: 1 step: 498, loss is 0.5240238904953003\n",
      "epoch: 1 step: 499, loss is 0.5113905072212219\n",
      "epoch: 1 step: 500, loss is 0.4435422420501709\n",
      "epoch: 1 step: 501, loss is 0.5919780731201172\n",
      "epoch: 1 step: 502, loss is 0.35232672095298767\n",
      "epoch: 1 step: 503, loss is 0.5110865831375122\n",
      "epoch: 1 step: 504, loss is 0.46253305673599243\n",
      "epoch: 1 step: 505, loss is 0.6683669686317444\n",
      "epoch: 1 step: 506, loss is 0.3872218132019043\n",
      "epoch: 1 step: 507, loss is 0.540306568145752\n",
      "epoch: 1 step: 508, loss is 0.42281538248062134\n",
      "epoch: 1 step: 509, loss is 0.6911334991455078\n",
      "epoch: 1 step: 510, loss is 0.4798263907432556\n",
      "epoch: 1 step: 511, loss is 0.5100961923599243\n",
      "epoch: 1 step: 512, loss is 0.457511842250824\n",
      "epoch: 1 step: 513, loss is 0.41632163524627686\n",
      "epoch: 1 step: 514, loss is 0.5212923288345337\n",
      "epoch: 1 step: 515, loss is 0.5745214223861694\n",
      "epoch: 1 step: 516, loss is 0.47649824619293213\n",
      "epoch: 1 step: 517, loss is 0.5916010141372681\n",
      "epoch: 1 step: 518, loss is 0.5529047250747681\n",
      "epoch: 1 step: 519, loss is 0.42103248834609985\n",
      "epoch: 1 step: 520, loss is 0.5279762744903564\n",
      "epoch: 1 step: 521, loss is 0.43536901473999023\n",
      "epoch: 1 step: 522, loss is 0.504307746887207\n",
      "epoch: 1 step: 523, loss is 0.8066925406455994\n",
      "epoch: 1 step: 524, loss is 0.5610496997833252\n",
      "epoch: 1 step: 525, loss is 0.4887695014476776\n",
      "epoch: 1 step: 526, loss is 0.5845776796340942\n",
      "epoch: 1 step: 527, loss is 0.3591036796569824\n",
      "epoch: 1 step: 528, loss is 0.6446939706802368\n",
      "epoch: 1 step: 529, loss is 0.47380784153938293\n",
      "epoch: 1 step: 530, loss is 0.5568816661834717\n",
      "epoch: 1 step: 531, loss is 0.6718963980674744\n",
      "epoch: 1 step: 532, loss is 0.4242043197154999\n",
      "epoch: 1 step: 533, loss is 0.5932279825210571\n",
      "epoch: 1 step: 534, loss is 0.42509377002716064\n",
      "epoch: 1 step: 535, loss is 0.3806585967540741\n",
      "epoch: 1 step: 536, loss is 0.6073874235153198\n",
      "epoch: 1 step: 537, loss is 0.42306774854660034\n",
      "epoch: 1 step: 538, loss is 0.6127933263778687\n",
      "epoch: 1 step: 539, loss is 0.4280807077884674\n",
      "epoch: 1 step: 540, loss is 0.564544677734375\n",
      "epoch: 1 step: 541, loss is 0.4221607446670532\n",
      "epoch: 1 step: 542, loss is 0.49648645520210266\n",
      "epoch: 1 step: 543, loss is 0.38711869716644287\n",
      "epoch: 1 step: 544, loss is 0.5861046314239502\n",
      "epoch: 1 step: 545, loss is 0.35544851422309875\n",
      "epoch: 1 step: 546, loss is 0.7140447497367859\n",
      "epoch: 1 step: 547, loss is 0.6029637455940247\n",
      "epoch: 1 step: 548, loss is 0.5438121557235718\n",
      "epoch: 1 step: 549, loss is 0.5029609203338623\n",
      "epoch: 1 step: 550, loss is 0.5753340721130371\n",
      "epoch: 1 step: 551, loss is 0.8187791109085083\n",
      "epoch: 1 step: 552, loss is 0.5171617269515991\n",
      "epoch: 1 step: 553, loss is 0.5067310333251953\n",
      "epoch: 1 step: 554, loss is 0.4292134642601013\n",
      "epoch: 1 step: 555, loss is 0.39700403809547424\n",
      "epoch: 1 step: 556, loss is 0.42142459750175476\n",
      "epoch: 1 step: 557, loss is 0.30723837018013\n",
      "epoch: 1 step: 558, loss is 0.449973464012146\n",
      "epoch: 1 step: 559, loss is 0.3941495716571808\n",
      "epoch: 1 step: 560, loss is 0.32242271304130554\n",
      "epoch: 1 step: 561, loss is 0.40765076875686646\n",
      "epoch: 1 step: 562, loss is 0.5180944800376892\n",
      "epoch: 1 step: 563, loss is 0.27911412715911865\n",
      "epoch: 1 step: 564, loss is 0.9403539299964905\n",
      "epoch: 1 step: 565, loss is 0.7140300273895264\n",
      "epoch: 1 step: 566, loss is 0.44756489992141724\n",
      "epoch: 1 step: 567, loss is 0.3643112778663635\n",
      "epoch: 1 step: 568, loss is 0.36109769344329834\n",
      "epoch: 1 step: 569, loss is 0.46280547976493835\n",
      "epoch: 1 step: 570, loss is 0.5568551421165466\n",
      "epoch: 1 step: 571, loss is 0.41746050119400024\n",
      "epoch: 1 step: 572, loss is 0.6199610233306885\n",
      "epoch: 1 step: 573, loss is 0.48414504528045654\n",
      "epoch: 1 step: 574, loss is 0.43287602066993713\n",
      "epoch: 1 step: 575, loss is 0.36711859703063965\n",
      "epoch: 1 step: 576, loss is 0.846580982208252\n",
      "epoch: 1 step: 577, loss is 0.634149432182312\n",
      "epoch: 1 step: 578, loss is 0.4386935830116272\n",
      "epoch: 1 step: 579, loss is 0.5590590834617615\n",
      "epoch: 1 step: 580, loss is 0.3875238299369812\n",
      "epoch: 1 step: 581, loss is 0.4274882376194\n",
      "epoch: 1 step: 582, loss is 0.5278319716453552\n",
      "epoch: 1 step: 583, loss is 0.44055408239364624\n",
      "epoch: 1 step: 584, loss is 0.326779305934906\n",
      "epoch: 1 step: 585, loss is 0.40055200457572937\n",
      "epoch: 1 step: 586, loss is 0.4333595335483551\n",
      "epoch: 1 step: 587, loss is 0.5993678569793701\n",
      "epoch: 1 step: 588, loss is 0.4494733512401581\n",
      "epoch: 1 step: 589, loss is 0.5367016792297363\n",
      "epoch: 1 step: 590, loss is 0.5062158107757568\n",
      "epoch: 1 step: 591, loss is 0.5488594770431519\n",
      "epoch: 1 step: 592, loss is 0.3728835880756378\n",
      "epoch: 1 step: 593, loss is 0.5116719603538513\n",
      "epoch: 1 step: 594, loss is 0.35470566153526306\n",
      "epoch: 1 step: 595, loss is 0.4514673054218292\n",
      "epoch: 1 step: 596, loss is 0.3646783232688904\n",
      "epoch: 1 step: 597, loss is 0.5631584525108337\n",
      "epoch: 1 step: 598, loss is 0.4339210093021393\n",
      "epoch: 1 step: 599, loss is 0.6244397759437561\n",
      "epoch: 1 step: 600, loss is 0.45564180612564087\n",
      "epoch: 1 step: 601, loss is 0.48053255677223206\n",
      "epoch: 1 step: 602, loss is 0.4395458996295929\n",
      "epoch: 1 step: 603, loss is 0.40971285104751587\n",
      "epoch: 1 step: 604, loss is 0.5333203077316284\n",
      "epoch: 1 step: 605, loss is 0.6323249936103821\n",
      "epoch: 1 step: 606, loss is 0.41243672370910645\n",
      "epoch: 1 step: 607, loss is 0.3786182999610901\n",
      "epoch: 1 step: 608, loss is 0.49113595485687256\n",
      "epoch: 1 step: 609, loss is 0.5540139675140381\n",
      "epoch: 1 step: 610, loss is 0.6904417276382446\n",
      "epoch: 1 step: 611, loss is 0.5706143975257874\n",
      "epoch: 1 step: 612, loss is 0.5323750376701355\n",
      "epoch: 1 step: 613, loss is 0.48642420768737793\n",
      "epoch: 1 step: 614, loss is 0.5114162564277649\n",
      "epoch: 1 step: 615, loss is 0.5311977863311768\n",
      "epoch: 1 step: 616, loss is 0.45235347747802734\n",
      "epoch: 1 step: 617, loss is 0.41007453203201294\n",
      "epoch: 1 step: 618, loss is 0.5348277688026428\n",
      "epoch: 1 step: 619, loss is 0.4098168611526489\n",
      "epoch: 1 step: 620, loss is 0.7591918706893921\n",
      "epoch: 1 step: 621, loss is 0.39006248116493225\n",
      "epoch: 1 step: 622, loss is 0.46341729164123535\n",
      "epoch: 1 step: 623, loss is 0.48214471340179443\n",
      "epoch: 1 step: 624, loss is 0.3432615399360657\n",
      "epoch: 1 step: 625, loss is 0.44531354308128357\n",
      "epoch: 1 step: 626, loss is 0.4494816064834595\n",
      "epoch: 1 step: 627, loss is 0.41772356629371643\n",
      "epoch: 1 step: 628, loss is 0.5109313130378723\n",
      "epoch: 1 step: 629, loss is 0.4525641202926636\n",
      "epoch: 1 step: 630, loss is 0.2710183262825012\n",
      "epoch: 1 step: 631, loss is 0.4722374677658081\n",
      "epoch: 1 step: 632, loss is 0.4639289677143097\n",
      "epoch: 1 step: 633, loss is 0.4557204842567444\n",
      "epoch: 1 step: 634, loss is 0.4547668695449829\n",
      "epoch: 1 step: 635, loss is 0.47161662578582764\n",
      "epoch: 1 step: 636, loss is 0.46208932995796204\n",
      "epoch: 1 step: 637, loss is 0.5323325395584106\n",
      "epoch: 1 step: 638, loss is 0.4610782861709595\n",
      "epoch: 1 step: 639, loss is 0.4145236611366272\n",
      "epoch: 1 step: 640, loss is 0.4540899097919464\n",
      "epoch: 1 step: 641, loss is 0.41611242294311523\n",
      "epoch: 1 step: 642, loss is 0.3389750123023987\n",
      "epoch: 1 step: 643, loss is 0.30437493324279785\n",
      "epoch: 1 step: 644, loss is 0.5819553136825562\n",
      "epoch: 1 step: 645, loss is 0.5601322054862976\n",
      "epoch: 1 step: 646, loss is 0.4482675790786743\n",
      "epoch: 1 step: 647, loss is 0.6016706228256226\n",
      "epoch: 1 step: 648, loss is 0.620219349861145\n",
      "epoch: 1 step: 649, loss is 0.5031941533088684\n",
      "epoch: 1 step: 650, loss is 0.37802308797836304\n",
      "epoch: 1 step: 651, loss is 0.5443817377090454\n",
      "epoch: 1 step: 652, loss is 0.37585967779159546\n",
      "epoch: 1 step: 653, loss is 0.32997292280197144\n",
      "epoch: 1 step: 654, loss is 0.5679855346679688\n",
      "epoch: 1 step: 655, loss is 0.48964545130729675\n",
      "epoch: 1 step: 656, loss is 0.42003875970840454\n",
      "epoch: 1 step: 657, loss is 0.39027106761932373\n",
      "epoch: 1 step: 658, loss is 0.5763435363769531\n",
      "epoch: 1 step: 659, loss is 0.40150219202041626\n",
      "epoch: 1 step: 660, loss is 0.5570271015167236\n",
      "epoch: 1 step: 661, loss is 0.4102717638015747\n",
      "epoch: 1 step: 662, loss is 0.41146788001060486\n",
      "epoch: 1 step: 663, loss is 0.4708077013492584\n",
      "epoch: 1 step: 664, loss is 0.4938332438468933\n",
      "epoch: 1 step: 665, loss is 0.3937531113624573\n",
      "epoch: 1 step: 666, loss is 0.436521053314209\n",
      "epoch: 1 step: 667, loss is 0.41403695940971375\n",
      "epoch: 1 step: 668, loss is 0.42434799671173096\n",
      "epoch: 1 step: 669, loss is 0.4338158369064331\n",
      "epoch: 1 step: 670, loss is 0.279491126537323\n",
      "epoch: 1 step: 671, loss is 0.5212820768356323\n",
      "epoch: 1 step: 672, loss is 0.5169415473937988\n",
      "epoch: 1 step: 673, loss is 0.3248489797115326\n",
      "epoch: 1 step: 674, loss is 0.4497870206832886\n",
      "epoch: 1 step: 675, loss is 0.37319374084472656\n",
      "epoch: 1 step: 676, loss is 0.3949738144874573\n",
      "epoch: 1 step: 677, loss is 0.4715880751609802\n",
      "epoch: 1 step: 678, loss is 0.3270273804664612\n",
      "epoch: 1 step: 679, loss is 0.4353094696998596\n",
      "epoch: 1 step: 680, loss is 0.45098602771759033\n",
      "epoch: 1 step: 681, loss is 0.6197256445884705\n",
      "epoch: 1 step: 682, loss is 0.6721882820129395\n",
      "epoch: 1 step: 683, loss is 0.622776985168457\n",
      "epoch: 1 step: 684, loss is 0.5450311899185181\n",
      "epoch: 1 step: 685, loss is 0.4405951499938965\n",
      "epoch: 1 step: 686, loss is 0.6138668060302734\n",
      "epoch: 1 step: 687, loss is 0.4861462712287903\n",
      "epoch: 1 step: 688, loss is 0.4399169683456421\n",
      "epoch: 1 step: 689, loss is 0.4920291304588318\n",
      "epoch: 1 step: 690, loss is 0.4623546004295349\n",
      "epoch: 1 step: 691, loss is 0.47637322545051575\n",
      "epoch: 1 step: 692, loss is 0.41069066524505615\n",
      "epoch: 1 step: 693, loss is 0.5740101337432861\n",
      "epoch: 1 step: 694, loss is 0.6082506775856018\n",
      "epoch: 1 step: 695, loss is 0.3898177146911621\n",
      "epoch: 1 step: 696, loss is 0.45015472173690796\n",
      "epoch: 1 step: 697, loss is 0.34361782670021057\n",
      "epoch: 1 step: 698, loss is 0.5479915738105774\n",
      "epoch: 1 step: 699, loss is 0.26047682762145996\n",
      "epoch: 1 step: 700, loss is 0.2246333658695221\n",
      "epoch: 1 step: 701, loss is 0.36700624227523804\n",
      "epoch: 1 step: 702, loss is 0.5307071805000305\n",
      "epoch: 1 step: 703, loss is 0.44109058380126953\n",
      "epoch: 1 step: 704, loss is 0.5495609641075134\n",
      "epoch: 1 step: 705, loss is 0.4506829082965851\n",
      "epoch: 1 step: 706, loss is 0.320998877286911\n",
      "epoch: 1 step: 707, loss is 0.29516008496284485\n",
      "epoch: 1 step: 708, loss is 0.5426552295684814\n",
      "epoch: 1 step: 709, loss is 0.5289679765701294\n",
      "epoch: 1 step: 710, loss is 0.41402241587638855\n",
      "epoch: 1 step: 711, loss is 0.354827344417572\n",
      "epoch: 1 step: 712, loss is 0.5439985394477844\n",
      "epoch: 1 step: 713, loss is 0.5562794804573059\n",
      "epoch: 1 step: 714, loss is 0.31887561082839966\n",
      "epoch: 1 step: 715, loss is 0.4526978135108948\n",
      "epoch: 1 step: 716, loss is 0.5574055910110474\n",
      "epoch: 1 step: 717, loss is 0.5279089212417603\n",
      "epoch: 1 step: 718, loss is 0.2658814787864685\n",
      "epoch: 1 step: 719, loss is 0.47247111797332764\n",
      "epoch: 1 step: 720, loss is 0.4937673807144165\n",
      "epoch: 1 step: 721, loss is 0.593377947807312\n",
      "epoch: 1 step: 722, loss is 0.5754591226577759\n",
      "epoch: 1 step: 723, loss is 0.33012109994888306\n",
      "epoch: 1 step: 724, loss is 0.5017261505126953\n",
      "epoch: 1 step: 725, loss is 0.4243537485599518\n",
      "epoch: 1 step: 726, loss is 0.5733820199966431\n",
      "epoch: 1 step: 727, loss is 0.272256463766098\n",
      "epoch: 1 step: 728, loss is 0.5961349010467529\n",
      "epoch: 1 step: 729, loss is 0.3137504458427429\n",
      "epoch: 1 step: 730, loss is 0.3969576060771942\n",
      "epoch: 1 step: 731, loss is 0.3063029646873474\n",
      "epoch: 1 step: 732, loss is 0.381076455116272\n",
      "epoch: 1 step: 733, loss is 0.6392393112182617\n",
      "epoch: 1 step: 734, loss is 0.38191765546798706\n",
      "epoch: 1 step: 735, loss is 0.5358072519302368\n",
      "epoch: 1 step: 736, loss is 0.4667300581932068\n",
      "epoch: 1 step: 737, loss is 0.4404597878456116\n",
      "epoch: 1 step: 738, loss is 0.5231419801712036\n",
      "epoch: 1 step: 739, loss is 0.5408837795257568\n",
      "epoch: 1 step: 740, loss is 0.4289880692958832\n",
      "epoch: 1 step: 741, loss is 0.4512108564376831\n",
      "epoch: 1 step: 742, loss is 0.4577281177043915\n",
      "epoch: 1 step: 743, loss is 0.4606284499168396\n",
      "epoch: 1 step: 744, loss is 0.4837000370025635\n",
      "epoch: 1 step: 745, loss is 0.34728217124938965\n",
      "epoch: 1 step: 746, loss is 0.4004825949668884\n",
      "epoch: 1 step: 747, loss is 0.31395965814590454\n",
      "epoch: 1 step: 748, loss is 0.23954908549785614\n",
      "epoch: 1 step: 749, loss is 0.34203705191612244\n",
      "epoch: 1 step: 750, loss is 0.44238704442977905\n",
      "epoch: 1 step: 751, loss is 0.6087468862533569\n",
      "epoch: 1 step: 752, loss is 0.36713147163391113\n",
      "epoch: 1 step: 753, loss is 0.27669447660446167\n",
      "epoch: 1 step: 754, loss is 0.4323616325855255\n",
      "epoch: 1 step: 755, loss is 0.2938358187675476\n",
      "epoch: 1 step: 756, loss is 0.3871823847293854\n",
      "epoch: 1 step: 757, loss is 0.40664443373680115\n",
      "epoch: 1 step: 758, loss is 0.23272499442100525\n",
      "epoch: 1 step: 759, loss is 0.5091211199760437\n",
      "epoch: 1 step: 760, loss is 0.6356199383735657\n",
      "epoch: 1 step: 761, loss is 0.3888288140296936\n",
      "epoch: 1 step: 762, loss is 0.3126160502433777\n",
      "epoch: 1 step: 763, loss is 0.3430960178375244\n",
      "epoch: 1 step: 764, loss is 0.3686580955982208\n",
      "epoch: 1 step: 765, loss is 0.47388824820518494\n",
      "epoch: 1 step: 766, loss is 0.48729151487350464\n",
      "epoch: 1 step: 767, loss is 0.46210792660713196\n",
      "epoch: 1 step: 768, loss is 0.47068464756011963\n",
      "epoch: 1 step: 769, loss is 0.4496646821498871\n",
      "epoch: 1 step: 770, loss is 0.3003852665424347\n",
      "epoch: 1 step: 771, loss is 0.45239418745040894\n",
      "epoch: 1 step: 772, loss is 0.6643580794334412\n",
      "epoch: 1 step: 773, loss is 0.37180304527282715\n",
      "epoch: 1 step: 774, loss is 0.4686053395271301\n",
      "epoch: 1 step: 775, loss is 0.39384791254997253\n",
      "epoch: 1 step: 776, loss is 0.4123328924179077\n",
      "epoch: 1 step: 777, loss is 0.34068626165390015\n",
      "epoch: 1 step: 778, loss is 0.585469126701355\n",
      "epoch: 1 step: 779, loss is 0.34498870372772217\n",
      "epoch: 1 step: 780, loss is 0.5381887555122375\n",
      "epoch: 1 step: 781, loss is 0.3144046664237976\n",
      "epoch: 1 step: 782, loss is 0.31952932476997375\n",
      "epoch: 1 step: 783, loss is 0.5499294400215149\n",
      "epoch: 1 step: 784, loss is 0.31590238213539124\n",
      "epoch: 1 step: 785, loss is 0.3965744078159332\n",
      "epoch: 1 step: 786, loss is 0.418532133102417\n",
      "epoch: 1 step: 787, loss is 0.46654489636421204\n",
      "epoch: 1 step: 788, loss is 0.5785607099533081\n",
      "epoch: 1 step: 789, loss is 0.5282666087150574\n",
      "epoch: 1 step: 790, loss is 0.3071390688419342\n",
      "epoch: 1 step: 791, loss is 0.3519306778907776\n",
      "epoch: 1 step: 792, loss is 0.576147198677063\n",
      "epoch: 1 step: 793, loss is 0.5673561692237854\n",
      "epoch: 1 step: 794, loss is 0.37390413880348206\n",
      "epoch: 1 step: 795, loss is 0.17780598998069763\n",
      "epoch: 1 step: 796, loss is 0.3644678592681885\n",
      "epoch: 1 step: 797, loss is 0.5850409269332886\n",
      "epoch: 1 step: 798, loss is 0.37364667654037476\n",
      "epoch: 1 step: 799, loss is 0.386474609375\n",
      "epoch: 1 step: 800, loss is 0.43540090322494507\n",
      "epoch: 1 step: 801, loss is 0.48790431022644043\n",
      "epoch: 1 step: 802, loss is 0.3496279716491699\n",
      "epoch: 1 step: 803, loss is 0.4193883240222931\n",
      "epoch: 1 step: 804, loss is 0.44648632407188416\n",
      "epoch: 1 step: 805, loss is 0.5935606956481934\n",
      "epoch: 1 step: 806, loss is 0.3455082178115845\n",
      "epoch: 1 step: 807, loss is 0.38957828283309937\n",
      "epoch: 1 step: 808, loss is 0.37812089920043945\n",
      "epoch: 1 step: 809, loss is 0.47360754013061523\n",
      "epoch: 1 step: 810, loss is 0.46971216797828674\n",
      "epoch: 1 step: 811, loss is 0.6441928148269653\n",
      "epoch: 1 step: 812, loss is 0.4586739242076874\n",
      "epoch: 1 step: 813, loss is 0.20651459693908691\n",
      "epoch: 1 step: 814, loss is 0.36223408579826355\n",
      "epoch: 1 step: 815, loss is 0.6408590078353882\n",
      "epoch: 1 step: 816, loss is 0.398526132106781\n",
      "epoch: 1 step: 817, loss is 0.5866012573242188\n",
      "epoch: 1 step: 818, loss is 0.3346382975578308\n",
      "epoch: 1 step: 819, loss is 0.4888307452201843\n",
      "epoch: 1 step: 820, loss is 0.7193479537963867\n",
      "epoch: 1 step: 821, loss is 0.39182087779045105\n",
      "epoch: 1 step: 822, loss is 0.42863786220550537\n",
      "epoch: 1 step: 823, loss is 0.4353262186050415\n",
      "epoch: 1 step: 824, loss is 0.48315244913101196\n",
      "epoch: 1 step: 825, loss is 0.3508679270744324\n",
      "epoch: 1 step: 826, loss is 0.44923728704452515\n",
      "epoch: 1 step: 827, loss is 0.380989670753479\n",
      "epoch: 1 step: 828, loss is 0.3497071862220764\n",
      "epoch: 1 step: 829, loss is 0.29181623458862305\n",
      "epoch: 1 step: 830, loss is 0.31020283699035645\n",
      "epoch: 1 step: 831, loss is 0.4693618416786194\n",
      "epoch: 1 step: 832, loss is 0.35129424929618835\n",
      "epoch: 1 step: 833, loss is 0.514620840549469\n",
      "epoch: 1 step: 834, loss is 0.6762518882751465\n",
      "epoch: 1 step: 835, loss is 0.33330023288726807\n",
      "epoch: 1 step: 836, loss is 0.315737247467041\n",
      "epoch: 1 step: 837, loss is 0.5693299770355225\n",
      "epoch: 1 step: 838, loss is 0.5484058856964111\n",
      "epoch: 1 step: 839, loss is 0.4145675599575043\n",
      "epoch: 1 step: 840, loss is 0.3677228093147278\n",
      "epoch: 1 step: 841, loss is 0.5800929069519043\n",
      "epoch: 1 step: 842, loss is 0.4214708209037781\n",
      "epoch: 1 step: 843, loss is 0.4164460301399231\n",
      "epoch: 1 step: 844, loss is 0.9247219562530518\n",
      "epoch: 1 step: 845, loss is 0.4680476188659668\n",
      "epoch: 1 step: 846, loss is 0.6578811407089233\n",
      "epoch: 1 step: 847, loss is 0.5752465128898621\n",
      "epoch: 1 step: 848, loss is 0.5833574533462524\n",
      "epoch: 1 step: 849, loss is 0.3756532669067383\n",
      "epoch: 1 step: 850, loss is 0.4425817131996155\n",
      "epoch: 1 step: 851, loss is 0.5162163972854614\n",
      "epoch: 1 step: 852, loss is 0.38804662227630615\n",
      "epoch: 1 step: 853, loss is 0.33381906151771545\n",
      "epoch: 1 step: 854, loss is 0.4715806245803833\n",
      "epoch: 1 step: 855, loss is 0.28428274393081665\n",
      "epoch: 1 step: 856, loss is 0.4044663906097412\n",
      "epoch: 1 step: 857, loss is 0.5534586906433105\n",
      "epoch: 1 step: 858, loss is 0.4254632592201233\n",
      "epoch: 1 step: 859, loss is 0.5933419466018677\n",
      "epoch: 1 step: 860, loss is 0.5230562686920166\n",
      "epoch: 1 step: 861, loss is 0.57150799036026\n",
      "epoch: 1 step: 862, loss is 0.43338102102279663\n",
      "epoch: 1 step: 863, loss is 0.46658313274383545\n",
      "epoch: 1 step: 864, loss is 0.6238574981689453\n",
      "epoch: 1 step: 865, loss is 0.42955392599105835\n",
      "epoch: 1 step: 866, loss is 0.4749739170074463\n",
      "epoch: 1 step: 867, loss is 0.37134283781051636\n",
      "epoch: 1 step: 868, loss is 0.7679951786994934\n",
      "epoch: 1 step: 869, loss is 0.3962526321411133\n",
      "epoch: 1 step: 870, loss is 0.2916070222854614\n",
      "epoch: 1 step: 871, loss is 0.5725407600402832\n",
      "epoch: 1 step: 872, loss is 0.3959426283836365\n",
      "epoch: 1 step: 873, loss is 0.7486022710800171\n",
      "epoch: 1 step: 874, loss is 0.3049849271774292\n",
      "epoch: 1 step: 875, loss is 0.38252079486846924\n",
      "epoch: 1 step: 876, loss is 0.38358983397483826\n",
      "epoch: 1 step: 877, loss is 0.5385677218437195\n",
      "epoch: 1 step: 878, loss is 0.5250484943389893\n",
      "epoch: 1 step: 879, loss is 0.5978733897209167\n",
      "epoch: 1 step: 880, loss is 0.6375417113304138\n",
      "epoch: 1 step: 881, loss is 0.5010074973106384\n",
      "epoch: 1 step: 882, loss is 0.5434790253639221\n",
      "epoch: 1 step: 883, loss is 0.2689066529273987\n",
      "epoch: 1 step: 884, loss is 0.46000024676322937\n",
      "epoch: 1 step: 885, loss is 0.40802398324012756\n",
      "epoch: 1 step: 886, loss is 0.5070793032646179\n",
      "epoch: 1 step: 887, loss is 0.4093033969402313\n",
      "epoch: 1 step: 888, loss is 0.4078335762023926\n",
      "epoch: 1 step: 889, loss is 0.2608903646469116\n",
      "epoch: 1 step: 890, loss is 0.4007684290409088\n",
      "epoch: 1 step: 891, loss is 0.2201227843761444\n",
      "epoch: 1 step: 892, loss is 0.7040791511535645\n",
      "epoch: 1 step: 893, loss is 0.4985964000225067\n",
      "epoch: 1 step: 894, loss is 0.42455819249153137\n",
      "epoch: 1 step: 895, loss is 0.4679489731788635\n",
      "epoch: 1 step: 896, loss is 0.4894760549068451\n",
      "epoch: 1 step: 897, loss is 0.5881239175796509\n",
      "epoch: 1 step: 898, loss is 0.49035128951072693\n",
      "epoch: 1 step: 899, loss is 0.3880073130130768\n",
      "epoch: 1 step: 900, loss is 0.41862282156944275\n",
      "epoch: 1 step: 901, loss is 0.2661546468734741\n",
      "epoch: 1 step: 902, loss is 0.36587977409362793\n",
      "epoch: 1 step: 903, loss is 0.45151084661483765\n",
      "epoch: 1 step: 904, loss is 0.5375263094902039\n",
      "epoch: 1 step: 905, loss is 0.37830373644828796\n",
      "epoch: 1 step: 906, loss is 0.4692501723766327\n",
      "epoch: 1 step: 907, loss is 0.2822030782699585\n",
      "epoch: 1 step: 908, loss is 0.44215232133865356\n",
      "epoch: 1 step: 909, loss is 0.3292582333087921\n",
      "epoch: 1 step: 910, loss is 0.49152663350105286\n",
      "epoch: 1 step: 911, loss is 0.3849391043186188\n",
      "epoch: 1 step: 912, loss is 0.5347555875778198\n",
      "epoch: 1 step: 913, loss is 0.4578244090080261\n",
      "epoch: 1 step: 914, loss is 0.6278963088989258\n",
      "epoch: 1 step: 915, loss is 0.4550939202308655\n",
      "epoch: 1 step: 916, loss is 0.6074333786964417\n",
      "epoch: 1 step: 917, loss is 0.43289071321487427\n",
      "epoch: 1 step: 918, loss is 0.4849192798137665\n",
      "epoch: 1 step: 919, loss is 0.2980077266693115\n",
      "epoch: 1 step: 920, loss is 0.3462908864021301\n",
      "epoch: 1 step: 921, loss is 0.3338812589645386\n",
      "epoch: 1 step: 922, loss is 0.4710462689399719\n",
      "epoch: 1 step: 923, loss is 0.41995400190353394\n",
      "epoch: 1 step: 924, loss is 0.39367014169692993\n",
      "epoch: 1 step: 925, loss is 0.3473588824272156\n",
      "epoch: 1 step: 926, loss is 0.47763791680336\n",
      "epoch: 1 step: 927, loss is 0.42513948678970337\n",
      "epoch: 1 step: 928, loss is 0.35401099920272827\n",
      "epoch: 1 step: 929, loss is 0.32394564151763916\n",
      "epoch: 1 step: 930, loss is 0.29805809259414673\n",
      "epoch: 1 step: 931, loss is 0.30984482169151306\n",
      "epoch: 1 step: 932, loss is 0.4971511960029602\n",
      "epoch: 1 step: 933, loss is 0.4523220658302307\n",
      "epoch: 1 step: 934, loss is 0.5513007640838623\n",
      "epoch: 1 step: 935, loss is 0.7688679099082947\n",
      "epoch: 1 step: 936, loss is 0.5237149596214294\n",
      "epoch: 1 step: 937, loss is 0.5646471977233887\n",
      "epoch: 2 step: 1, loss is 0.45105892419815063\n",
      "epoch: 2 step: 2, loss is 0.49389249086380005\n",
      "epoch: 2 step: 3, loss is 0.31898248195648193\n",
      "epoch: 2 step: 4, loss is 0.36510276794433594\n",
      "epoch: 2 step: 5, loss is 0.43101251125335693\n",
      "epoch: 2 step: 6, loss is 0.3320363163948059\n",
      "epoch: 2 step: 7, loss is 0.33085620403289795\n",
      "epoch: 2 step: 8, loss is 0.2577250301837921\n",
      "epoch: 2 step: 9, loss is 0.26656636595726013\n",
      "epoch: 2 step: 10, loss is 0.5972065329551697\n",
      "epoch: 2 step: 11, loss is 0.36528944969177246\n",
      "epoch: 2 step: 12, loss is 0.4371297359466553\n",
      "epoch: 2 step: 13, loss is 0.2926658093929291\n",
      "epoch: 2 step: 14, loss is 0.4089481234550476\n",
      "epoch: 2 step: 15, loss is 0.3233394920825958\n",
      "epoch: 2 step: 16, loss is 0.6174944043159485\n",
      "epoch: 2 step: 17, loss is 0.3405618667602539\n",
      "epoch: 2 step: 18, loss is 0.6654955744743347\n",
      "epoch: 2 step: 19, loss is 0.5144641399383545\n",
      "epoch: 2 step: 20, loss is 0.3910304009914398\n",
      "epoch: 2 step: 21, loss is 0.3892846703529358\n",
      "epoch: 2 step: 22, loss is 0.375188946723938\n",
      "epoch: 2 step: 23, loss is 0.5247299671173096\n",
      "epoch: 2 step: 24, loss is 0.5058193802833557\n",
      "epoch: 2 step: 25, loss is 0.6186692714691162\n",
      "epoch: 2 step: 26, loss is 0.5222445130348206\n",
      "epoch: 2 step: 27, loss is 0.36907708644866943\n",
      "epoch: 2 step: 28, loss is 0.40546607971191406\n",
      "epoch: 2 step: 29, loss is 0.5150160789489746\n",
      "epoch: 2 step: 30, loss is 0.418626070022583\n",
      "epoch: 2 step: 31, loss is 0.5671433806419373\n",
      "epoch: 2 step: 32, loss is 0.43021059036254883\n",
      "epoch: 2 step: 33, loss is 0.322496622800827\n",
      "epoch: 2 step: 34, loss is 0.43130844831466675\n",
      "epoch: 2 step: 35, loss is 0.43759462237358093\n",
      "epoch: 2 step: 36, loss is 0.3857850730419159\n",
      "epoch: 2 step: 37, loss is 0.8743434548377991\n",
      "epoch: 2 step: 38, loss is 0.22667020559310913\n",
      "epoch: 2 step: 39, loss is 0.5907031297683716\n",
      "epoch: 2 step: 40, loss is 0.3704048693180084\n",
      "epoch: 2 step: 41, loss is 0.600814700126648\n",
      "epoch: 2 step: 42, loss is 0.262986958026886\n",
      "epoch: 2 step: 43, loss is 0.3192428648471832\n",
      "epoch: 2 step: 44, loss is 0.630387544631958\n",
      "epoch: 2 step: 45, loss is 0.4030589163303375\n",
      "epoch: 2 step: 46, loss is 0.428453266620636\n",
      "epoch: 2 step: 47, loss is 0.5350202322006226\n",
      "epoch: 2 step: 48, loss is 0.3085293769836426\n",
      "epoch: 2 step: 49, loss is 0.6899469494819641\n",
      "epoch: 2 step: 50, loss is 0.3299860954284668\n",
      "epoch: 2 step: 51, loss is 0.5453430414199829\n",
      "epoch: 2 step: 52, loss is 0.3047868311405182\n",
      "epoch: 2 step: 53, loss is 0.4444960355758667\n",
      "epoch: 2 step: 54, loss is 0.3882942795753479\n",
      "epoch: 2 step: 55, loss is 0.344618558883667\n",
      "epoch: 2 step: 56, loss is 0.4138171672821045\n",
      "epoch: 2 step: 57, loss is 0.4496932625770569\n",
      "epoch: 2 step: 58, loss is 0.31622588634490967\n",
      "epoch: 2 step: 59, loss is 0.3555208146572113\n",
      "epoch: 2 step: 60, loss is 0.4429977238178253\n",
      "epoch: 2 step: 61, loss is 0.4465041160583496\n",
      "epoch: 2 step: 62, loss is 0.5218268036842346\n",
      "epoch: 2 step: 63, loss is 0.5346182584762573\n",
      "epoch: 2 step: 64, loss is 0.38683265447616577\n",
      "epoch: 2 step: 65, loss is 0.35623830556869507\n",
      "epoch: 2 step: 66, loss is 0.5329638719558716\n",
      "epoch: 2 step: 67, loss is 0.387438029050827\n",
      "epoch: 2 step: 68, loss is 0.45461851358413696\n",
      "epoch: 2 step: 69, loss is 0.3553959131240845\n",
      "epoch: 2 step: 70, loss is 0.37673014402389526\n",
      "epoch: 2 step: 71, loss is 0.32602089643478394\n",
      "epoch: 2 step: 72, loss is 0.5243626236915588\n",
      "epoch: 2 step: 73, loss is 0.41715049743652344\n",
      "epoch: 2 step: 74, loss is 0.44741952419281006\n",
      "epoch: 2 step: 75, loss is 0.4037517309188843\n",
      "epoch: 2 step: 76, loss is 0.44394707679748535\n",
      "epoch: 2 step: 77, loss is 0.3532271087169647\n",
      "epoch: 2 step: 78, loss is 0.4111323952674866\n",
      "epoch: 2 step: 79, loss is 0.438460111618042\n",
      "epoch: 2 step: 80, loss is 0.48308634757995605\n",
      "epoch: 2 step: 81, loss is 0.40165191888809204\n",
      "epoch: 2 step: 82, loss is 0.38366562128067017\n",
      "epoch: 2 step: 83, loss is 0.3736162781715393\n",
      "epoch: 2 step: 84, loss is 0.3466801643371582\n",
      "epoch: 2 step: 85, loss is 0.38017719984054565\n",
      "epoch: 2 step: 86, loss is 0.32148998975753784\n",
      "epoch: 2 step: 87, loss is 0.2829221487045288\n",
      "epoch: 2 step: 88, loss is 0.3436408042907715\n",
      "epoch: 2 step: 89, loss is 0.4465760588645935\n",
      "epoch: 2 step: 90, loss is 0.2458949089050293\n",
      "epoch: 2 step: 91, loss is 0.4190542995929718\n",
      "epoch: 2 step: 92, loss is 0.2738338112831116\n",
      "epoch: 2 step: 93, loss is 0.27669191360473633\n",
      "epoch: 2 step: 94, loss is 0.48886388540267944\n",
      "epoch: 2 step: 95, loss is 0.45881974697113037\n",
      "epoch: 2 step: 96, loss is 0.4981045126914978\n",
      "epoch: 2 step: 97, loss is 0.2846059501171112\n",
      "epoch: 2 step: 98, loss is 0.5705671310424805\n",
      "epoch: 2 step: 99, loss is 0.40343695878982544\n",
      "epoch: 2 step: 100, loss is 0.46139830350875854\n",
      "epoch: 2 step: 101, loss is 0.354524165391922\n",
      "epoch: 2 step: 102, loss is 0.3180403709411621\n",
      "epoch: 2 step: 103, loss is 0.4267699122428894\n",
      "epoch: 2 step: 104, loss is 0.420291006565094\n",
      "epoch: 2 step: 105, loss is 0.5978209376335144\n",
      "epoch: 2 step: 106, loss is 0.5612255930900574\n",
      "epoch: 2 step: 107, loss is 0.3015422224998474\n",
      "epoch: 2 step: 108, loss is 0.44594818353652954\n",
      "epoch: 2 step: 109, loss is 0.46161749958992004\n",
      "epoch: 2 step: 110, loss is 0.28561559319496155\n",
      "epoch: 2 step: 111, loss is 0.4024116098880768\n",
      "epoch: 2 step: 112, loss is 0.42727142572402954\n",
      "epoch: 2 step: 113, loss is 0.29485851526260376\n",
      "epoch: 2 step: 114, loss is 0.4479384124279022\n",
      "epoch: 2 step: 115, loss is 0.3380451202392578\n",
      "epoch: 2 step: 116, loss is 0.455549031496048\n",
      "epoch: 2 step: 117, loss is 0.45898333191871643\n",
      "epoch: 2 step: 118, loss is 0.4950997829437256\n",
      "epoch: 2 step: 119, loss is 0.3538922667503357\n",
      "epoch: 2 step: 120, loss is 0.2970955967903137\n",
      "epoch: 2 step: 121, loss is 0.3817318081855774\n",
      "epoch: 2 step: 122, loss is 0.48650500178337097\n",
      "epoch: 2 step: 123, loss is 0.48602211475372314\n",
      "epoch: 2 step: 124, loss is 0.4245239496231079\n",
      "epoch: 2 step: 125, loss is 0.39978456497192383\n",
      "epoch: 2 step: 126, loss is 0.5217704772949219\n",
      "epoch: 2 step: 127, loss is 0.365595281124115\n",
      "epoch: 2 step: 128, loss is 0.4675880968570709\n",
      "epoch: 2 step: 129, loss is 0.4504353702068329\n",
      "epoch: 2 step: 130, loss is 0.3229225277900696\n",
      "epoch: 2 step: 131, loss is 0.5093141794204712\n",
      "epoch: 2 step: 132, loss is 0.5663455128669739\n",
      "epoch: 2 step: 133, loss is 0.26039060950279236\n",
      "epoch: 2 step: 134, loss is 0.2738787531852722\n",
      "epoch: 2 step: 135, loss is 0.6514953970909119\n",
      "epoch: 2 step: 136, loss is 0.5313866138458252\n",
      "epoch: 2 step: 137, loss is 0.3031988739967346\n",
      "epoch: 2 step: 138, loss is 0.29825541377067566\n",
      "epoch: 2 step: 139, loss is 0.338864803314209\n",
      "epoch: 2 step: 140, loss is 0.3914066553115845\n",
      "epoch: 2 step: 141, loss is 0.6018266677856445\n",
      "epoch: 2 step: 142, loss is 0.3807356357574463\n",
      "epoch: 2 step: 143, loss is 0.3590032458305359\n",
      "epoch: 2 step: 144, loss is 0.43258512020111084\n",
      "epoch: 2 step: 145, loss is 0.4207347333431244\n",
      "epoch: 2 step: 146, loss is 0.45172566175460815\n",
      "epoch: 2 step: 147, loss is 0.37819498777389526\n",
      "epoch: 2 step: 148, loss is 0.2464269995689392\n",
      "epoch: 2 step: 149, loss is 0.35319873690605164\n",
      "epoch: 2 step: 150, loss is 0.214621901512146\n",
      "epoch: 2 step: 151, loss is 0.3666965365409851\n",
      "epoch: 2 step: 152, loss is 0.3767291307449341\n",
      "epoch: 2 step: 153, loss is 0.32361310720443726\n",
      "epoch: 2 step: 154, loss is 0.32186993956565857\n",
      "epoch: 2 step: 155, loss is 0.46520766615867615\n",
      "epoch: 2 step: 156, loss is 0.48579713702201843\n",
      "epoch: 2 step: 157, loss is 0.3769729733467102\n",
      "epoch: 2 step: 158, loss is 0.33681872487068176\n",
      "epoch: 2 step: 159, loss is 0.353071004152298\n",
      "epoch: 2 step: 160, loss is 0.35942238569259644\n",
      "epoch: 2 step: 161, loss is 0.37719017267227173\n",
      "epoch: 2 step: 162, loss is 0.5515640377998352\n",
      "epoch: 2 step: 163, loss is 0.453887939453125\n",
      "epoch: 2 step: 164, loss is 0.3440021574497223\n",
      "epoch: 2 step: 165, loss is 0.4317835569381714\n",
      "epoch: 2 step: 166, loss is 0.5040723085403442\n",
      "epoch: 2 step: 167, loss is 0.40800607204437256\n",
      "epoch: 2 step: 168, loss is 0.5547662973403931\n",
      "epoch: 2 step: 169, loss is 0.4002366364002228\n",
      "epoch: 2 step: 170, loss is 0.24236449599266052\n",
      "epoch: 2 step: 171, loss is 0.3566147983074188\n",
      "epoch: 2 step: 172, loss is 0.40558797121047974\n",
      "epoch: 2 step: 173, loss is 0.4014924168586731\n",
      "epoch: 2 step: 174, loss is 0.4921846091747284\n",
      "epoch: 2 step: 175, loss is 0.436567485332489\n",
      "epoch: 2 step: 176, loss is 0.5008231997489929\n",
      "epoch: 2 step: 177, loss is 0.4975367784500122\n",
      "epoch: 2 step: 178, loss is 0.35220545530319214\n",
      "epoch: 2 step: 179, loss is 0.3123514652252197\n",
      "epoch: 2 step: 180, loss is 0.3808269202709198\n",
      "epoch: 2 step: 181, loss is 0.3454137146472931\n",
      "epoch: 2 step: 182, loss is 0.623395562171936\n",
      "epoch: 2 step: 183, loss is 0.4701025187969208\n",
      "epoch: 2 step: 184, loss is 0.5086565017700195\n",
      "epoch: 2 step: 185, loss is 0.42710331082344055\n",
      "epoch: 2 step: 186, loss is 0.6455104947090149\n",
      "epoch: 2 step: 187, loss is 0.31604257225990295\n",
      "epoch: 2 step: 188, loss is 0.38907456398010254\n",
      "epoch: 2 step: 189, loss is 0.44179174304008484\n",
      "epoch: 2 step: 190, loss is 0.2708076536655426\n",
      "epoch: 2 step: 191, loss is 0.5470660328865051\n",
      "epoch: 2 step: 192, loss is 0.4948441982269287\n",
      "epoch: 2 step: 193, loss is 0.3180577754974365\n",
      "epoch: 2 step: 194, loss is 0.541050136089325\n",
      "epoch: 2 step: 195, loss is 0.4294770359992981\n",
      "epoch: 2 step: 196, loss is 0.49760884046554565\n",
      "epoch: 2 step: 197, loss is 0.31795287132263184\n",
      "epoch: 2 step: 198, loss is 0.2894516587257385\n",
      "epoch: 2 step: 199, loss is 0.32786014676094055\n",
      "epoch: 2 step: 200, loss is 0.6016234159469604\n",
      "epoch: 2 step: 201, loss is 0.5581082105636597\n",
      "epoch: 2 step: 202, loss is 0.30931201577186584\n",
      "epoch: 2 step: 203, loss is 0.392259806394577\n",
      "epoch: 2 step: 204, loss is 0.37732136249542236\n",
      "epoch: 2 step: 205, loss is 0.30765917897224426\n",
      "epoch: 2 step: 206, loss is 0.4610629975795746\n",
      "epoch: 2 step: 207, loss is 0.33395034074783325\n",
      "epoch: 2 step: 208, loss is 0.4546128511428833\n",
      "epoch: 2 step: 209, loss is 0.5282996892929077\n",
      "epoch: 2 step: 210, loss is 0.4047441780567169\n",
      "epoch: 2 step: 211, loss is 0.5665993094444275\n",
      "epoch: 2 step: 212, loss is 0.5976758599281311\n",
      "epoch: 2 step: 213, loss is 0.48320263624191284\n",
      "epoch: 2 step: 214, loss is 0.551786482334137\n",
      "epoch: 2 step: 215, loss is 0.3363884687423706\n",
      "epoch: 2 step: 216, loss is 0.28691554069519043\n",
      "epoch: 2 step: 217, loss is 0.3194231390953064\n",
      "epoch: 2 step: 218, loss is 0.47913938760757446\n",
      "epoch: 2 step: 219, loss is 0.3295849859714508\n",
      "epoch: 2 step: 220, loss is 0.4544886648654938\n",
      "epoch: 2 step: 221, loss is 0.5420191287994385\n",
      "epoch: 2 step: 222, loss is 0.5241503715515137\n",
      "epoch: 2 step: 223, loss is 0.43989813327789307\n",
      "epoch: 2 step: 224, loss is 0.5801674723625183\n",
      "epoch: 2 step: 225, loss is 0.251173734664917\n",
      "epoch: 2 step: 226, loss is 0.30743294954299927\n",
      "epoch: 2 step: 227, loss is 0.5415430665016174\n",
      "epoch: 2 step: 228, loss is 0.29511696100234985\n",
      "epoch: 2 step: 229, loss is 0.4188270568847656\n",
      "epoch: 2 step: 230, loss is 0.4375374913215637\n",
      "epoch: 2 step: 231, loss is 0.48297765851020813\n",
      "epoch: 2 step: 232, loss is 0.565330445766449\n",
      "epoch: 2 step: 233, loss is 0.2773219645023346\n",
      "epoch: 2 step: 234, loss is 0.5495617389678955\n",
      "epoch: 2 step: 235, loss is 0.5458407402038574\n",
      "epoch: 2 step: 236, loss is 0.39787983894348145\n",
      "epoch: 2 step: 237, loss is 0.5935380458831787\n",
      "epoch: 2 step: 238, loss is 0.370513916015625\n",
      "epoch: 2 step: 239, loss is 0.34139758348464966\n",
      "epoch: 2 step: 240, loss is 0.41729557514190674\n",
      "epoch: 2 step: 241, loss is 0.35430729389190674\n",
      "epoch: 2 step: 242, loss is 0.4800299406051636\n",
      "epoch: 2 step: 243, loss is 0.37491777539253235\n",
      "epoch: 2 step: 244, loss is 0.4089319407939911\n",
      "epoch: 2 step: 245, loss is 0.6836824417114258\n",
      "epoch: 2 step: 246, loss is 0.7243225574493408\n",
      "epoch: 2 step: 247, loss is 0.3435847759246826\n",
      "epoch: 2 step: 248, loss is 0.31043896079063416\n",
      "epoch: 2 step: 249, loss is 0.4524749219417572\n",
      "epoch: 2 step: 250, loss is 0.45486992597579956\n",
      "epoch: 2 step: 251, loss is 0.6232075691223145\n",
      "epoch: 2 step: 252, loss is 0.26405346393585205\n",
      "epoch: 2 step: 253, loss is 0.3662327826023102\n",
      "epoch: 2 step: 254, loss is 0.5523753762245178\n",
      "epoch: 2 step: 255, loss is 0.4746975600719452\n",
      "epoch: 2 step: 256, loss is 0.48987120389938354\n",
      "epoch: 2 step: 257, loss is 0.425281286239624\n",
      "epoch: 2 step: 258, loss is 0.3771664500236511\n",
      "epoch: 2 step: 259, loss is 0.3598599433898926\n",
      "epoch: 2 step: 260, loss is 0.4781140685081482\n",
      "epoch: 2 step: 261, loss is 0.40589195489883423\n",
      "epoch: 2 step: 262, loss is 0.3617880940437317\n",
      "epoch: 2 step: 263, loss is 0.5849949717521667\n",
      "epoch: 2 step: 264, loss is 0.2991600036621094\n",
      "epoch: 2 step: 265, loss is 0.4598569869995117\n",
      "epoch: 2 step: 266, loss is 0.2852139472961426\n",
      "epoch: 2 step: 267, loss is 0.5009874701499939\n",
      "epoch: 2 step: 268, loss is 0.2715716063976288\n",
      "epoch: 2 step: 269, loss is 0.4445359706878662\n",
      "epoch: 2 step: 270, loss is 0.3259132206439972\n",
      "epoch: 2 step: 271, loss is 0.35066014528274536\n",
      "epoch: 2 step: 272, loss is 0.5631732940673828\n",
      "epoch: 2 step: 273, loss is 0.4254663288593292\n",
      "epoch: 2 step: 274, loss is 0.41281116008758545\n",
      "epoch: 2 step: 275, loss is 0.40188711881637573\n",
      "epoch: 2 step: 276, loss is 0.5927382707595825\n",
      "epoch: 2 step: 277, loss is 0.30545687675476074\n",
      "epoch: 2 step: 278, loss is 0.5628111362457275\n",
      "epoch: 2 step: 279, loss is 0.31817832589149475\n",
      "epoch: 2 step: 280, loss is 0.4744705557823181\n",
      "epoch: 2 step: 281, loss is 0.309021532535553\n",
      "epoch: 2 step: 282, loss is 0.5076520442962646\n",
      "epoch: 2 step: 283, loss is 0.774701714515686\n",
      "epoch: 2 step: 284, loss is 0.33543241024017334\n",
      "epoch: 2 step: 285, loss is 0.5945649147033691\n",
      "epoch: 2 step: 286, loss is 0.45705488324165344\n",
      "epoch: 2 step: 287, loss is 0.3509373068809509\n",
      "epoch: 2 step: 288, loss is 0.42380189895629883\n",
      "epoch: 2 step: 289, loss is 0.32372257113456726\n",
      "epoch: 2 step: 290, loss is 0.2839251756668091\n",
      "epoch: 2 step: 291, loss is 0.3545393943786621\n",
      "epoch: 2 step: 292, loss is 0.37539803981781006\n",
      "epoch: 2 step: 293, loss is 0.41948169469833374\n",
      "epoch: 2 step: 294, loss is 0.406072199344635\n",
      "epoch: 2 step: 295, loss is 0.41674453020095825\n",
      "epoch: 2 step: 296, loss is 0.3926275372505188\n",
      "epoch: 2 step: 297, loss is 0.35653993487358093\n",
      "epoch: 2 step: 298, loss is 0.292269229888916\n",
      "epoch: 2 step: 299, loss is 0.48285147547721863\n",
      "epoch: 2 step: 300, loss is 0.3344114422798157\n",
      "epoch: 2 step: 301, loss is 0.38037049770355225\n",
      "epoch: 2 step: 302, loss is 0.4938390254974365\n",
      "epoch: 2 step: 303, loss is 0.44149351119995117\n",
      "epoch: 2 step: 304, loss is 0.33282527327537537\n",
      "epoch: 2 step: 305, loss is 0.39800548553466797\n",
      "epoch: 2 step: 306, loss is 0.374896377325058\n",
      "epoch: 2 step: 307, loss is 0.4981904625892639\n",
      "epoch: 2 step: 308, loss is 0.2776932716369629\n",
      "epoch: 2 step: 309, loss is 0.4468196928501129\n",
      "epoch: 2 step: 310, loss is 0.39595869183540344\n",
      "epoch: 2 step: 311, loss is 0.3466914892196655\n",
      "epoch: 2 step: 312, loss is 0.5012783408164978\n",
      "epoch: 2 step: 313, loss is 0.4295588731765747\n",
      "epoch: 2 step: 314, loss is 0.4085789918899536\n",
      "epoch: 2 step: 315, loss is 0.5329310297966003\n",
      "epoch: 2 step: 316, loss is 0.6134558916091919\n",
      "epoch: 2 step: 317, loss is 0.5296312570571899\n",
      "epoch: 2 step: 318, loss is 0.3269999623298645\n",
      "epoch: 2 step: 319, loss is 0.6224462985992432\n",
      "epoch: 2 step: 320, loss is 0.5284281969070435\n",
      "epoch: 2 step: 321, loss is 0.5078850388526917\n",
      "epoch: 2 step: 322, loss is 0.49754974246025085\n",
      "epoch: 2 step: 323, loss is 0.5041710138320923\n",
      "epoch: 2 step: 324, loss is 0.38235795497894287\n",
      "epoch: 2 step: 325, loss is 0.3329867422580719\n",
      "epoch: 2 step: 326, loss is 0.3004821836948395\n",
      "epoch: 2 step: 327, loss is 0.4488769471645355\n",
      "epoch: 2 step: 328, loss is 0.40660226345062256\n",
      "epoch: 2 step: 329, loss is 0.3259560763835907\n",
      "epoch: 2 step: 330, loss is 0.26736879348754883\n",
      "epoch: 2 step: 331, loss is 0.38356494903564453\n",
      "epoch: 2 step: 332, loss is 0.2967507839202881\n",
      "epoch: 2 step: 333, loss is 0.1939002275466919\n",
      "epoch: 2 step: 334, loss is 0.5963216423988342\n",
      "epoch: 2 step: 335, loss is 0.5365238785743713\n",
      "epoch: 2 step: 336, loss is 0.4301491677761078\n",
      "epoch: 2 step: 337, loss is 0.3885767161846161\n",
      "epoch: 2 step: 338, loss is 0.5825626850128174\n",
      "epoch: 2 step: 339, loss is 0.3153553605079651\n",
      "epoch: 2 step: 340, loss is 0.45407480001449585\n",
      "epoch: 2 step: 341, loss is 0.44470661878585815\n",
      "epoch: 2 step: 342, loss is 0.36726322770118713\n",
      "epoch: 2 step: 343, loss is 0.2829671800136566\n",
      "epoch: 2 step: 344, loss is 0.47254878282546997\n",
      "epoch: 2 step: 345, loss is 0.34135138988494873\n",
      "epoch: 2 step: 346, loss is 0.32187747955322266\n",
      "epoch: 2 step: 347, loss is 0.4432987570762634\n",
      "epoch: 2 step: 348, loss is 0.6662598848342896\n",
      "epoch: 2 step: 349, loss is 0.29147082567214966\n",
      "epoch: 2 step: 350, loss is 0.442487895488739\n",
      "epoch: 2 step: 351, loss is 0.2490742802619934\n",
      "epoch: 2 step: 352, loss is 0.5695008039474487\n",
      "epoch: 2 step: 353, loss is 0.24826757609844208\n",
      "epoch: 2 step: 354, loss is 0.4551926851272583\n",
      "epoch: 2 step: 355, loss is 0.49669307470321655\n",
      "epoch: 2 step: 356, loss is 0.4904479682445526\n",
      "epoch: 2 step: 357, loss is 0.5132743120193481\n",
      "epoch: 2 step: 358, loss is 0.5608978867530823\n",
      "epoch: 2 step: 359, loss is 0.4837952256202698\n",
      "epoch: 2 step: 360, loss is 0.3390473425388336\n",
      "epoch: 2 step: 361, loss is 0.3206890821456909\n",
      "epoch: 2 step: 362, loss is 0.46301940083503723\n",
      "epoch: 2 step: 363, loss is 0.37556174397468567\n",
      "epoch: 2 step: 364, loss is 0.32288363575935364\n",
      "epoch: 2 step: 365, loss is 0.4966936707496643\n",
      "epoch: 2 step: 366, loss is 0.4965232014656067\n",
      "epoch: 2 step: 367, loss is 0.2970731854438782\n",
      "epoch: 2 step: 368, loss is 0.2910672426223755\n",
      "epoch: 2 step: 369, loss is 0.39065924286842346\n",
      "epoch: 2 step: 370, loss is 0.4428718090057373\n",
      "epoch: 2 step: 371, loss is 0.4039863348007202\n",
      "epoch: 2 step: 372, loss is 0.46194058656692505\n",
      "epoch: 2 step: 373, loss is 0.284209668636322\n",
      "epoch: 2 step: 374, loss is 0.44300293922424316\n",
      "epoch: 2 step: 375, loss is 0.3312841057777405\n",
      "epoch: 2 step: 376, loss is 0.32261064648628235\n",
      "epoch: 2 step: 377, loss is 0.3786280155181885\n",
      "epoch: 2 step: 378, loss is 0.37391582131385803\n",
      "epoch: 2 step: 379, loss is 0.3147999048233032\n",
      "epoch: 2 step: 380, loss is 0.26668038964271545\n",
      "epoch: 2 step: 381, loss is 0.3471974730491638\n",
      "epoch: 2 step: 382, loss is 0.18131834268569946\n",
      "epoch: 2 step: 383, loss is 0.23241019248962402\n",
      "epoch: 2 step: 384, loss is 0.37350213527679443\n",
      "epoch: 2 step: 385, loss is 0.26179027557373047\n",
      "epoch: 2 step: 386, loss is 0.290534645318985\n",
      "epoch: 2 step: 387, loss is 0.4364658296108246\n",
      "epoch: 2 step: 388, loss is 0.4414014518260956\n",
      "epoch: 2 step: 389, loss is 0.5095457434654236\n",
      "epoch: 2 step: 390, loss is 0.4073614478111267\n",
      "epoch: 2 step: 391, loss is 0.4749263525009155\n",
      "epoch: 2 step: 392, loss is 0.27569276094436646\n",
      "epoch: 2 step: 393, loss is 0.4393799901008606\n",
      "epoch: 2 step: 394, loss is 0.3470953404903412\n",
      "epoch: 2 step: 395, loss is 0.36669158935546875\n",
      "epoch: 2 step: 396, loss is 0.21768900752067566\n",
      "epoch: 2 step: 397, loss is 0.29051673412323\n",
      "epoch: 2 step: 398, loss is 0.39538663625717163\n",
      "epoch: 2 step: 399, loss is 0.37876880168914795\n",
      "epoch: 2 step: 400, loss is 0.3304978609085083\n",
      "epoch: 2 step: 401, loss is 0.6209322214126587\n",
      "epoch: 2 step: 402, loss is 0.3257875442504883\n",
      "epoch: 2 step: 403, loss is 0.3315034508705139\n",
      "epoch: 2 step: 404, loss is 0.3498464822769165\n",
      "epoch: 2 step: 405, loss is 0.44637787342071533\n",
      "epoch: 2 step: 406, loss is 0.3791041374206543\n",
      "epoch: 2 step: 407, loss is 0.440614253282547\n",
      "epoch: 2 step: 408, loss is 0.29871562123298645\n",
      "epoch: 2 step: 409, loss is 0.37088847160339355\n",
      "epoch: 2 step: 410, loss is 0.42832478880882263\n",
      "epoch: 2 step: 411, loss is 0.5811743140220642\n",
      "epoch: 2 step: 412, loss is 0.3001520037651062\n",
      "epoch: 2 step: 413, loss is 0.4402162432670593\n",
      "epoch: 2 step: 414, loss is 0.35935282707214355\n",
      "epoch: 2 step: 415, loss is 0.443007230758667\n",
      "epoch: 2 step: 416, loss is 0.46490252017974854\n",
      "epoch: 2 step: 417, loss is 0.38233888149261475\n",
      "epoch: 2 step: 418, loss is 0.2721235752105713\n",
      "epoch: 2 step: 419, loss is 0.43402788043022156\n",
      "epoch: 2 step: 420, loss is 0.3319564461708069\n",
      "epoch: 2 step: 421, loss is 0.3528931736946106\n",
      "epoch: 2 step: 422, loss is 0.5624347925186157\n",
      "epoch: 2 step: 423, loss is 0.24895580112934113\n",
      "epoch: 2 step: 424, loss is 0.3588709235191345\n",
      "epoch: 2 step: 425, loss is 0.3866462707519531\n",
      "epoch: 2 step: 426, loss is 0.37591296434402466\n",
      "epoch: 2 step: 427, loss is 0.34792664647102356\n",
      "epoch: 2 step: 428, loss is 0.2745853066444397\n",
      "epoch: 2 step: 429, loss is 0.24950283765792847\n",
      "epoch: 2 step: 430, loss is 0.34826788306236267\n",
      "epoch: 2 step: 431, loss is 0.3789099156856537\n",
      "epoch: 2 step: 432, loss is 0.4629025459289551\n",
      "epoch: 2 step: 433, loss is 0.40257900953292847\n",
      "epoch: 2 step: 434, loss is 0.29716041684150696\n",
      "epoch: 2 step: 435, loss is 0.3006894886493683\n",
      "epoch: 2 step: 436, loss is 0.38150718808174133\n",
      "epoch: 2 step: 437, loss is 0.3753909468650818\n",
      "epoch: 2 step: 438, loss is 0.304371178150177\n",
      "epoch: 2 step: 439, loss is 0.49003905057907104\n",
      "epoch: 2 step: 440, loss is 0.3560100197792053\n",
      "epoch: 2 step: 441, loss is 0.25744810700416565\n",
      "epoch: 2 step: 442, loss is 0.20387579500675201\n",
      "epoch: 2 step: 443, loss is 0.4290882349014282\n",
      "epoch: 2 step: 444, loss is 0.29933708906173706\n",
      "epoch: 2 step: 445, loss is 0.37529879808425903\n",
      "epoch: 2 step: 446, loss is 0.3678435683250427\n",
      "epoch: 2 step: 447, loss is 0.41166242957115173\n",
      "epoch: 2 step: 448, loss is 0.5791482925415039\n",
      "epoch: 2 step: 449, loss is 0.37549248337745667\n",
      "epoch: 2 step: 450, loss is 0.509837806224823\n",
      "epoch: 2 step: 451, loss is 0.400489866733551\n",
      "epoch: 2 step: 452, loss is 0.342174232006073\n",
      "epoch: 2 step: 453, loss is 0.2826651334762573\n",
      "epoch: 2 step: 454, loss is 0.4981227517127991\n",
      "epoch: 2 step: 455, loss is 0.35752591490745544\n",
      "epoch: 2 step: 456, loss is 0.46964460611343384\n",
      "epoch: 2 step: 457, loss is 0.37241315841674805\n",
      "epoch: 2 step: 458, loss is 0.2970808148384094\n",
      "epoch: 2 step: 459, loss is 0.455357164144516\n",
      "epoch: 2 step: 460, loss is 0.3660144805908203\n",
      "epoch: 2 step: 461, loss is 0.3520875573158264\n",
      "epoch: 2 step: 462, loss is 0.3906013071537018\n",
      "epoch: 2 step: 463, loss is 0.5697667002677917\n",
      "epoch: 2 step: 464, loss is 0.36949267983436584\n",
      "epoch: 2 step: 465, loss is 0.6636950969696045\n",
      "epoch: 2 step: 466, loss is 0.32831209897994995\n",
      "epoch: 2 step: 467, loss is 0.28464439511299133\n",
      "epoch: 2 step: 468, loss is 0.6132904291152954\n",
      "epoch: 2 step: 469, loss is 0.4256942868232727\n",
      "epoch: 2 step: 470, loss is 0.2975615859031677\n",
      "epoch: 2 step: 471, loss is 0.4637007415294647\n",
      "epoch: 2 step: 472, loss is 0.5175981521606445\n",
      "epoch: 2 step: 473, loss is 0.2806221842765808\n",
      "epoch: 2 step: 474, loss is 0.3146132826805115\n",
      "epoch: 2 step: 475, loss is 0.5133563876152039\n",
      "epoch: 2 step: 476, loss is 0.3557104468345642\n",
      "epoch: 2 step: 477, loss is 0.309675395488739\n",
      "epoch: 2 step: 478, loss is 0.5493594408035278\n",
      "epoch: 2 step: 479, loss is 0.3881758451461792\n",
      "epoch: 2 step: 480, loss is 0.35082918405532837\n",
      "epoch: 2 step: 481, loss is 0.32077503204345703\n",
      "epoch: 2 step: 482, loss is 0.33351951837539673\n",
      "epoch: 2 step: 483, loss is 0.4983903169631958\n",
      "epoch: 2 step: 484, loss is 0.5759215950965881\n",
      "epoch: 2 step: 485, loss is 0.33927011489868164\n",
      "epoch: 2 step: 486, loss is 0.5764020085334778\n",
      "epoch: 2 step: 487, loss is 0.46841946244239807\n",
      "epoch: 2 step: 488, loss is 0.37519270181655884\n",
      "epoch: 2 step: 489, loss is 0.2929040193557739\n",
      "epoch: 2 step: 490, loss is 0.36548683047294617\n",
      "epoch: 2 step: 491, loss is 0.2692630887031555\n",
      "epoch: 2 step: 492, loss is 0.5572800040245056\n",
      "epoch: 2 step: 493, loss is 0.42677775025367737\n",
      "epoch: 2 step: 494, loss is 0.4389924108982086\n",
      "epoch: 2 step: 495, loss is 0.5007215142250061\n",
      "epoch: 2 step: 496, loss is 0.3925729990005493\n",
      "epoch: 2 step: 497, loss is 0.5844864249229431\n",
      "epoch: 2 step: 498, loss is 0.34373995661735535\n",
      "epoch: 2 step: 499, loss is 0.4326708912849426\n",
      "epoch: 2 step: 500, loss is 0.5979893207550049\n",
      "epoch: 2 step: 501, loss is 0.3468438982963562\n",
      "epoch: 2 step: 502, loss is 0.594575047492981\n",
      "epoch: 2 step: 503, loss is 0.4935692250728607\n",
      "epoch: 2 step: 504, loss is 0.5270165205001831\n",
      "epoch: 2 step: 505, loss is 0.4235163927078247\n",
      "epoch: 2 step: 506, loss is 0.4488965570926666\n",
      "epoch: 2 step: 507, loss is 0.6807994842529297\n",
      "epoch: 2 step: 508, loss is 0.35443511605262756\n",
      "epoch: 2 step: 509, loss is 0.34165599942207336\n",
      "epoch: 2 step: 510, loss is 0.6022853851318359\n",
      "epoch: 2 step: 511, loss is 0.5402318239212036\n",
      "epoch: 2 step: 512, loss is 0.3525278568267822\n",
      "epoch: 2 step: 513, loss is 0.42358142137527466\n",
      "epoch: 2 step: 514, loss is 0.3854191303253174\n",
      "epoch: 2 step: 515, loss is 0.38135862350463867\n",
      "epoch: 2 step: 516, loss is 0.5729687213897705\n",
      "epoch: 2 step: 517, loss is 0.5810675621032715\n",
      "epoch: 2 step: 518, loss is 0.4725007712841034\n",
      "epoch: 2 step: 519, loss is 0.4697442054748535\n",
      "epoch: 2 step: 520, loss is 0.7803906202316284\n",
      "epoch: 2 step: 521, loss is 0.4504375457763672\n",
      "epoch: 2 step: 522, loss is 0.4288695454597473\n",
      "epoch: 2 step: 523, loss is 0.46762871742248535\n",
      "epoch: 2 step: 524, loss is 0.5503127574920654\n",
      "epoch: 2 step: 525, loss is 0.16351941227912903\n",
      "epoch: 2 step: 526, loss is 0.3261658549308777\n",
      "epoch: 2 step: 527, loss is 0.39889979362487793\n",
      "epoch: 2 step: 528, loss is 0.45412853360176086\n",
      "epoch: 2 step: 529, loss is 0.3858061730861664\n",
      "epoch: 2 step: 530, loss is 0.5375826358795166\n",
      "epoch: 2 step: 531, loss is 0.33114808797836304\n",
      "epoch: 2 step: 532, loss is 0.3702206313610077\n",
      "epoch: 2 step: 533, loss is 0.4422619044780731\n",
      "epoch: 2 step: 534, loss is 0.2980239987373352\n",
      "epoch: 2 step: 535, loss is 0.4276044964790344\n",
      "epoch: 2 step: 536, loss is 0.3966403007507324\n",
      "epoch: 2 step: 537, loss is 0.39349231123924255\n",
      "epoch: 2 step: 538, loss is 0.2685537040233612\n",
      "epoch: 2 step: 539, loss is 0.4405428171157837\n",
      "epoch: 2 step: 540, loss is 0.2481517642736435\n",
      "epoch: 2 step: 541, loss is 0.4819967746734619\n",
      "epoch: 2 step: 542, loss is 0.3760911524295807\n",
      "epoch: 2 step: 543, loss is 0.2946029007434845\n",
      "epoch: 2 step: 544, loss is 0.26666009426116943\n",
      "epoch: 2 step: 545, loss is 0.3212493062019348\n",
      "epoch: 2 step: 546, loss is 0.49186423420906067\n",
      "epoch: 2 step: 547, loss is 0.2551172971725464\n",
      "epoch: 2 step: 548, loss is 0.3614007830619812\n",
      "epoch: 2 step: 549, loss is 0.5061441659927368\n",
      "epoch: 2 step: 550, loss is 0.4343445897102356\n",
      "epoch: 2 step: 551, loss is 0.3951106071472168\n",
      "epoch: 2 step: 552, loss is 0.5272660255432129\n",
      "epoch: 2 step: 553, loss is 0.43729886412620544\n",
      "epoch: 2 step: 554, loss is 0.16839414834976196\n",
      "epoch: 2 step: 555, loss is 0.38970616459846497\n",
      "epoch: 2 step: 556, loss is 0.4089510142803192\n",
      "epoch: 2 step: 557, loss is 0.48506513237953186\n",
      "epoch: 2 step: 558, loss is 0.4109654426574707\n",
      "epoch: 2 step: 559, loss is 0.31038862466812134\n",
      "epoch: 2 step: 560, loss is 0.41404852271080017\n",
      "epoch: 2 step: 561, loss is 0.2550351619720459\n",
      "epoch: 2 step: 562, loss is 0.7390905022621155\n",
      "epoch: 2 step: 563, loss is 0.28058338165283203\n",
      "epoch: 2 step: 564, loss is 0.25157630443573\n",
      "epoch: 2 step: 565, loss is 0.3943212628364563\n",
      "epoch: 2 step: 566, loss is 0.7030906677246094\n",
      "epoch: 2 step: 567, loss is 0.5729588270187378\n",
      "epoch: 2 step: 568, loss is 0.40189963579177856\n",
      "epoch: 2 step: 569, loss is 0.33613884449005127\n",
      "epoch: 2 step: 570, loss is 0.2613763213157654\n",
      "epoch: 2 step: 571, loss is 0.24199756979942322\n",
      "epoch: 2 step: 572, loss is 0.42050087451934814\n",
      "epoch: 2 step: 573, loss is 0.3590642213821411\n",
      "epoch: 2 step: 574, loss is 0.47813722491264343\n",
      "epoch: 2 step: 575, loss is 0.47870349884033203\n",
      "epoch: 2 step: 576, loss is 0.2924802303314209\n",
      "epoch: 2 step: 577, loss is 0.27292561531066895\n",
      "epoch: 2 step: 578, loss is 0.3170264959335327\n",
      "epoch: 2 step: 579, loss is 0.3913502097129822\n",
      "epoch: 2 step: 580, loss is 0.4748046100139618\n",
      "epoch: 2 step: 581, loss is 0.40036922693252563\n",
      "epoch: 2 step: 582, loss is 0.6520704030990601\n",
      "epoch: 2 step: 583, loss is 0.3122207820415497\n",
      "epoch: 2 step: 584, loss is 0.40020930767059326\n",
      "epoch: 2 step: 585, loss is 0.320919007062912\n",
      "epoch: 2 step: 586, loss is 0.4721131920814514\n",
      "epoch: 2 step: 587, loss is 0.27805227041244507\n",
      "epoch: 2 step: 588, loss is 0.4996100664138794\n",
      "epoch: 2 step: 589, loss is 0.36327019333839417\n",
      "epoch: 2 step: 590, loss is 0.5078298449516296\n",
      "epoch: 2 step: 591, loss is 0.38558900356292725\n",
      "epoch: 2 step: 592, loss is 0.3965642750263214\n",
      "epoch: 2 step: 593, loss is 0.3775114417076111\n",
      "epoch: 2 step: 594, loss is 0.3162563443183899\n",
      "epoch: 2 step: 595, loss is 0.35231801867485046\n",
      "epoch: 2 step: 596, loss is 0.335573673248291\n",
      "epoch: 2 step: 597, loss is 0.3136492669582367\n",
      "epoch: 2 step: 598, loss is 0.3013785481452942\n",
      "epoch: 2 step: 599, loss is 0.3597636818885803\n",
      "epoch: 2 step: 600, loss is 0.5487320423126221\n",
      "epoch: 2 step: 601, loss is 0.33074674010276794\n",
      "epoch: 2 step: 602, loss is 0.3808167576789856\n",
      "epoch: 2 step: 603, loss is 0.3955756425857544\n",
      "epoch: 2 step: 604, loss is 0.31984567642211914\n",
      "epoch: 2 step: 605, loss is 0.28951501846313477\n",
      "epoch: 2 step: 606, loss is 0.2644188404083252\n",
      "epoch: 2 step: 607, loss is 0.3096068799495697\n",
      "epoch: 2 step: 608, loss is 0.3391433656215668\n",
      "epoch: 2 step: 609, loss is 0.44294726848602295\n",
      "epoch: 2 step: 610, loss is 0.3076174259185791\n",
      "epoch: 2 step: 611, loss is 0.6083874702453613\n",
      "epoch: 2 step: 612, loss is 0.5729248523712158\n",
      "epoch: 2 step: 613, loss is 0.4016885757446289\n",
      "epoch: 2 step: 614, loss is 0.31459563970565796\n",
      "epoch: 2 step: 615, loss is 0.25772595405578613\n",
      "epoch: 2 step: 616, loss is 0.38204461336135864\n",
      "epoch: 2 step: 617, loss is 0.4202529489994049\n",
      "epoch: 2 step: 618, loss is 0.28692740201950073\n",
      "epoch: 2 step: 619, loss is 0.28782254457473755\n",
      "epoch: 2 step: 620, loss is 0.4024927020072937\n",
      "epoch: 2 step: 621, loss is 0.37152671813964844\n",
      "epoch: 2 step: 622, loss is 0.3696020841598511\n",
      "epoch: 2 step: 623, loss is 0.1722739338874817\n",
      "epoch: 2 step: 624, loss is 0.5017043352127075\n",
      "epoch: 2 step: 625, loss is 0.32263240218162537\n",
      "epoch: 2 step: 626, loss is 0.414144903421402\n",
      "epoch: 2 step: 627, loss is 0.20999754965305328\n",
      "epoch: 2 step: 628, loss is 0.48074430227279663\n",
      "epoch: 2 step: 629, loss is 0.45732569694519043\n",
      "epoch: 2 step: 630, loss is 0.37485384941101074\n",
      "epoch: 2 step: 631, loss is 0.20096173882484436\n",
      "epoch: 2 step: 632, loss is 0.3014717102050781\n",
      "epoch: 2 step: 633, loss is 0.31344565749168396\n",
      "epoch: 2 step: 634, loss is 0.5376074314117432\n",
      "epoch: 2 step: 635, loss is 0.2823141813278198\n",
      "epoch: 2 step: 636, loss is 0.39386144280433655\n",
      "epoch: 2 step: 637, loss is 0.6717585921287537\n",
      "epoch: 2 step: 638, loss is 0.3342195749282837\n",
      "epoch: 2 step: 639, loss is 0.2604506313800812\n",
      "epoch: 2 step: 640, loss is 0.7187159061431885\n",
      "epoch: 2 step: 641, loss is 0.5178622603416443\n",
      "epoch: 2 step: 642, loss is 0.3461318612098694\n",
      "epoch: 2 step: 643, loss is 0.33812037110328674\n",
      "epoch: 2 step: 644, loss is 0.4816708564758301\n",
      "epoch: 2 step: 645, loss is 0.507440984249115\n",
      "epoch: 2 step: 646, loss is 0.3463645875453949\n",
      "epoch: 2 step: 647, loss is 0.33120524883270264\n",
      "epoch: 2 step: 648, loss is 0.24581582844257355\n",
      "epoch: 2 step: 649, loss is 0.46124571561813354\n",
      "epoch: 2 step: 650, loss is 0.30966681241989136\n",
      "epoch: 2 step: 651, loss is 0.5018181204795837\n",
      "epoch: 2 step: 652, loss is 0.42308536171913147\n",
      "epoch: 2 step: 653, loss is 0.3879971504211426\n",
      "epoch: 2 step: 654, loss is 0.3381137549877167\n",
      "epoch: 2 step: 655, loss is 0.6289211511611938\n",
      "epoch: 2 step: 656, loss is 0.26278331875801086\n",
      "epoch: 2 step: 657, loss is 0.43862587213516235\n",
      "epoch: 2 step: 658, loss is 0.4711478352546692\n",
      "epoch: 2 step: 659, loss is 0.3763490915298462\n",
      "epoch: 2 step: 660, loss is 0.49583056569099426\n",
      "epoch: 2 step: 661, loss is 0.2058899998664856\n",
      "epoch: 2 step: 662, loss is 0.32385915517807007\n",
      "epoch: 2 step: 663, loss is 0.2640891969203949\n",
      "epoch: 2 step: 664, loss is 0.4157290756702423\n",
      "epoch: 2 step: 665, loss is 0.3231135606765747\n",
      "epoch: 2 step: 666, loss is 0.3249518871307373\n",
      "epoch: 2 step: 667, loss is 0.4855002164840698\n",
      "epoch: 2 step: 668, loss is 0.4447709619998932\n",
      "epoch: 2 step: 669, loss is 0.4797082841396332\n",
      "epoch: 2 step: 670, loss is 0.40160515904426575\n",
      "epoch: 2 step: 671, loss is 0.44847428798675537\n",
      "epoch: 2 step: 672, loss is 0.41431280970573425\n",
      "epoch: 2 step: 673, loss is 0.33876270055770874\n",
      "epoch: 2 step: 674, loss is 0.37135249376296997\n",
      "epoch: 2 step: 675, loss is 0.48981407284736633\n",
      "epoch: 2 step: 676, loss is 0.2381497472524643\n",
      "epoch: 2 step: 677, loss is 0.4197537899017334\n",
      "epoch: 2 step: 678, loss is 0.30112141370773315\n",
      "epoch: 2 step: 679, loss is 0.31883031129837036\n",
      "epoch: 2 step: 680, loss is 0.48593562841415405\n",
      "epoch: 2 step: 681, loss is 0.44744300842285156\n",
      "epoch: 2 step: 682, loss is 0.392483651638031\n",
      "epoch: 2 step: 683, loss is 0.22550302743911743\n",
      "epoch: 2 step: 684, loss is 0.36953842639923096\n",
      "epoch: 2 step: 685, loss is 0.4408203363418579\n",
      "epoch: 2 step: 686, loss is 0.6389039754867554\n",
      "epoch: 2 step: 687, loss is 0.33032581210136414\n",
      "epoch: 2 step: 688, loss is 0.29059869050979614\n",
      "epoch: 2 step: 689, loss is 0.3771170377731323\n",
      "epoch: 2 step: 690, loss is 0.23247142136096954\n",
      "epoch: 2 step: 691, loss is 0.4966110587120056\n",
      "epoch: 2 step: 692, loss is 0.4547191858291626\n",
      "epoch: 2 step: 693, loss is 0.40055757761001587\n",
      "epoch: 2 step: 694, loss is 0.5240659713745117\n",
      "epoch: 2 step: 695, loss is 0.2846427261829376\n",
      "epoch: 2 step: 696, loss is 0.4115970730781555\n",
      "epoch: 2 step: 697, loss is 0.3115486800670624\n",
      "epoch: 2 step: 698, loss is 0.4406837522983551\n",
      "epoch: 2 step: 699, loss is 0.4419967234134674\n",
      "epoch: 2 step: 700, loss is 0.26368388533592224\n",
      "epoch: 2 step: 701, loss is 0.5219664573669434\n",
      "epoch: 2 step: 702, loss is 0.49996426701545715\n",
      "epoch: 2 step: 703, loss is 0.24447622895240784\n",
      "epoch: 2 step: 704, loss is 0.3736448287963867\n",
      "epoch: 2 step: 705, loss is 0.3453562259674072\n",
      "epoch: 2 step: 706, loss is 0.36206555366516113\n",
      "epoch: 2 step: 707, loss is 0.38940173387527466\n",
      "epoch: 2 step: 708, loss is 0.256855309009552\n",
      "epoch: 2 step: 709, loss is 0.4702298045158386\n",
      "epoch: 2 step: 710, loss is 0.22796888649463654\n",
      "epoch: 2 step: 711, loss is 0.37299394607543945\n",
      "epoch: 2 step: 712, loss is 0.40909239649772644\n",
      "epoch: 2 step: 713, loss is 0.5173723697662354\n",
      "epoch: 2 step: 714, loss is 0.3330601453781128\n",
      "epoch: 2 step: 715, loss is 0.38207098841667175\n",
      "epoch: 2 step: 716, loss is 0.5304957032203674\n",
      "epoch: 2 step: 717, loss is 0.2807513177394867\n",
      "epoch: 2 step: 718, loss is 0.44506388902664185\n",
      "epoch: 2 step: 719, loss is 0.41155534982681274\n",
      "epoch: 2 step: 720, loss is 0.32963132858276367\n",
      "epoch: 2 step: 721, loss is 0.31564104557037354\n",
      "epoch: 2 step: 722, loss is 0.22805596888065338\n",
      "epoch: 2 step: 723, loss is 0.29031991958618164\n",
      "epoch: 2 step: 724, loss is 0.2639598250389099\n",
      "epoch: 2 step: 725, loss is 0.5470684766769409\n",
      "epoch: 2 step: 726, loss is 0.257782518863678\n",
      "epoch: 2 step: 727, loss is 0.30676060914993286\n",
      "epoch: 2 step: 728, loss is 0.3092743456363678\n",
      "epoch: 2 step: 729, loss is 0.3572523891925812\n",
      "epoch: 2 step: 730, loss is 0.2364872694015503\n",
      "epoch: 2 step: 731, loss is 0.2887439727783203\n",
      "epoch: 2 step: 732, loss is 0.3610277473926544\n",
      "epoch: 2 step: 733, loss is 0.4944658577442169\n",
      "epoch: 2 step: 734, loss is 0.3974727988243103\n",
      "epoch: 2 step: 735, loss is 0.5719417333602905\n",
      "epoch: 2 step: 736, loss is 0.321230411529541\n",
      "epoch: 2 step: 737, loss is 0.3315582573413849\n",
      "epoch: 2 step: 738, loss is 0.47165417671203613\n",
      "epoch: 2 step: 739, loss is 0.48133915662765503\n",
      "epoch: 2 step: 740, loss is 0.22052745521068573\n",
      "epoch: 2 step: 741, loss is 0.2514430284500122\n",
      "epoch: 2 step: 742, loss is 0.4142425060272217\n",
      "epoch: 2 step: 743, loss is 0.4361976981163025\n",
      "epoch: 2 step: 744, loss is 0.32929539680480957\n",
      "epoch: 2 step: 745, loss is 0.40138882398605347\n",
      "epoch: 2 step: 746, loss is 0.3635224401950836\n",
      "epoch: 2 step: 747, loss is 0.4519107937812805\n",
      "epoch: 2 step: 748, loss is 0.41145050525665283\n",
      "epoch: 2 step: 749, loss is 0.5627183318138123\n",
      "epoch: 2 step: 750, loss is 0.37715283036231995\n",
      "epoch: 2 step: 751, loss is 0.4627823233604431\n",
      "epoch: 2 step: 752, loss is 0.3968159556388855\n",
      "epoch: 2 step: 753, loss is 0.3228023052215576\n",
      "epoch: 2 step: 754, loss is 0.49391472339630127\n",
      "epoch: 2 step: 755, loss is 0.4162335991859436\n",
      "epoch: 2 step: 756, loss is 0.4754297137260437\n",
      "epoch: 2 step: 757, loss is 0.33122187852859497\n",
      "epoch: 2 step: 758, loss is 0.2099251002073288\n",
      "epoch: 2 step: 759, loss is 0.35824674367904663\n",
      "epoch: 2 step: 760, loss is 0.5282756090164185\n",
      "epoch: 2 step: 761, loss is 0.34805965423583984\n",
      "epoch: 2 step: 762, loss is 0.368935227394104\n",
      "epoch: 2 step: 763, loss is 0.2306305170059204\n",
      "epoch: 2 step: 764, loss is 0.3719121217727661\n",
      "epoch: 2 step: 765, loss is 0.24049389362335205\n",
      "epoch: 2 step: 766, loss is 0.3637845516204834\n",
      "epoch: 2 step: 767, loss is 0.21588864922523499\n",
      "epoch: 2 step: 768, loss is 0.6174741983413696\n",
      "epoch: 2 step: 769, loss is 0.47794777154922485\n",
      "epoch: 2 step: 770, loss is 0.2801671028137207\n",
      "epoch: 2 step: 771, loss is 0.6206282377243042\n",
      "epoch: 2 step: 772, loss is 0.33575987815856934\n",
      "epoch: 2 step: 773, loss is 0.3877110481262207\n",
      "epoch: 2 step: 774, loss is 0.5104196071624756\n",
      "epoch: 2 step: 775, loss is 0.3682829737663269\n",
      "epoch: 2 step: 776, loss is 0.31870394945144653\n",
      "epoch: 2 step: 777, loss is 0.4721897840499878\n",
      "epoch: 2 step: 778, loss is 0.28802886605262756\n",
      "epoch: 2 step: 779, loss is 0.24973559379577637\n",
      "epoch: 2 step: 780, loss is 0.46509456634521484\n",
      "epoch: 2 step: 781, loss is 0.3378356695175171\n",
      "epoch: 2 step: 782, loss is 0.36146634817123413\n",
      "epoch: 2 step: 783, loss is 0.5137929916381836\n",
      "epoch: 2 step: 784, loss is 0.25686025619506836\n",
      "epoch: 2 step: 785, loss is 0.40264105796813965\n",
      "epoch: 2 step: 786, loss is 0.32853978872299194\n",
      "epoch: 2 step: 787, loss is 0.38363343477249146\n",
      "epoch: 2 step: 788, loss is 0.36645829677581787\n",
      "epoch: 2 step: 789, loss is 0.34485700726509094\n",
      "epoch: 2 step: 790, loss is 0.3431175649166107\n",
      "epoch: 2 step: 791, loss is 0.29416173696517944\n",
      "epoch: 2 step: 792, loss is 0.4147622585296631\n",
      "epoch: 2 step: 793, loss is 0.2671707272529602\n",
      "epoch: 2 step: 794, loss is 0.33842790126800537\n",
      "epoch: 2 step: 795, loss is 0.4972050189971924\n",
      "epoch: 2 step: 796, loss is 0.3593491315841675\n",
      "epoch: 2 step: 797, loss is 0.31429994106292725\n",
      "epoch: 2 step: 798, loss is 0.5673431158065796\n",
      "epoch: 2 step: 799, loss is 0.4544183611869812\n",
      "epoch: 2 step: 800, loss is 0.30555859208106995\n",
      "epoch: 2 step: 801, loss is 0.4348437488079071\n",
      "epoch: 2 step: 802, loss is 0.5507081747055054\n",
      "epoch: 2 step: 803, loss is 0.28819841146469116\n",
      "epoch: 2 step: 804, loss is 0.45136991143226624\n",
      "epoch: 2 step: 805, loss is 0.3941556513309479\n",
      "epoch: 2 step: 806, loss is 0.5290935039520264\n",
      "epoch: 2 step: 807, loss is 0.46143457293510437\n",
      "epoch: 2 step: 808, loss is 0.33334535360336304\n",
      "epoch: 2 step: 809, loss is 0.3611884117126465\n",
      "epoch: 2 step: 810, loss is 0.44167107343673706\n",
      "epoch: 2 step: 811, loss is 0.27836722135543823\n",
      "epoch: 2 step: 812, loss is 0.391169011592865\n",
      "epoch: 2 step: 813, loss is 0.3061293363571167\n",
      "epoch: 2 step: 814, loss is 0.3076483905315399"
     ]
    }
   ],
   "source": [
    "print(\"--- Training without Regularization ---\")\n",
    "acc_no_reg = train_and_eval(ForwardFashion)\n",
    "\n",
    "print(\"\\n--- Training with Regularization ---\")\n",
    "acc_with_reg = train_and_eval(ForwardFashionRegularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "\n",
    " **结果分析**:\n",
    "\n",
    " 1.  **无正则化模型**: 准确率约为 `90.42%`。\n",
    " 2.  **有正则化模型**: 准确率约为 `88.29%`(不知道为什么上面结果没有显示出来)。\n",
    "\n",
    " **结论**:\n",
    " 从结果可以看出，添加了Dropout和BatchNorm的正则化模型在Fashion-MNIST测试集上的表现并没有优于没有正则化的模型，这与我的预期有一些差距。\n",
    " 我认为这可能是因为Fashion-MNIST数据集相对简单，正则化技术对它的影响有限。\n",
    " 在实践中，我们应当根据模型的复杂度和数据集的大小，合理地组合使用不同的正则化方法是提升模型性能的常用策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
